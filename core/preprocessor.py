"""
ì„¹ì…˜ ì „ì²˜ë¦¬ ëª¨ë“ˆ
Gemini Vision ê¸°ë°˜ PDF ì„¹ì…˜ ì „ì²˜ë¦¬ ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
"""
import os
from PyPDF2 import PdfReader, PdfWriter
from langchain.docstore.document import Document
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from .vision_processor import VisionProcessor
from .chunker import DocumentChunker
import config


class SectionPreprocessor:
    """ì„¹ì…˜ ì „ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_name: LLM ëª¨ë¸ëª… (Gemini Vision ëª¨ë¸)
        """
        self.model_name = model_name or config.DEFAULT_LLM_MODEL
        self.vision_processor = VisionProcessor(model_name)
        self.chunker = DocumentChunker()
    
    def extract_pdf_section(self, pdf_path: str, start_page: int, end_page: int) -> str:
        """
        PDFì—ì„œ íŠ¹ì • í˜ì´ì§€ ë²”ìœ„ë§Œ ì¶”ì¶œí•˜ì—¬ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            pdf_path: ì›ë³¸ PDF íŒŒì¼ ê²½ë¡œ
            start_page: ì‹œì‘ í˜ì´ì§€ (1-based)
            end_page: ë í˜ì´ì§€ (1-based)
            
        Returns:
            temp_path: ì„ì‹œ PDF íŒŒì¼ ê²½ë¡œ
        """
        reader = PdfReader(pdf_path)
        writer = PdfWriter()
        
        for page_num in range(start_page - 1, min(end_page, len(reader.pages))):
            writer.add_page(reader.pages[page_num])
        
        temp_path = f"{config.TOC_SECTIONS_DIR}/section_{start_page}_{end_page}.pdf"
        with open(temp_path, "wb") as output_file:
            writer.write(output_file)
        
        return temp_path
    
    def preprocess_section(self, section: dict, pdf_path: str) -> dict:
        """
        ì„¹ì…˜ì„ ì „ì²˜ë¦¬í•˜ì—¬ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        
        Args:
            section: ì„¹ì…˜ ì •ë³´ {"title": "...", "start_page": X, "end_page": Y}
            pdf_path: ì›ë³¸ PDF íŒŒì¼ ê²½ë¡œ
            
        Returns:
            {
                "vectorstore": FAISS ë²¡í„°ìŠ¤í† ì–´,
                "documents": Document ë¦¬ìŠ¤íŠ¸,
                "table_count": í‘œ ê°œìˆ˜
            }
        """
        try:
            # ì„¹ì…˜ ì¶”ì¶œ
            section_path = self.extract_pdf_section(
                pdf_path,
                section.get("start_page", 1),
                section.get("end_page", 1)
            )
        except Exception as e:
            print(f"   âš ï¸  ì„¹ì…˜ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            # ë¹ˆ ê²°ê³¼ ë°˜í™˜
            return {
                "vectorstore": None,
                "documents": [],
                "table_count": 0
            }
        
        # Gemini Visionìœ¼ë¡œ í˜ì´ì§€ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
        try:
            print(f"\nğŸ“„ [{section.get('title', 'ì•Œ ìˆ˜ ì—†ìŒ')}] Gemini Visionìœ¼ë¡œ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‹œì‘...")
            markdown_results = self.vision_processor.convert_section_to_markdown(
                pdf_path,
                section.get('start_page', 1),
                section.get('end_page', 1)
            )
            
            if not markdown_results:
                print(f"   âš ï¸  ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {
                    "vectorstore": None,
                    "documents": [],
                    "table_count": 0
                }
            
            print(f"   âœ… {len(markdown_results)}ê°œ í˜ì´ì§€ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì™„ë£Œ")
        except Exception as e:
            print(f"   âš ï¸  ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            print(f"   ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
            return {
                "vectorstore": None,
                "documents": [],
                "table_count": 0
            }
        
        # Dual Chunking ì „ëµìœ¼ë¡œ ë¬¸ì„œ ì²˜ë¦¬
        split_docs = []
        table_count = 0
        
        for page_num, markdown_text in markdown_results:
            try:
                # chunkerì˜ Dual Chunking ë©”ì„œë“œ ì‚¬ìš©
                page_docs = self.chunker.chunk_markdown_with_dual_chunking(
                    markdown_text,
                    page_number=page_num
                )
                
                # ì„¹ì…˜ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                for doc in page_docs:
                    doc.metadata.update({
                        'section_title': section.get('title', 'ì•Œ ìˆ˜ ì—†ìŒ'),
                        'section_start': section.get('start_page', 1),
                        'section_end': section.get('end_page', 1)
                    })
                    
                    # í‘œ ê°œìˆ˜ ì¹´ìš´íŠ¸
                    if doc.metadata.get('type') == 'table':
                        table_count += 1
                    
                    split_docs.append(doc)
                    
            except Exception as e:
                print(f"   âš ï¸  í˜ì´ì§€ {page_num} Dual Chunking ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì „ì²´ í˜ì´ì§€ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ ë¬¸ì„œë¡œ ì¶”ê°€
                try:
                    fallback_doc = Document(
                        page_content=markdown_text,
                        metadata={
                            'section_title': section.get('title', 'ì•Œ ìˆ˜ ì—†ìŒ'),
                            'section_start': section.get('start_page', 1),
                            'section_end': section.get('end_page', 1),
                            'page_number': page_num,
                            'type': 'text',
                            'chunk_type': 'page',
                            'is_table': False
                        }
                    )
                    split_docs.append(fallback_doc)
                except:
                    pass
        
        # ì„ë² ë”© ìƒì„± ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        try:
            if not split_docs or len(split_docs) == 0:
                print(f"   âš ï¸  ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                # ë¹ˆ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
                embeddings = GoogleGenerativeAIEmbeddings(model=config.DEFAULT_EMBEDDING_MODEL)
                # ë¹ˆ Documentë¡œ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
                empty_doc = Document(page_content="", metadata={})
                vectorstore = FAISS.from_documents(documents=[empty_doc], embedding=embeddings)
            else:
                embeddings = GoogleGenerativeAIEmbeddings(model=config.DEFAULT_EMBEDDING_MODEL)
                vectorstore = FAISS.from_documents(documents=split_docs, embedding=embeddings)
        except Exception as e:
            print(f"   âš ï¸  ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë²¡í„°ìŠ¤í† ì–´ ë°˜í™˜
            try:
                embeddings = GoogleGenerativeAIEmbeddings(model=config.DEFAULT_EMBEDDING_MODEL)
                empty_doc = Document(page_content="", metadata={})
                vectorstore = FAISS.from_documents(documents=[empty_doc], embedding=embeddings)
            except:
                vectorstore = None
        
        return {
            "vectorstore": vectorstore,
            "documents": split_docs if split_docs else [],
            "table_count": table_count
        }

