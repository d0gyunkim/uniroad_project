"""
RAG ì‹œìŠ¤í…œ ëª¨ë“ˆ
ì§ˆì˜ì‘ë‹µ íŒŒì´í”„ë¼ì¸ ë° ê´€ë ¨ ì„¹ì…˜ ì„ íƒ
"""
import os
import re
from typing import List, Optional, Tuple
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_teddynote.prompts import load_prompt
from langchain.docstore.document import Document
from supabase import create_client, Client
from dotenv import load_dotenv
from .searcher import SearchEngine, SupabaseSearcher
from .chunker import DocumentChunker
from .quality_evaluator import QualityEvaluator
import config

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(override=True)


class RAGSystem:
    """RAG ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_name: LLM ëª¨ë¸ëª…
        """
        self.model_name = model_name or config.DEFAULT_LLM_MODEL
        self.searcher = SearchEngine()
        self.chunker = DocumentChunker()
        self.quality_evaluator = QualityEvaluator(model_name)
        
        # Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        if supabase_url and supabase_key:
            self.supabase: Client = create_client(supabase_url, supabase_key)
            
            # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
            embedding_model = config.DEFAULT_EMBEDDING_MODEL
            embedding_kwargs = {
                "request_timeout": 600,
                "batch_size": 100,
                "max_retries": 10,
                "retry_delay": 15
            }
            if embedding_model:
                embedding_kwargs["model"] = embedding_model
            
            self.embeddings = GoogleGenerativeAIEmbeddings(**embedding_kwargs)
            self.supabase_searcher = SupabaseSearcher(self.supabase, self.embeddings)
        else:
            self.supabase = None
            self.embeddings = None
            self.supabase_searcher = None
            print("âš ï¸  Supabase í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¡œì»¬ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
    
    def _get_relevant_sections(self, school_name: str) -> List[dict]:
        """
        Supabaseì—ì„œ í•™êµì˜ ì„¹ì…˜ ëª©ë¡ ì¡°íšŒ
        
        Args:
            school_name: í•™êµ ì´ë¦„
            
        Returns:
            ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸ [{"id": int, "section_name": str, "page_start": int, "page_end": int}, ...]
        """
        if not self.supabase:
            raise ValueError("Supabase í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            # documents í…Œì´ë¸”ì—ì„œ school_nameìœ¼ë¡œ ë¬¸ì„œ ì°¾ê¸°
            documents_response = self.supabase.table("documents").select("id").eq("school_name", school_name).execute()
            
            if not documents_response.data:
                return []
            
            document_ids = [doc["id"] for doc in documents_response.data]
            
            # document_sections í…Œì´ë¸”ì—ì„œ ì„¹ì…˜ ëª©ë¡ ì¡°íšŒ
            sections_response = self.supabase.table("document_sections").select(
                "id, section_name, page_start, page_end"
            ).in_("document_id", document_ids).execute()
            
            sections = []
            for section in sections_response.data:
                sections.append({
                    "id": section["id"],
                    "title": section["section_name"],
                    "start_page": section["page_start"],
                    "end_page": section["page_end"]
                })
            
            return sections
            
        except Exception as e:
            print(f"âŒ Supabase ì„¹ì…˜ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback
            print(traceback.format_exc())
            return []
    
    def find_relevant_sections(self, question: str, toc_index: list = None, school_name: str = None) -> tuple:
        """
        ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì„¹ì…˜ì„ ì°¾ê³  ê´€ë ¨ì„±ì— ë”°ë¼ ì¬ìˆœìœ„í™”(re-rank)í•˜ëŠ” ë©”ì„œë“œ
        
        [ì‘ë™ ì›ë¦¬]
        1. LLMìœ¼ë¡œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì„¹ì…˜ ì„ íƒ
        2. ì„ íƒëœ ì„¹ì…˜ë“¤ì„ ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„±ì— ë”°ë¼ ì ìˆ˜í™”
        3. ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì¬ìˆœìœ„í™”í•˜ì—¬ ë°˜í™˜
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            toc_index: ëª©ì°¨ ì¸ë±ìŠ¤ (ì„ íƒ, Noneì´ë©´ Supabaseì—ì„œ ì¡°íšŒ)
            school_name: í•™êµ ì´ë¦„ (toc_indexê°€ Noneì¼ ë•Œ í•„ìˆ˜)
            
        Returns:
            (section_numbers, thinking_process): ì¬ìˆœìœ„í™”ëœ ì„¹ì…˜ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ì™€ ì‚¬ê³  ê³¼ì •
        """
        # toc_indexê°€ ì—†ìœ¼ë©´ Supabaseì—ì„œ ì¡°íšŒ
        if toc_index is None:
            if not school_name:
                raise ValueError("toc_indexê°€ Noneì¼ ë•ŒëŠ” school_nameì´ í•„ìˆ˜ì…ë‹ˆë‹¤.")
            toc_index = self._get_relevant_sections(school_name)
            if not toc_index:
                return ([0], "ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        sections_list = "\n".join([
            f"{idx+1}. {section.get('title', 'ì•Œ ìˆ˜ ì—†ìŒ')} (í˜ì´ì§€ {section.get('start_page', '?')}-{section.get('end_page', '?')})"
            for idx, section in enumerate(toc_index)
        ])
        
        # 1ë‹¨ê³„: ê´€ë ¨ ì„¹ì…˜ ì„ íƒ
        routing_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ëª¨ì§‘ìš”ê°• ë¬¸ì„œì˜ ëª©ì°¨ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³ , ë‹µë³€ì„ ì°¾ê¸° ìœ„í•´ ì–´ë–¤ ì„¹ì…˜ì„ í™•ì¸í•´ì•¼ í• ì§€ ìƒê°í•´ë³´ì„¸ìš”.

**ì‚¬ìš©ì ì§ˆë¬¸:**
{question}

**ë¬¸ì„œì˜ ëª©ì°¨ (ì„¹ì…˜ ëª©ë¡):**
{sections_list}

**ë¶„ì„ ê³¼ì •:**
1. ì§ˆë¬¸ì˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì–´ë–¤ ì„¹ì…˜ì„ í™•ì¸í•´ì•¼ í• ì§€ ìƒê°í•´ë³´ì„¸ìš”, ë‹¨ ì§ˆë¬¸ì˜ ë‹¨ì ì¸ ë‚´ìš© ë¿ë§Œ ì•„ë‹ˆë¼, ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ì„¸ìš”
2. ê° ì„¹ì…˜ì˜ ì œëª©ì„ ë³´ê³  ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”,ë‹¨ ì§ˆë¬¸ì˜ í‚¤ì›Œë“œë‚˜ ë‹¨ì–´ì—ë§Œ ì§‘ì¤‘í•˜ì§€ ë§ˆì„¸ìš”
3. ê´€ë ¨ ì„¹ì…˜ì„ ì„ íƒí•˜ê³  ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”

**ì¶œë ¥ í˜•ì‹:**
ì‚¬ê³  ê³¼ì •: [ì§ˆë¬¸ ë¶„ì„ ë° ì„¹ì…˜ ì„ íƒ ì´ìœ ]
ì„ íƒí•œ ì„¹ì…˜: [ì„¹ì…˜ ë²ˆí˜¸ë“¤, ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: 1, 3]

---
**ë¶„ì„:**
""")
        
        llm = ChatGoogleGenerativeAI(model=self.model_name, temperature=0)
        chain = routing_prompt | llm | StrOutputParser()
        
        response = chain.invoke({"question": question, "sections_list": sections_list})
        
        # ì‚¬ê³  ê³¼ì • ì¶”ì¶œ
        thinking_process = ""
        if "ì‚¬ê³  ê³¼ì •:" in response:
            thinking_part = response.split("ì‚¬ê³  ê³¼ì •:")[1]
            if "ì„ íƒí•œ ì„¹ì…˜:" in thinking_part:
                thinking_process = thinking_part.split("ì„ íƒí•œ ì„¹ì…˜:")[0].strip()
            else:
                thinking_process = thinking_part.strip()
        
        # ì„¹ì…˜ ë²ˆí˜¸ ì¶”ì¶œ
        section_numbers = []
        if "ì„ íƒí•œ ì„¹ì…˜:" in response:
            section_part = response.split("ì„ íƒí•œ ì„¹ì…˜:")[1]
            for num_str in re.findall(r'\d+', section_part):
                num = int(num_str)
                if 1 <= num <= len(toc_index):
                    section_numbers.append(num - 1)
        else:
            for num_str in re.findall(r'\d+', response):
                num = int(num_str)
                if 1 <= num <= len(toc_index):
                    section_numbers.append(num - 1)
        
        if not section_numbers:
            section_numbers = [0]
        
        # Re-ranking ì œê±°: Vector Searchì˜ ìœ ì‚¬ë„ ì ìˆ˜ê°€ ì´ë¯¸ ì¶©ë¶„íˆ ì •í™•í•˜ë¯€ë¡œ
        # ì¤‘ë³µëœ LLM í˜¸ì¶œì„ ì œê±°í•˜ì—¬ ì‘ë‹µ ì†ë„ ìµœì í™”
        return section_numbers, thinking_process
    
    # _rerank_sections ë©”ì„œë“œ ì œê±°: Re-ranking ë‹¨ê³„ë¥¼ ì œê±°í•˜ì—¬ LLM í˜¸ì¶œ ì ˆì•½ ë° ì‘ë‹µ ì†ë„ ìµœì í™”
    # Vector Searchì˜ ìœ ì‚¬ë„ ì ìˆ˜ê°€ ì´ë¯¸ ì¶©ë¶„íˆ ì •í™•í•˜ë¯€ë¡œ ì¤‘ë³µëœ LLM í˜¸ì¶œ ë¶ˆí•„ìš”
    
    def retrieve(
        self,
        question: str,
        school_name: str,
        section_id: Optional[int] = None,
        top_k: int = None
    ) -> List[tuple]:
        """
        Supabaseì—ì„œ ë¬¸ì„œ ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            school_name: í•™êµ ì´ë¦„ (í•„ìˆ˜)
            section_id: ì„¹ì…˜ ID (ì„ íƒ, ë¼ìš°íŒ… ê²°ê³¼)
            top_k: ë°˜í™˜í•  ìƒìœ„ ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’: TOP_K_PER_SECTION)
            
        Returns:
            [(Document, score), ...] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸
        """
        if not self.supabase_searcher:
            raise ValueError("Supabase ê²€ìƒ‰ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if top_k is None:
            top_k = config.TOP_K_PER_SECTION
        
        # SupabaseSearcherë¡œ ê²€ìƒ‰ ìˆ˜í–‰
        documents = self.supabase_searcher.search(
            query=question,
            school_name=school_name,
            section_id=section_id,
            top_k=top_k
        )
        
        # Documentì™€ scoreë¥¼ íŠœí”Œë¡œ ë³€í™˜
        docs_with_scores = []
        for doc in documents:
            score = doc.metadata.get("score", 0.0)
            docs_with_scores.append((doc, score))
        
        return docs_with_scores
    
    def search_global_raw(
        self,
        school_name: str,
        query: str,
        top_k: int = 30,
        content_weight: float = 0.6,
        summary_weight: float = 0.4
    ) -> List[dict]:
        """
        ë¼ìš°íŒ… ì—†ëŠ” ì „ì—­ ê²€ìƒ‰ ëª¨ë“œ
        
        ì„¹ì…˜ ì„ íƒ ì—†ì´ í•´ë‹¹ í•™êµì˜ ì „ì²´ ë¬¸ì„œë¥¼ ëŒ€ìƒìœ¼ë¡œ ê²€ìƒ‰í•˜ê³ ,
        ìš”ì•½ë¬¸ê³¼ ì²­í¬ ê°„ì˜ ê°€ì¤‘ í‰ê·  ìœ ì‚¬ë„ ìƒìœ„ 10ê°œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            school_name: í•™êµ ì´ë¦„ (í•„ìˆ˜)
            query: ê²€ìƒ‰ ì§ˆë¬¸ í…ìŠ¤íŠ¸
            top_k: ì´ˆê¸° ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’: 30)
            content_weight: ë‚´ìš© ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 0.6)
            summary_weight: Summary ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 0.4)
            
        Returns:
            [
                {
                    "chunk_id": int,
                    "section_id": int,
                    "document_id": int
                },
                ...
            ] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ (ìƒìœ„ 10ê°œ)
        """
        if not self.supabase_searcher:
            raise ValueError("Supabase ê²€ìƒ‰ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if not self.embeddings:
            raise ValueError("ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # 1. ë¼ìš°íŒ… ìƒëµ: ì„¹ì…˜ ì„ íƒ ì—†ì´ ë°”ë¡œ ì „ì²´ ë¬¸ì„œ ê²€ìƒ‰
        print(f"ğŸ” ì „ì—­ ê²€ìƒ‰ ëª¨ë“œ: '{query}' (í•™êµ: {school_name})")
        documents = self.supabase_searcher.search(
            query=query,
            school_name=school_name,
            section_id=None,  # Noneìœ¼ë¡œ ì „ë‹¬í•˜ì—¬ ì „ì²´ ë¬¸ì„œ ê²€ìƒ‰
            top_k=top_k
        )
        
        if not documents:
            print("âš ï¸  ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"âœ… ì´ˆê¸° ê²€ìƒ‰ ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ ë°œê²¬")
        
        # 2. ê° ì²­í¬ì˜ document_id ìˆ˜ì§‘
        document_ids = []
        for doc in documents:
            doc_id = doc.metadata.get("document_id")
            if doc_id:
                document_ids.append(doc_id)
        
        # 3. documents í…Œì´ë¸”ì—ì„œ summary ì¼ê´„ ì¡°íšŒ
        document_summaries = {}
        if document_ids and self.supabase:
            try:
                unique_doc_ids = list(set(document_ids))
                response = self.supabase.table("documents").select("id, summary").in_("id", unique_doc_ids).execute()
                for doc in response.data:
                    document_summaries[doc["id"]] = doc.get("summary")
            except Exception as e:
                print(f"âš ï¸ ë¬¸ì„œ summary ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # 4. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        try:
            query_embedding = self.embeddings.embed_query(query)
        except Exception as e:
            print(f"âš ï¸ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return []
        
        # 5. ê° ì²­í¬ì— ëŒ€í•´ ê°€ì¤‘ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
        import numpy as np
        
        def calculate_cosine_similarity(vec1, vec2):
            """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
            vec1 = np.array(vec1)
            vec2 = np.array(vec2)
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            if norm1 == 0 or norm2 == 0:
                return 0.0
            return dot_product / (norm1 * norm2)
        
        chunks_with_weighted_scores = []
        
        for doc in documents:
            # ë‚´ìš© ìœ ì‚¬ë„ (ê¸°ì¡´ score)
            content_similarity = doc.metadata.get("score", 0.0)
            
            # Summary ìœ ì‚¬ë„ ê³„ì‚°
            summary_similarity = 0.0
            document_id = doc.metadata.get("document_id")
            if document_id and document_id in document_summaries:
                summary = document_summaries[document_id]
                if summary:
                    try:
                        summary_embedding = self.embeddings.embed_query(summary)
                        summary_similarity = calculate_cosine_similarity(query_embedding, summary_embedding)
                    except Exception as e:
                        print(f"âš ï¸ Summary ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                        summary_similarity = 0.0
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            weighted_average = (content_similarity * content_weight) + (summary_similarity * summary_weight)
            
            chunks_with_weighted_scores.append({
                "doc": doc,
                "weighted_score": weighted_average,
                "content_score": content_similarity,
                "summary_score": summary_similarity
            })
        
        # 6. ê°€ì¤‘ í‰ê·  ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        chunks_with_weighted_scores.sort(key=lambda x: x["weighted_score"], reverse=True)
        
        # 7. ìƒìœ„ 10ê°œ ì„ íƒ
        top_10_chunks = chunks_with_weighted_scores[:10]
        
        print(f"ğŸ“Š ê°€ì¤‘ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ: ìƒìœ„ 10ê°œ ì„ íƒ")
        
        # 8. ê²°ê³¼ í¬ë§·íŒ…: ì²­í¬ id, ì„¹ì…˜ id, ë¬¸ì„œ id ë¿ë§Œ ì•„ë‹ˆë¼
        #    page_number, chunk_type, contentê¹Œì§€ ëª¨ë‘ í¬í•¨í•˜ì—¬ ë°˜í™˜
        results = []
        for item in top_10_chunks:
            doc = item["doc"]
            metadata = doc.metadata
            chunk_id = metadata.get("chunk_id")
            
            if not chunk_id:
                continue
            
            results.append({
                "chunk_id": chunk_id,
                "section_id": metadata.get("section_id"),
                "document_id": metadata.get("document_id"),
                "page_number": metadata.get("page_number"),
                "chunk_type": metadata.get("chunk_type"),
                # SupabaseSearcherì—ì„œ raw_dataë¥¼ page_contentë¡œ ë„£ì—ˆìœ¼ë¯€ë¡œ
                # ì—¬ê¸°ì„œëŠ” ê·¸ëŒ€ë¡œ contentë¡œ ë…¸ì¶œ
                "content": doc.page_content,
                "score": metadata.get("score", 0.0),
            })
        
        return results
    
    def answer(
        self,
        question: str,
        school_name: str,
        conversation_history: List[tuple] = None,
        stream: bool = False,
        max_retries: int = None
    ):
        """
        ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (Supabase ê¸°ë°˜)
        
        [ì²˜ë¦¬ ê³¼ì •]
        1. ë™ì  ë¼ìš°íŒ…: LLMìœ¼ë¡œ ê´€ë ¨ ì„¹ì…˜ ì„ íƒ
        2. Supabase ê²€ìƒ‰: ì„ íƒëœ ì„¹ì…˜ì—ì„œ ë¬¸ì„œ ê²€ìƒ‰
        3. ë¬¸ì„œ ë³‘í•© ë° ì •ë ¬
        4. Elbow Method ë™ì  ì»·ì˜¤í”„ ì ìš©
        5. ë‹µë³€ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            school_name: í•™êµ ì´ë¦„ (í•„ìˆ˜)
            conversation_history: ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ [(role, content), ...]
            stream: ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì—¬ë¶€
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: config.DEFAULT_MAX_RETRIES)
            
        Returns:
            stream=False: {"answer": str, "evidence": list, "selected_sections": list}
            stream=True: generator (ë‹µë³€ ì²­í¬ ìŠ¤íŠ¸ë¦¬ë°)
        """
        if not self.supabase_searcher:
            raise ValueError("Supabase ê²€ìƒ‰ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if max_retries is None:
            max_retries = config.DEFAULT_MAX_RETRIES
        
        # 1ë‹¨ê³„: ë™ì  ë¼ìš°íŒ… - ê´€ë ¨ ì„¹ì…˜ ì„ íƒ
        section_indices, thinking_process = self.find_relevant_sections(
            question=question,
            toc_index=None,  # Supabaseì—ì„œ ì¡°íšŒ
            school_name=school_name
        )
        
        # Supabaseì—ì„œ ì„¹ì…˜ ëª©ë¡ ë‹¤ì‹œ ì¡°íšŒ (section_id í•„ìš”)
        sections = self._get_relevant_sections(school_name)
        if not sections:
            if stream:
                yield "ê´€ë ¨ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                return
            else:
                return {
                    "answer": "ê´€ë ¨ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "evidence": [],
                    "selected_sections": []
                }
        
        selected_sections = [sections[idx] for idx in section_indices if idx < len(sections)]
        section_ids = [section["id"] for section in selected_sections]
        
        # 2ë‹¨ê³„: ì„ íƒëœ ì„¹ì…˜ë³„ ê²€ìƒ‰ (ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥í•˜ì§€ë§Œ ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ìˆœì°¨ ì²˜ë¦¬)
        all_retrieved_docs_with_scores = []
        
        for section_id in section_ids:
            docs_with_scores = self.retrieve(
                question=question,
                school_name=school_name,
                section_id=section_id,
                top_k=config.TOP_K_PER_SECTION
            )
            all_retrieved_docs_with_scores.extend(docs_with_scores)
        
        # 3ë‹¨ê³„: ë¬¸ì„œ ë³‘í•© ë° ì •ë ¬
        retrieved_docs = self.merge_and_sort_docs(all_retrieved_docs_with_scores)
        
        # 4ë‹¨ê³„: Elbow Method ë™ì  ì»·ì˜¤í”„ ì ìš©
        filtered_docs = self._apply_dynamic_cutoff(retrieved_docs, min_k=5, drop_threshold=0.15)
        
        if not filtered_docs:
            if stream:
                yield "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                return
            else:
                return {
                    "answer": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "evidence": [],
                    "selected_sections": selected_sections
                }
        
        # 5ë‹¨ê³„: ë‹µë³€ ìƒì„±
        if stream:
            # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ: generator ë°˜í™˜
            answer_chunks = []
            for chunk in self.generate_answer(question, filtered_docs, conversation_history, stream=True):
                answer_chunks.append(chunk)
                yield chunk
            
            # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ë©”íƒ€ë°ì´í„° ë°˜í™˜ì€ ë¶ˆê°€ëŠ¥ (generatorë§Œ ë°˜í™˜)
            return
        else:
            # ì¼ë°˜ ëª¨ë“œ: ì „ì²´ ë‹µë³€ ë°˜í™˜
            answer = self.generate_answer(question, filtered_docs, conversation_history, stream=False)
            
            # ê·¼ê±° ë¬¸ì„œ ì •ë³´ ìˆ˜ì§‘
            evidence = []
            for doc in filtered_docs:
                evidence.append({
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "page_number": doc.metadata.get("page_number", 0),
                    "score": doc.metadata.get("score", 0.0),
                    "chunk_type": doc.metadata.get("chunk_type", "text")
                })
            
            return {
                "answer": answer,
                "evidence": evidence,
                "selected_sections": selected_sections,
                "thinking_process": thinking_process
            }
    
    def merge_and_sort_docs(self, all_retrieved_docs_with_scores: list) -> list:
        """
        ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ë³‘í•©í•˜ê³  ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì •ë ¬
        í‘œ ë¬¸ì„œì™€ í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬
        
        Args:
            all_retrieved_docs_with_scores: [(Document, score), ...] ë¦¬ìŠ¤íŠ¸
            
        Returns:
            retrieved_docs: ì •ë ¬ëœ Document ë¦¬ìŠ¤íŠ¸ (ìƒìœ„ 20ê°œ)
        """
        # 1ë‹¨ê³„: í‘œ ë¬¸ì„œì™€ í…ìŠ¤íŠ¸ ë¬¸ì„œ ë¶„ë¦¬
        table_docs_with_scores = []
        text_docs_with_scores = []
        seen_tables = set()  # í‘œ ì¤‘ë³µ ì œê±°ìš©
        
        for doc, score in all_retrieved_docs_with_scores:
            if doc.metadata.get('is_table', False):
                # í‘œ ë¬¸ì„œëŠ” ì¤‘ë³µ ì œê±°ë§Œ ìˆ˜í–‰ (ë¬¸ì„œ ë‚´ìš© ê¸°ë°˜)
                table_key = f"{doc.metadata.get('section_title', 'unknown')}_{hash(doc.page_content)}"
                if table_key not in seen_tables:
                    seen_tables.add(table_key)
                    table_docs_with_scores.append((doc, score))
            else:
                text_docs_with_scores.append((doc, score))
        
        # 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ë¬¸ì„œì˜ ì—°ì†ëœ ì²­í¬ ë³‘í•©
        merged_text_docs_with_scores = []
        seen_chunks = set()
        chunks_by_section = {}
        doc_scores_map = {}
        
        for doc, score in text_docs_with_scores:
            section_key = doc.metadata.get('section_title', 'unknown')
            chunk_type = doc.metadata.get('chunk_type', 'token')
            
            # í˜ì´ì§€ ë‹¨ìœ„ ì²­í‚¹ì¸ ê²½ìš° page_numberë¥¼ ìš°ì„  ì‚¬ìš©, ì•„ë‹ˆë©´ chunk_index ì‚¬ìš©
            if chunk_type == 'page':
                chunk_identifier = doc.metadata.get('page_number', -1)
                chunk_key = f"{section_key}_page_{chunk_identifier}"
            else:
                chunk_index = doc.metadata.get('chunk_index', -1)
                chunk_key = f"{section_key}_{chunk_index}"
            
            doc_id = id(doc)
            
            if chunk_key not in seen_chunks:
                seen_chunks.add(chunk_key)
                if section_key not in chunks_by_section:
                    chunks_by_section[section_key] = []
                chunks_by_section[section_key].append(doc)
                doc_scores_map[doc_id] = score
        
        # ì„¹ì…˜ë³„ë¡œ ì—°ì†ëœ ì²­í¬ ë³‘í•©
        for section_key, section_chunks in chunks_by_section.items():
            if not section_chunks:
                continue
            
            # ì²­í¬ íƒ€ì…ì— ë”°ë¼ ì •ë ¬ í‚¤ ì„ íƒ
            first_chunk_type = section_chunks[0].metadata.get('chunk_type', 'token') if section_chunks else 'token'
            if first_chunk_type == 'page':
                # í˜ì´ì§€ ë‹¨ìœ„ ì²­í‚¹: page_numberë¡œ ì •ë ¬
                section_chunks.sort(key=lambda x: x.metadata.get('page_number', 0))
            else:
                # í† í° ê¸°ë°˜ ì²­í‚¹: chunk_indexë¡œ ì •ë ¬
                section_chunks.sort(key=lambda x: x.metadata.get('chunk_index', 0))
            
            i = 0
            while i < len(section_chunks):
                try:
                    if i >= len(section_chunks):
                        break
                    current_chunk = section_chunks[i]
                    merged_chunks = [current_chunk]
                    
                    j = i + 1
                    chunk_type = current_chunk.metadata.get('chunk_type', 'token')
                    
                    while j < len(section_chunks):
                        try:
                            if j >= len(section_chunks):
                                break
                            next_chunk = section_chunks[j]
                            
                            # ì²­í¬ íƒ€ì…ì— ë”°ë¼ ì—°ì†ì„± í™•ì¸
                            if chunk_type == 'page':
                                # í˜ì´ì§€ ë‹¨ìœ„ ì²­í‚¹: page_numberê°€ ì—°ì†ì¸ì§€ í™•ì¸
                                current_page = current_chunk.metadata.get('page_number', -1)
                                next_page = next_chunk.metadata.get('page_number', -1)
                                is_consecutive = current_page >= 0 and next_page == current_page + 1
                            else:
                                # í† í° ê¸°ë°˜ ì²­í‚¹: chunk_indexê°€ ì—°ì†ì¸ì§€ í™•ì¸
                                current_index = current_chunk.metadata.get('chunk_index', -1)
                                next_index = next_chunk.metadata.get('chunk_index', -1)
                                is_consecutive = current_index >= 0 and next_index == current_index + 1
                            
                            if is_consecutive:
                                merged_chunks.append(next_chunk)
                                current_chunk = next_chunk
                                j += 1
                            else:
                                break
                        except (IndexError, KeyError, TypeError) as e:
                            print(f"âš ï¸  ì²­í¬ ë³‘í•© ì¤‘ ì˜¤ë¥˜ (j={j}): {e}")
                            break
                    
                    # ì—°ì†ëœ ì²­í¬ê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš° ë³‘í•©
                    if len(merged_chunks) > 1:
                        chunk_type = merged_chunks[0].metadata.get('chunk_type', 'token') if merged_chunks else 'token'
                        max_score = 0
                        
                        # í˜ì´ì§€ ë‹¨ìœ„ ì²­í‚¹ì¸ ê²½ìš° ë‹¨ìˆœ ì—°ê²°, í† í° ê¸°ë°˜ì€ overlap ë³‘í•©
                        if chunk_type == 'page':
                            # í˜ì´ì§€ ë‹¨ìœ„: ë‹¨ìˆœíˆ ë‚´ìš©ì„ ì—°ê²° (overlap ì—†ìŒ)
                            merged_content = "\n\n".join([
                                chunk.page_content if hasattr(chunk, 'page_content') else str(chunk)
                                for chunk in merged_chunks
                            ])
                            
                            # ì ìˆ˜ ê³„ì‚°
                            for chunk_doc in merged_chunks:
                                doc_id = id(chunk_doc)
                                if doc_id in doc_scores_map:
                                    max_score = max(max_score, doc_scores_map[doc_id])
                            
                            if merged_chunks and len(merged_chunks) > 0:
                                merged_doc = Document(
                                    page_content=merged_content,
                                    metadata=merged_chunks[0].metadata.copy() if hasattr(merged_chunks[0], 'metadata') else {}
                                )
                                # í˜ì´ì§€ ë²”ìœ„ ì •ë³´ ì¶”ê°€
                                page_numbers = [chunk.metadata.get('page_number', 0) for chunk in merged_chunks if chunk.metadata.get('page_number', 0) > 0]
                                if page_numbers:
                                    merged_doc.metadata['page_range'] = f"{min(page_numbers)}-{max(page_numbers)}"
                                merged_doc.metadata['merged_chunks'] = len(merged_chunks)
                                merged_text_docs_with_scores.append((merged_doc, max_score))
                        else:
                            # í† í° ê¸°ë°˜: overlap ì •ë³´ë¥¼ ì‚¬ìš©í•œ ë³‘í•©
                            chunk_data = []
                            
                            for chunk_doc in merged_chunks:
                                try:
                                    chunk_data.append({
                                        "content": chunk_doc.page_content if hasattr(chunk_doc, 'page_content') else str(chunk_doc),
                                        "start_pos": chunk_doc.metadata.get('chunk_start_pos', 0) if hasattr(chunk_doc, 'metadata') else 0,
                                        "end_pos": chunk_doc.metadata.get('chunk_end_pos', 0) if hasattr(chunk_doc, 'metadata') else 0,
                                        "overlap_prev": {
                                            "text": chunk_doc.metadata.get('overlap_prev_text', '') if hasattr(chunk_doc, 'metadata') else '',
                                            "start": chunk_doc.metadata.get('overlap_prev_start', 0) if hasattr(chunk_doc, 'metadata') else 0,
                                            "end": chunk_doc.metadata.get('overlap_prev_end', 0) if hasattr(chunk_doc, 'metadata') else 0
                                        },
                                        "overlap_next": {
                                            "text": chunk_doc.metadata.get('overlap_next_text', '') if hasattr(chunk_doc, 'metadata') else '',
                                            "start": chunk_doc.metadata.get('overlap_next_start', 0) if hasattr(chunk_doc, 'metadata') else 0,
                                            "end": chunk_doc.metadata.get('overlap_next_end', 0) if hasattr(chunk_doc, 'metadata') else 0
                                        }
                                    })
                                    doc_id = id(chunk_doc)
                                    if doc_id in doc_scores_map:
                                        max_score = max(max_score, doc_scores_map[doc_id])
                                except (AttributeError, KeyError, TypeError) as e:
                                    print(f"âš ï¸  ì²­í¬ ë°ì´í„° ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                                    continue
                            
                            if chunk_data:
                                try:
                                    merged_content = self.chunker.merge_chunks_with_overlap(chunk_data)
                                    if merged_chunks and len(merged_chunks) > 0:
                                        merged_doc = Document(
                                            page_content=merged_content,
                                            metadata=merged_chunks[0].metadata.copy() if hasattr(merged_chunks[0], 'metadata') else {}
                                        )
                                        merged_doc.metadata['merged_chunks'] = len(merged_chunks)
                                        merged_text_docs_with_scores.append((merged_doc, max_score))
                                except Exception as e:
                                    print(f"âš ï¸  ì²­í¬ ë³‘í•© ì¤‘ ì˜¤ë¥˜: {e}")
                                    # ë³‘í•© ì‹¤íŒ¨ ì‹œ ì²« ë²ˆì§¸ ì²­í¬ë§Œ ì¶”ê°€
                                    if merged_chunks and len(merged_chunks) > 0:
                                        try:
                                            doc_id = id(merged_chunks[0])
                                            score = doc_scores_map.get(doc_id, 0)
                                            merged_text_docs_with_scores.append((merged_chunks[0], score))
                                        except:
                                            pass
                    else:
                        # ë³‘í•©í•  ì²­í¬ê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì¶”ê°€
                        if merged_chunks and len(merged_chunks) > 0:
                            try:
                                doc_id = id(merged_chunks[0])
                                score = doc_scores_map.get(doc_id, 0)
                                merged_text_docs_with_scores.append((merged_chunks[0], score))
                            except (IndexError, KeyError, TypeError) as e:
                                print(f"âš ï¸  ì²­í¬ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
                    
                    i = j
                except (IndexError, KeyError, TypeError) as e:
                    print(f"âš ï¸  ì„¹ì…˜ ì²­í¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (i={i}): {e}")
                    i += 1  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ìŒìœ¼ë¡œ ì´ë™
        
        # 3ë‹¨ê³„: í‘œ ë¬¸ì„œì™€ í…ìŠ¤íŠ¸ ë¬¸ì„œ í†µí•© í›„ ì ìˆ˜ë¡œ ì •ë ¬
        all_docs_with_scores = table_docs_with_scores + merged_text_docs_with_scores
        all_docs_with_scores.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ TOP_K_FINALê°œ ì„ íƒ
        retrieved_docs_with_scores = all_docs_with_scores[:config.TOP_K_FINAL]
        
        # ì ìˆ˜ ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì €ì¥ (ë™ì  ì»·ì˜¤í”„ë¥¼ ìœ„í•´)
        for doc, score in retrieved_docs_with_scores:
            doc.metadata['similarity_score'] = score
        
        return [doc for doc, score in retrieved_docs_with_scores]
    
    # í‘œ ë¶„ì„ ë‹¨ê³„ ì œê±°: ë³„ë„ LLM í˜¸ì¶œì„ ì œê±°í•˜ê³  raw_dataë¥¼ ì§ì ‘ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨í•˜ì—¬
    # (í‘œ ë¶„ì„ í˜¸ì¶œ + ë‹µë³€ ìƒì„± í˜¸ì¶œ) -> (ë‹¨ì¼ ë‹µë³€ ìƒì„± í˜¸ì¶œ)ë¡œ í†µí•©í•˜ì—¬ ì†ë„ 2ë°° í–¥ìƒ
    
    def _apply_dynamic_cutoff(self, retrieved_docs: list, min_k: int = 5, drop_threshold: float = 0.15) -> list:
        """
        Elbow Method ê¸°ë°˜ ë™ì  ì»·ì˜¤í”„ ì ìš©
        
        [ì›ë¦¬]
        - ìƒìœ„ min_kê°œëŠ” ë¬´ì¡°ê±´ ìœ ì§€ (ì•ˆì „ì¥ì¹˜)
        - ê·¸ ì´í›„ ë¬¸ì„œë¶€í„° ì ìˆ˜ ì°¨ì´ë¥¼ ê³„ì‚°
        - ì ìˆ˜ ì°¨ì´ê°€ drop_threshold ì´ìƒ ë²Œì–´ì§€ë©´ ê·¸ ì´í›„ ë¬¸ì„œ ì œê±°
        - ê´€ë ¨ì„±ì´ ë‚®ì€ ë¬¸ì„œ ì œê±°ë¡œ TTFT í–¥ìƒ ë° í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€
        
        Args:
            retrieved_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (similarity_score ë©”íƒ€ë°ì´í„° í¬í•¨)
            min_k: ìµœì†Œ ìœ ì§€ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)
            drop_threshold: ì ìˆ˜ ê¸‰ë½ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.15)
            
        Returns:
            filtered_docs: í•„í„°ë§ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not retrieved_docs:
            return []
        
        # ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì •ë ¬ í™•ì¸ (ì´ë¯¸ ì •ë ¬ë˜ì–´ ìˆì–´ì•¼ í•¨)
        docs_with_scores = []
        for doc in retrieved_docs:
            score = doc.metadata.get('similarity_score', 0.0)
            docs_with_scores.append((doc, score))
        
        # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš°ë¥¼ ëŒ€ë¹„)
        docs_with_scores.sort(key=lambda x: x[1], reverse=True)
        
        original_count = len(docs_with_scores)
        
        # ìƒìœ„ min_kê°œëŠ” ë¬´ì¡°ê±´ ìœ ì§€
        if len(docs_with_scores) <= min_k:
            # ë¬¸ì„œ ìˆ˜ê°€ min_k ì´í•˜ë©´ ëª¨ë‘ ìœ ì§€
            filtered_docs = [doc for doc, score in docs_with_scores]
            print(f"ğŸ“Š ë™ì  ì»·ì˜¤í”„: {original_count}ê°œ ë¬¸ì„œ â†’ {len(filtered_docs)}ê°œ ìœ ì§€ (min_k={min_k} ì´í•˜)")
            return filtered_docs
        
        # min_kê°œ ì´í›„ë¶€í„° ì ìˆ˜ ì°¨ì´ í™•ì¸
        kept_docs = docs_with_scores[:min_k]  # ìƒìœ„ min_kê°œëŠ” ë¬´ì¡°ê±´ ìœ ì§€
        
        for i in range(min_k, len(docs_with_scores) - 1):
            current_doc, current_score = docs_with_scores[i]
            next_doc, next_score = docs_with_scores[i + 1]
            
            # ì ìˆ˜ ì°¨ì´ ê³„ì‚°
            score_drop = current_score - next_score
            
            # ì ìˆ˜ ì°¨ì´ê°€ ì„ê³„ê°’ ì´ìƒì´ë©´ ì—¬ê¸°ì„œ ì»·ì˜¤í”„
            if score_drop >= drop_threshold:
                print(f"ğŸ“Š ë™ì  ì»·ì˜¤í”„: {original_count}ê°œ ë¬¸ì„œ â†’ {len(kept_docs)}ê°œ ìœ ì§€ (ì¸ë±ìŠ¤ {i}ì—ì„œ {score_drop:.3f} ì ìˆ˜ ì°¨ì´ ê°ì§€)")
                break
            
            # ì ìˆ˜ ì°¨ì´ê°€ ì„ê³„ê°’ ë¯¸ë§Œì´ë©´ ìœ ì§€
            kept_docs.append((current_doc, current_score))
        else:
            # ë§ˆì§€ë§‰ ë¬¸ì„œê¹Œì§€ ì„ê³„ê°’ ë¯¸ë§Œì´ë©´ ë§ˆì§€ë§‰ ë¬¸ì„œë„ ìœ ì§€
            if len(docs_with_scores) > min_k:
                kept_docs.append(docs_with_scores[-1])
            print(f"ğŸ“Š ë™ì  ì»·ì˜¤í”„: {original_count}ê°œ ë¬¸ì„œ â†’ {len(kept_docs)}ê°œ ìœ ì§€ (ì„ê³„ê°’ ë¯¸ë§Œìœ¼ë¡œ ëª¨ë‘ ìœ ì§€)")
        
        # Documentë§Œ ë°˜í™˜ (ì ìˆ˜ëŠ” ë©”íƒ€ë°ì´í„°ì— ì´ë¯¸ ì €ì¥ë¨)
        filtered_docs = [doc for doc, score in kept_docs]
        
        return filtered_docs
    
    def generate_answer(self, question: str, retrieved_docs: list, conversation_history: list = None, stream: bool = False):
        """
        ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„± (ëŒ€í™” ì—°ì†ì„± ê³ ë ¤, ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
        í‘œëŠ” ë³„ë„ ë¶„ì„ ì—†ì´ raw_dataë¥¼ ì§ì ‘ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨í•˜ì—¬ ì†ë„ ìµœì í™”

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            retrieved_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” [(Document, score), ...] íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            conversation_history: ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ [(role, content), ...]
            stream: ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì—¬ë¶€ (Trueë©´ generator ë°˜í™˜, Falseë©´ ë¬¸ìì—´ ë°˜í™˜)

        Returns:
            answer: ìƒì„±ëœ ë‹µë³€ (stream=Falseì¼ ë•Œ) ë˜ëŠ” generator (stream=Trueì¼ ë•Œ)
        """
        if not retrieved_docs:
            if stream:
                yield "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                return
            else:
                return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # retrieved_docsê°€ íŠœí”Œ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸í•˜ê³  Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if retrieved_docs and isinstance(retrieved_docs[0], tuple):
            # [(Document, score), ...] í˜•íƒœì¸ ê²½ìš° Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            docs_list = []
            for doc, score in retrieved_docs:
                # ì ìˆ˜ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì €ì¥
                doc.metadata['similarity_score'] = score
                docs_list.append(doc)
            retrieved_docs = docs_list

        # Elbow Method ê¸°ë°˜ ë™ì  ì»·ì˜¤í”„ ì ìš© (TTFT í–¥ìƒ ë° ë…¸ì´ì¦ˆ ì œê±°)
        filtered_docs = self._apply_dynamic_cutoff(retrieved_docs, min_k=5, drop_threshold=0.15)
        
        # í•„í„°ë§ëœ ë¬¸ì„œë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        # í‘œì™€ í…ìŠ¤íŠ¸ ë¶„ë¦¬
        table_docs = [doc for doc in filtered_docs if doc.metadata.get('is_table', False)]
        text_docs = [doc for doc in filtered_docs if not doc.metadata.get('is_table', False)]
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (Dual Chunking ì „ëµ: í‘œëŠ” raw_data ì§ì ‘ ì‚¬ìš©, ë³„ë„ ë¶„ì„ ì œê±°)
        context_parts = []
        doc_counter = 1  # ì „ì²´ ë¬¸ì„œ ë²ˆí˜¸ (í‘œì™€ í…ìŠ¤íŠ¸ í†µí•©)
        
        # 1. í…ìŠ¤íŠ¸ ë¬¸ì„œ ì¶”ê°€
        for doc in text_docs:
            section_info = ""
            if 'section_title' in doc.metadata:
                section_info = f"[{doc.metadata['section_title']}] "
            
            # ë³‘í•©ëœ ì²­í¬ ì •ë³´ ì¶”ê°€
            merged_info = ""
            if doc.metadata.get('merged_chunks', 0) > 1:
                merged_info = f" [ë³‘í•©ëœ {doc.metadata['merged_chunks']}ê°œ ì²­í¬]"
            
            context_parts.append(f"ë¬¸ì„œ {doc_counter}. {section_info}{merged_info}\n{doc.page_content}\n")
            doc_counter += 1
        
        # 2. í‘œ ë¬¸ì„œ ì¶”ê°€ (Dual Chunking: raw_dataë¥¼ ì§ì ‘ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨)
        # ë³„ë„ LLM í˜¸ì¶œ ì—†ì´ ì›ë³¸ í‘œ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ ì†ë„ ìµœì í™”
        for table_doc in table_docs:
            section_info = ""
            if 'section_title' in table_doc.metadata:
                section_info = f"[{table_doc.metadata['section_title']}] "
            
            # Context Swap: raw_dataë¥¼ ì§ì ‘ ì‚¬ìš© (í‘œ ë¶„ì„ ë‹¨ê³„ ì œê±°)
            raw_data = table_doc.metadata.get('raw_data', None)
            if raw_data:
                context_parts.append(
                    f"ë¬¸ì„œ {doc_counter}. {section_info}[í‘œ ë°ì´í„° - ì›ë³¸ ë§ˆí¬ë‹¤ìš´]\n"
                    f"{raw_data}\n"
                )
            else:
                # raw_dataê°€ ì—†ëŠ” ê²½ìš° (ë ˆê±°ì‹œ í˜¸í™˜ì„±: summary ì‚¬ìš©)
                context_parts.append(
                    f"ë¬¸ì„œ {doc_counter}. {section_info}[í‘œ ë°ì´í„° - ìš”ì•½]\n"
                    f"{table_doc.page_content}\n"
                )
            doc_counter += 1
        
        context = "\n---\n".join(context_parts)
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…
        history_text = ""
        if conversation_history and len(conversation_history) > 0:
            history_parts = []
            # ìµœê·¼ 6ê°œ ëŒ€í™”ë§Œ í¬í•¨ (ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡)
            recent_history = conversation_history[-6:] if len(conversation_history) > 6 else conversation_history
            for role, content in recent_history:
                if role == "user":
                    history_parts.append(f"í•™ìƒ: {content}")
                elif role == "assistant":
                    history_parts.append(f"ì»¨ì„¤í„´íŠ¸: {content}")
            
            if history_parts:
                history_text = "\n\n**ì´ì „ ëŒ€í™” ë§¥ë½:**\n" + "\n".join(history_parts) + "\n"
        
        prompt = load_prompt("prompts/pdf-rag.yaml", encoding="utf-8")
        llm = ChatGoogleGenerativeAI(model=self.model_name, temperature=0, streaming=stream)
        chain = prompt | llm | StrOutputParser()
        
        if stream:
            # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ: generator ë°˜í™˜
            for chunk in chain.stream({
                "context": context, 
                "question": question,
                "conversation_history": history_text
            }):
                yield chunk
        else:
            # ì¼ë°˜ ëª¨ë“œ: ë¬¸ìì—´ ë°˜í™˜
            answer = chain.invoke({
                "context": context, 
                "question": question,
                "conversation_history": history_text
            })
            return answer

