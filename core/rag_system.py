"""
RAG ì‹œìŠ¤í…œ ëª¨ë“ˆ
ì§ˆì˜ì‘ë‹µ íŒŒì´í”„ë¼ì¸ ë° ê´€ë ¨ ì„¹ì…˜ ì„ íƒ
"""
import re
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_teddynote.prompts import load_prompt
from langchain.docstore.document import Document
from .searcher import SearchEngine
from .chunker import DocumentChunker
from .quality_evaluator import QualityEvaluator
import config


class RAGSystem:
    """RAG ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_name: LLM ëª¨ë¸ëª…
        """
        self.model_name = model_name or config.DEFAULT_LLM_MODEL
        self.searcher = SearchEngine()
        self.chunker = DocumentChunker()
        self.quality_evaluator = QualityEvaluator(model_name)
    
    def find_relevant_sections(self, question: str, toc_index: list) -> tuple:
        """
        ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì„¹ì…˜ì„ ì°¾ê³  ê´€ë ¨ì„±ì— ë”°ë¼ ì¬ìˆœìœ„í™”(re-rank)í•˜ëŠ” ë©”ì„œë“œ
        
        [ì‘ë™ ì›ë¦¬]
        1. LLMìœ¼ë¡œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì„¹ì…˜ ì„ íƒ
        2. ì„ íƒëœ ì„¹ì…˜ë“¤ì„ ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„±ì— ë”°ë¼ ì ìˆ˜í™”
        3. ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì¬ìˆœìœ„í™”í•˜ì—¬ ë°˜í™˜
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            toc_index: ëª©ì°¨ ì¸ë±ìŠ¤
            
        Returns:
            (section_numbers, thinking_process): ì¬ìˆœìœ„í™”ëœ ì„¹ì…˜ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ì™€ ì‚¬ê³  ê³¼ì •
        """
        sections_list = "\n".join([
            f"{idx+1}. {section.get('title', 'ì•Œ ìˆ˜ ì—†ìŒ')} (í˜ì´ì§€ {section.get('start_page', '?')}-{section.get('end_page', '?')})"
            for idx, section in enumerate(toc_index)
        ])
        
        # 1ë‹¨ê³„: ê´€ë ¨ ì„¹ì…˜ ì„ íƒ
        routing_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ëª¨ì§‘ìš”ê°• ë¬¸ì„œì˜ ëª©ì°¨ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³ , ë‹µë³€ì„ ì°¾ê¸° ìœ„í•´ ì–´ë–¤ ì„¹ì…˜ì„ í™•ì¸í•´ì•¼ í• ì§€ ìƒê°í•´ë³´ì„¸ìš”.

**ì‚¬ìš©ì ì§ˆë¬¸:**
{question}

**ë¬¸ì„œì˜ ëª©ì°¨ (ì„¹ì…˜ ëª©ë¡):**
{sections_list}

**ë¶„ì„ ê³¼ì •:**
1. ì§ˆë¬¸ì˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì–´ë–¤ ì„¹ì…˜ì„ í™•ì¸í•´ì•¼ í• ì§€ ìƒê°í•´ë³´ì„¸ìš”, ë‹¨ ì§ˆë¬¸ì˜ ë‹¨ì ì¸ ë‚´ìš© ë¿ë§Œ ì•„ë‹ˆë¼, ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ì„¸ìš”
2. ê° ì„¹ì…˜ì˜ ì œëª©ì„ ë³´ê³  ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”,ë‹¨ ì§ˆë¬¸ì˜ í‚¤ì›Œë“œë‚˜ ë‹¨ì–´ì—ë§Œ ì§‘ì¤‘í•˜ì§€ ë§ˆì„¸ìš”
3. ê´€ë ¨ ì„¹ì…˜ì„ ì„ íƒí•˜ê³  ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”

**ì¶œë ¥ í˜•ì‹:**
ì‚¬ê³  ê³¼ì •: [ì§ˆë¬¸ ë¶„ì„ ë° ì„¹ì…˜ ì„ íƒ ì´ìœ ]
ì„ íƒí•œ ì„¹ì…˜: [ì„¹ì…˜ ë²ˆí˜¸ë“¤, ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: 1, 3]

---
**ë¶„ì„:**
""")
        
        llm = ChatGoogleGenerativeAI(model=self.model_name, temperature=0)
        chain = routing_prompt | llm | StrOutputParser()
        
        response = chain.invoke({"question": question, "sections_list": sections_list})
        
        # ì‚¬ê³  ê³¼ì • ì¶”ì¶œ
        thinking_process = ""
        if "ì‚¬ê³  ê³¼ì •:" in response:
            thinking_part = response.split("ì‚¬ê³  ê³¼ì •:")[1]
            if "ì„ íƒí•œ ì„¹ì…˜:" in thinking_part:
                thinking_process = thinking_part.split("ì„ íƒí•œ ì„¹ì…˜:")[0].strip()
            else:
                thinking_process = thinking_part.strip()
        
        # ì„¹ì…˜ ë²ˆí˜¸ ì¶”ì¶œ
        section_numbers = []
        if "ì„ íƒí•œ ì„¹ì…˜:" in response:
            section_part = response.split("ì„ íƒí•œ ì„¹ì…˜:")[1]
            for num_str in re.findall(r'\d+', section_part):
                num = int(num_str)
                if 1 <= num <= len(toc_index):
                    section_numbers.append(num - 1)
        else:
            for num_str in re.findall(r'\d+', response):
                num = int(num_str)
                if 1 <= num <= len(toc_index):
                    section_numbers.append(num - 1)
        
        if not section_numbers:
            section_numbers = [0]
        
        # Re-ranking ì œê±°: Vector Searchì˜ ìœ ì‚¬ë„ ì ìˆ˜ê°€ ì´ë¯¸ ì¶©ë¶„íˆ ì •í™•í•˜ë¯€ë¡œ
        # ì¤‘ë³µëœ LLM í˜¸ì¶œì„ ì œê±°í•˜ì—¬ ì‘ë‹µ ì†ë„ ìµœì í™”
        return section_numbers, thinking_process
    
    # _rerank_sections ë©”ì„œë“œ ì œê±°: Re-ranking ë‹¨ê³„ë¥¼ ì œê±°í•˜ì—¬ LLM í˜¸ì¶œ ì ˆì•½ ë° ì‘ë‹µ ì†ë„ ìµœì í™”
    # Vector Searchì˜ ìœ ì‚¬ë„ ì ìˆ˜ê°€ ì´ë¯¸ ì¶©ë¶„íˆ ì •í™•í•˜ë¯€ë¡œ ì¤‘ë³µëœ LLM í˜¸ì¶œ ë¶ˆí•„ìš”
    
    def merge_and_sort_docs(self, all_retrieved_docs_with_scores: list) -> list:
        """
        ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ë³‘í•©í•˜ê³  ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì •ë ¬
        í‘œ ë¬¸ì„œì™€ í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬
        
        Args:
            all_retrieved_docs_with_scores: [(Document, score), ...] ë¦¬ìŠ¤íŠ¸
            
        Returns:
            retrieved_docs: ì •ë ¬ëœ Document ë¦¬ìŠ¤íŠ¸ (ìƒìœ„ 20ê°œ)
        """
        # 1ë‹¨ê³„: í‘œ ë¬¸ì„œì™€ í…ìŠ¤íŠ¸ ë¬¸ì„œ ë¶„ë¦¬
        table_docs_with_scores = []
        text_docs_with_scores = []
        seen_tables = set()  # í‘œ ì¤‘ë³µ ì œê±°ìš©
        
        for doc, score in all_retrieved_docs_with_scores:
            if doc.metadata.get('is_table', False):
                # í‘œ ë¬¸ì„œëŠ” ì¤‘ë³µ ì œê±°ë§Œ ìˆ˜í–‰ (ë¬¸ì„œ ë‚´ìš© ê¸°ë°˜)
                table_key = f"{doc.metadata.get('section_title', 'unknown')}_{hash(doc.page_content)}"
                if table_key not in seen_tables:
                    seen_tables.add(table_key)
                    table_docs_with_scores.append((doc, score))
            else:
                text_docs_with_scores.append((doc, score))
        
        # 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ë¬¸ì„œì˜ ì—°ì†ëœ ì²­í¬ ë³‘í•©
        merged_text_docs_with_scores = []
        seen_chunks = set()
        chunks_by_section = {}
        doc_scores_map = {}
        
        for doc, score in text_docs_with_scores:
            section_key = doc.metadata.get('section_title', 'unknown')
            chunk_type = doc.metadata.get('chunk_type', 'token')
            
            # í˜ì´ì§€ ë‹¨ìœ„ ì²­í‚¹ì¸ ê²½ìš° page_numberë¥¼ ìš°ì„  ì‚¬ìš©, ì•„ë‹ˆë©´ chunk_index ì‚¬ìš©
            if chunk_type == 'page':
                chunk_identifier = doc.metadata.get('page_number', -1)
                chunk_key = f"{section_key}_page_{chunk_identifier}"
            else:
                chunk_index = doc.metadata.get('chunk_index', -1)
                chunk_key = f"{section_key}_{chunk_index}"
            
            doc_id = id(doc)
            
            if chunk_key not in seen_chunks:
                seen_chunks.add(chunk_key)
                if section_key not in chunks_by_section:
                    chunks_by_section[section_key] = []
                chunks_by_section[section_key].append(doc)
                doc_scores_map[doc_id] = score
        
        # ì„¹ì…˜ë³„ë¡œ ì—°ì†ëœ ì²­í¬ ë³‘í•©
        for section_key, section_chunks in chunks_by_section.items():
            if not section_chunks:
                continue
            
            # ì²­í¬ íƒ€ì…ì— ë”°ë¼ ì •ë ¬ í‚¤ ì„ íƒ
            first_chunk_type = section_chunks[0].metadata.get('chunk_type', 'token') if section_chunks else 'token'
            if first_chunk_type == 'page':
                # í˜ì´ì§€ ë‹¨ìœ„ ì²­í‚¹: page_numberë¡œ ì •ë ¬
                section_chunks.sort(key=lambda x: x.metadata.get('page_number', 0))
            else:
                # í† í° ê¸°ë°˜ ì²­í‚¹: chunk_indexë¡œ ì •ë ¬
                section_chunks.sort(key=lambda x: x.metadata.get('chunk_index', 0))
            
            i = 0
            while i < len(section_chunks):
                try:
                    if i >= len(section_chunks):
                        break
                    current_chunk = section_chunks[i]
                    merged_chunks = [current_chunk]
                    
                    j = i + 1
                    chunk_type = current_chunk.metadata.get('chunk_type', 'token')
                    
                    while j < len(section_chunks):
                        try:
                            if j >= len(section_chunks):
                                break
                            next_chunk = section_chunks[j]
                            
                            # ì²­í¬ íƒ€ì…ì— ë”°ë¼ ì—°ì†ì„± í™•ì¸
                            if chunk_type == 'page':
                                # í˜ì´ì§€ ë‹¨ìœ„ ì²­í‚¹: page_numberê°€ ì—°ì†ì¸ì§€ í™•ì¸
                                current_page = current_chunk.metadata.get('page_number', -1)
                                next_page = next_chunk.metadata.get('page_number', -1)
                                is_consecutive = current_page >= 0 and next_page == current_page + 1
                            else:
                                # í† í° ê¸°ë°˜ ì²­í‚¹: chunk_indexê°€ ì—°ì†ì¸ì§€ í™•ì¸
                                current_index = current_chunk.metadata.get('chunk_index', -1)
                                next_index = next_chunk.metadata.get('chunk_index', -1)
                                is_consecutive = current_index >= 0 and next_index == current_index + 1
                            
                            if is_consecutive:
                                merged_chunks.append(next_chunk)
                                current_chunk = next_chunk
                                j += 1
                            else:
                                break
                        except (IndexError, KeyError, TypeError) as e:
                            print(f"âš ï¸  ì²­í¬ ë³‘í•© ì¤‘ ì˜¤ë¥˜ (j={j}): {e}")
                            break
                    
                    # ì—°ì†ëœ ì²­í¬ê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš° ë³‘í•©
                    if len(merged_chunks) > 1:
                        chunk_type = merged_chunks[0].metadata.get('chunk_type', 'token') if merged_chunks else 'token'
                        max_score = 0
                        
                        # í˜ì´ì§€ ë‹¨ìœ„ ì²­í‚¹ì¸ ê²½ìš° ë‹¨ìˆœ ì—°ê²°, í† í° ê¸°ë°˜ì€ overlap ë³‘í•©
                        if chunk_type == 'page':
                            # í˜ì´ì§€ ë‹¨ìœ„: ë‹¨ìˆœíˆ ë‚´ìš©ì„ ì—°ê²° (overlap ì—†ìŒ)
                            merged_content = "\n\n".join([
                                chunk.page_content if hasattr(chunk, 'page_content') else str(chunk)
                                for chunk in merged_chunks
                            ])
                            
                            # ì ìˆ˜ ê³„ì‚°
                            for chunk_doc in merged_chunks:
                                doc_id = id(chunk_doc)
                                if doc_id in doc_scores_map:
                                    max_score = max(max_score, doc_scores_map[doc_id])
                            
                            if merged_chunks and len(merged_chunks) > 0:
                                merged_doc = Document(
                                    page_content=merged_content,
                                    metadata=merged_chunks[0].metadata.copy() if hasattr(merged_chunks[0], 'metadata') else {}
                                )
                                # í˜ì´ì§€ ë²”ìœ„ ì •ë³´ ì¶”ê°€
                                page_numbers = [chunk.metadata.get('page_number', 0) for chunk in merged_chunks if chunk.metadata.get('page_number', 0) > 0]
                                if page_numbers:
                                    merged_doc.metadata['page_range'] = f"{min(page_numbers)}-{max(page_numbers)}"
                                merged_doc.metadata['merged_chunks'] = len(merged_chunks)
                                merged_text_docs_with_scores.append((merged_doc, max_score))
                        else:
                            # í† í° ê¸°ë°˜: overlap ì •ë³´ë¥¼ ì‚¬ìš©í•œ ë³‘í•©
                            chunk_data = []
                            
                            for chunk_doc in merged_chunks:
                                try:
                                    chunk_data.append({
                                        "content": chunk_doc.page_content if hasattr(chunk_doc, 'page_content') else str(chunk_doc),
                                        "start_pos": chunk_doc.metadata.get('chunk_start_pos', 0) if hasattr(chunk_doc, 'metadata') else 0,
                                        "end_pos": chunk_doc.metadata.get('chunk_end_pos', 0) if hasattr(chunk_doc, 'metadata') else 0,
                                        "overlap_prev": {
                                            "text": chunk_doc.metadata.get('overlap_prev_text', '') if hasattr(chunk_doc, 'metadata') else '',
                                            "start": chunk_doc.metadata.get('overlap_prev_start', 0) if hasattr(chunk_doc, 'metadata') else 0,
                                            "end": chunk_doc.metadata.get('overlap_prev_end', 0) if hasattr(chunk_doc, 'metadata') else 0
                                        },
                                        "overlap_next": {
                                            "text": chunk_doc.metadata.get('overlap_next_text', '') if hasattr(chunk_doc, 'metadata') else '',
                                            "start": chunk_doc.metadata.get('overlap_next_start', 0) if hasattr(chunk_doc, 'metadata') else 0,
                                            "end": chunk_doc.metadata.get('overlap_next_end', 0) if hasattr(chunk_doc, 'metadata') else 0
                                        }
                                    })
                                    doc_id = id(chunk_doc)
                                    if doc_id in doc_scores_map:
                                        max_score = max(max_score, doc_scores_map[doc_id])
                                except (AttributeError, KeyError, TypeError) as e:
                                    print(f"âš ï¸  ì²­í¬ ë°ì´í„° ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                                    continue
                            
                            if chunk_data:
                                try:
                                    merged_content = self.chunker.merge_chunks_with_overlap(chunk_data)
                                    if merged_chunks and len(merged_chunks) > 0:
                                        merged_doc = Document(
                                            page_content=merged_content,
                                            metadata=merged_chunks[0].metadata.copy() if hasattr(merged_chunks[0], 'metadata') else {}
                                        )
                                        merged_doc.metadata['merged_chunks'] = len(merged_chunks)
                                        merged_text_docs_with_scores.append((merged_doc, max_score))
                                except Exception as e:
                                    print(f"âš ï¸  ì²­í¬ ë³‘í•© ì¤‘ ì˜¤ë¥˜: {e}")
                                    # ë³‘í•© ì‹¤íŒ¨ ì‹œ ì²« ë²ˆì§¸ ì²­í¬ë§Œ ì¶”ê°€
                                    if merged_chunks and len(merged_chunks) > 0:
                                        try:
                                            doc_id = id(merged_chunks[0])
                                            score = doc_scores_map.get(doc_id, 0)
                                            merged_text_docs_with_scores.append((merged_chunks[0], score))
                                        except:
                                            pass
                    else:
                        # ë³‘í•©í•  ì²­í¬ê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì¶”ê°€
                        if merged_chunks and len(merged_chunks) > 0:
                            try:
                                doc_id = id(merged_chunks[0])
                                score = doc_scores_map.get(doc_id, 0)
                                merged_text_docs_with_scores.append((merged_chunks[0], score))
                            except (IndexError, KeyError, TypeError) as e:
                                print(f"âš ï¸  ì²­í¬ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
                    
                    i = j
                except (IndexError, KeyError, TypeError) as e:
                    print(f"âš ï¸  ì„¹ì…˜ ì²­í¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (i={i}): {e}")
                    i += 1  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ìŒìœ¼ë¡œ ì´ë™
        
        # 3ë‹¨ê³„: í‘œ ë¬¸ì„œì™€ í…ìŠ¤íŠ¸ ë¬¸ì„œ í†µí•© í›„ ì ìˆ˜ë¡œ ì •ë ¬
        all_docs_with_scores = table_docs_with_scores + merged_text_docs_with_scores
        all_docs_with_scores.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ TOP_K_FINALê°œ ì„ íƒ
        retrieved_docs_with_scores = all_docs_with_scores[:config.TOP_K_FINAL]
        
        # ì ìˆ˜ ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì €ì¥ (ë™ì  ì»·ì˜¤í”„ë¥¼ ìœ„í•´)
        for doc, score in retrieved_docs_with_scores:
            doc.metadata['similarity_score'] = score
        
        return [doc for doc, score in retrieved_docs_with_scores]
    
    # í‘œ ë¶„ì„ ë‹¨ê³„ ì œê±°: ë³„ë„ LLM í˜¸ì¶œì„ ì œê±°í•˜ê³  raw_dataë¥¼ ì§ì ‘ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨í•˜ì—¬
    # (í‘œ ë¶„ì„ í˜¸ì¶œ + ë‹µë³€ ìƒì„± í˜¸ì¶œ) -> (ë‹¨ì¼ ë‹µë³€ ìƒì„± í˜¸ì¶œ)ë¡œ í†µí•©í•˜ì—¬ ì†ë„ 2ë°° í–¥ìƒ
    
    def _apply_dynamic_cutoff(self, retrieved_docs: list, min_k: int = 5, drop_threshold: float = 0.15) -> list:
        """
        Elbow Method ê¸°ë°˜ ë™ì  ì»·ì˜¤í”„ ì ìš©
        
        [ì›ë¦¬]
        - ìƒìœ„ min_kê°œëŠ” ë¬´ì¡°ê±´ ìœ ì§€ (ì•ˆì „ì¥ì¹˜)
        - ê·¸ ì´í›„ ë¬¸ì„œë¶€í„° ì ìˆ˜ ì°¨ì´ë¥¼ ê³„ì‚°
        - ì ìˆ˜ ì°¨ì´ê°€ drop_threshold ì´ìƒ ë²Œì–´ì§€ë©´ ê·¸ ì´í›„ ë¬¸ì„œ ì œê±°
        - ê´€ë ¨ì„±ì´ ë‚®ì€ ë¬¸ì„œ ì œê±°ë¡œ TTFT í–¥ìƒ ë° í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€
        
        Args:
            retrieved_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (similarity_score ë©”íƒ€ë°ì´í„° í¬í•¨)
            min_k: ìµœì†Œ ìœ ì§€ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)
            drop_threshold: ì ìˆ˜ ê¸‰ë½ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.15)
            
        Returns:
            filtered_docs: í•„í„°ë§ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not retrieved_docs:
            return []
        
        # ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì •ë ¬ í™•ì¸ (ì´ë¯¸ ì •ë ¬ë˜ì–´ ìˆì–´ì•¼ í•¨)
        docs_with_scores = []
        for doc in retrieved_docs:
            score = doc.metadata.get('similarity_score', 0.0)
            docs_with_scores.append((doc, score))
        
        # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš°ë¥¼ ëŒ€ë¹„)
        docs_with_scores.sort(key=lambda x: x[1], reverse=True)
        
        original_count = len(docs_with_scores)
        
        # ìƒìœ„ min_kê°œëŠ” ë¬´ì¡°ê±´ ìœ ì§€
        if len(docs_with_scores) <= min_k:
            # ë¬¸ì„œ ìˆ˜ê°€ min_k ì´í•˜ë©´ ëª¨ë‘ ìœ ì§€
            filtered_docs = [doc for doc, score in docs_with_scores]
            print(f"ğŸ“Š ë™ì  ì»·ì˜¤í”„: {original_count}ê°œ ë¬¸ì„œ â†’ {len(filtered_docs)}ê°œ ìœ ì§€ (min_k={min_k} ì´í•˜)")
            return filtered_docs
        
        # min_kê°œ ì´í›„ë¶€í„° ì ìˆ˜ ì°¨ì´ í™•ì¸
        kept_docs = docs_with_scores[:min_k]  # ìƒìœ„ min_kê°œëŠ” ë¬´ì¡°ê±´ ìœ ì§€
        
        for i in range(min_k, len(docs_with_scores) - 1):
            current_doc, current_score = docs_with_scores[i]
            next_doc, next_score = docs_with_scores[i + 1]
            
            # ì ìˆ˜ ì°¨ì´ ê³„ì‚°
            score_drop = current_score - next_score
            
            # ì ìˆ˜ ì°¨ì´ê°€ ì„ê³„ê°’ ì´ìƒì´ë©´ ì—¬ê¸°ì„œ ì»·ì˜¤í”„
            if score_drop >= drop_threshold:
                print(f"ğŸ“Š ë™ì  ì»·ì˜¤í”„: {original_count}ê°œ ë¬¸ì„œ â†’ {len(kept_docs)}ê°œ ìœ ì§€ (ì¸ë±ìŠ¤ {i}ì—ì„œ {score_drop:.3f} ì ìˆ˜ ì°¨ì´ ê°ì§€)")
                break
            
            # ì ìˆ˜ ì°¨ì´ê°€ ì„ê³„ê°’ ë¯¸ë§Œì´ë©´ ìœ ì§€
            kept_docs.append((current_doc, current_score))
        else:
            # ë§ˆì§€ë§‰ ë¬¸ì„œê¹Œì§€ ì„ê³„ê°’ ë¯¸ë§Œì´ë©´ ë§ˆì§€ë§‰ ë¬¸ì„œë„ ìœ ì§€
            if len(docs_with_scores) > min_k:
                kept_docs.append(docs_with_scores[-1])
            print(f"ğŸ“Š ë™ì  ì»·ì˜¤í”„: {original_count}ê°œ ë¬¸ì„œ â†’ {len(kept_docs)}ê°œ ìœ ì§€ (ì„ê³„ê°’ ë¯¸ë§Œìœ¼ë¡œ ëª¨ë‘ ìœ ì§€)")
        
        # Documentë§Œ ë°˜í™˜ (ì ìˆ˜ëŠ” ë©”íƒ€ë°ì´í„°ì— ì´ë¯¸ ì €ì¥ë¨)
        filtered_docs = [doc for doc, score in kept_docs]
        
        return filtered_docs
    
    def generate_answer(self, question: str, retrieved_docs: list, conversation_history: list = None, stream: bool = False):
        """
        ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„± (ëŒ€í™” ì—°ì†ì„± ê³ ë ¤, ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
        í‘œëŠ” ë³„ë„ ë¶„ì„ ì—†ì´ raw_dataë¥¼ ì§ì ‘ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨í•˜ì—¬ ì†ë„ ìµœì í™”

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            retrieved_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            conversation_history: ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ [(role, content), ...]
            stream: ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì—¬ë¶€ (Trueë©´ generator ë°˜í™˜, Falseë©´ ë¬¸ìì—´ ë°˜í™˜)

        Returns:
            answer: ìƒì„±ëœ ë‹µë³€ (stream=Falseì¼ ë•Œ) ë˜ëŠ” generator (stream=Trueì¼ ë•Œ)
        """
        if not retrieved_docs:
            if stream:
                yield "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                return
            else:
                return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # Elbow Method ê¸°ë°˜ ë™ì  ì»·ì˜¤í”„ ì ìš© (TTFT í–¥ìƒ ë° ë…¸ì´ì¦ˆ ì œê±°)
        filtered_docs = self._apply_dynamic_cutoff(retrieved_docs, min_k=5, drop_threshold=0.15)
        
        # í•„í„°ë§ëœ ë¬¸ì„œë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        # í‘œì™€ í…ìŠ¤íŠ¸ ë¶„ë¦¬
        table_docs = [doc for doc in filtered_docs if doc.metadata.get('is_table', False)]
        text_docs = [doc for doc in filtered_docs if not doc.metadata.get('is_table', False)]
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (Dual Chunking ì „ëµ: í‘œëŠ” raw_data ì§ì ‘ ì‚¬ìš©, ë³„ë„ ë¶„ì„ ì œê±°)
        context_parts = []
        doc_counter = 1  # ì „ì²´ ë¬¸ì„œ ë²ˆí˜¸ (í‘œì™€ í…ìŠ¤íŠ¸ í†µí•©)
        
        # 1. í…ìŠ¤íŠ¸ ë¬¸ì„œ ì¶”ê°€
        for doc in text_docs:
            section_info = ""
            if 'section_title' in doc.metadata:
                section_info = f"[{doc.metadata['section_title']}] "
            
            # ë³‘í•©ëœ ì²­í¬ ì •ë³´ ì¶”ê°€
            merged_info = ""
            if doc.metadata.get('merged_chunks', 0) > 1:
                merged_info = f" [ë³‘í•©ëœ {doc.metadata['merged_chunks']}ê°œ ì²­í¬]"
            
            context_parts.append(f"ë¬¸ì„œ {doc_counter}. {section_info}{merged_info}\n{doc.page_content}\n")
            doc_counter += 1
        
        # 2. í‘œ ë¬¸ì„œ ì¶”ê°€ (Dual Chunking: raw_dataë¥¼ ì§ì ‘ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨)
        # ë³„ë„ LLM í˜¸ì¶œ ì—†ì´ ì›ë³¸ í‘œ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ ì†ë„ ìµœì í™”
        for table_doc in table_docs:
            section_info = ""
            if 'section_title' in table_doc.metadata:
                section_info = f"[{table_doc.metadata['section_title']}] "
            
            # Context Swap: raw_dataë¥¼ ì§ì ‘ ì‚¬ìš© (í‘œ ë¶„ì„ ë‹¨ê³„ ì œê±°)
            raw_data = table_doc.metadata.get('raw_data', None)
            if raw_data:
                context_parts.append(
                    f"ë¬¸ì„œ {doc_counter}. {section_info}[í‘œ ë°ì´í„° - ì›ë³¸ ë§ˆí¬ë‹¤ìš´]\n"
                    f"{raw_data}\n"
                )
            else:
                # raw_dataê°€ ì—†ëŠ” ê²½ìš° (ë ˆê±°ì‹œ í˜¸í™˜ì„±: summary ì‚¬ìš©)
                context_parts.append(
                    f"ë¬¸ì„œ {doc_counter}. {section_info}[í‘œ ë°ì´í„° - ìš”ì•½]\n"
                    f"{table_doc.page_content}\n"
                )
            doc_counter += 1
        
        context = "\n---\n".join(context_parts)
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…
        history_text = ""
        if conversation_history and len(conversation_history) > 0:
            history_parts = []
            # ìµœê·¼ 6ê°œ ëŒ€í™”ë§Œ í¬í•¨ (ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡)
            recent_history = conversation_history[-6:] if len(conversation_history) > 6 else conversation_history
            for role, content in recent_history:
                if role == "user":
                    history_parts.append(f"í•™ìƒ: {content}")
                elif role == "assistant":
                    history_parts.append(f"ì»¨ì„¤í„´íŠ¸: {content}")
            
            if history_parts:
                history_text = "\n\n**ì´ì „ ëŒ€í™” ë§¥ë½:**\n" + "\n".join(history_parts) + "\n"
        
        prompt = load_prompt("prompts/pdf-rag.yaml", encoding="utf-8")
        llm = ChatGoogleGenerativeAI(model=self.model_name, temperature=0, streaming=stream)
        chain = prompt | llm | StrOutputParser()
        
        if stream:
            # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ: generator ë°˜í™˜
            for chunk in chain.stream({
                "context": context, 
                "question": question,
                "conversation_history": history_text
            }):
                yield chunk
        else:
            # ì¼ë°˜ ëª¨ë“œ: ë¬¸ìì—´ ë°˜í™˜
            answer = chain.invoke({
                "context": context, 
                "question": question,
                "conversation_history": history_text
            })
            return answer

