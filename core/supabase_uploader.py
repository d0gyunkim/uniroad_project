"""
Supabase ì—…ë¡œë” ëª¨ë“ˆ
ì „ì²˜ë¦¬ëœ PDF ë°ì´í„°ë¥¼ Supabase (PostgreSQL + pgvector)ì— ì—…ë¡œë“œ
"""
import os
from typing import Dict, List, Optional, Any
from dotenv import load_dotenv
from supabase import create_client, Client
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.docstore.document import Document
import config

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(override=True)


class SupabaseUploader:
    """Supabaseì— ë¬¸ì„œ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        """Supabase í´ë¼ì´ì–¸íŠ¸ì™€ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        # Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        
        if not supabase_url or not supabase_key:
            raise ValueError(
                "Supabase í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
                ".env íŒŒì¼ì— SUPABASE_URLê³¼ SUPABASE_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”."
            )
        
        self.supabase: Client = create_client(supabase_url, supabase_key)
        print("âœ… Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        embedding_model = config.DEFAULT_EMBEDDING_MODEL
        embedding_kwargs = {
            "request_timeout": 600,
            "batch_size": 100,
            "max_retries": 10,
            "retry_delay": 15
        }
        # ëª¨ë¸ì´ Noneì´ ì•„ë‹ˆë©´ ì¶”ê°€
        if embedding_model:
            embedding_kwargs["model"] = embedding_model
        
        self.embeddings = GoogleGenerativeAIEmbeddings(**embedding_kwargs)
        print("âœ… GoogleGenerativeAIEmbeddings ì´ˆê¸°í™” ì™„ë£Œ")
    
    def upload_to_supabase(
        self,
        school_name: str,
        file_path: str,
        processed_data: Dict[str, Any]
    ) -> Optional[int]:
        """
        ì „ì²˜ë¦¬ëœ PDF ë°ì´í„°ë¥¼ Supabaseì— ì—…ë¡œë“œ
        
        Args:
            school_name: í•™êµ ì´ë¦„
            file_path: PDF íŒŒì¼ ê²½ë¡œ
            processed_data: ì „ì²˜ë¦¬ëœ ë°ì´í„°
                - toc_sections: ëª©ì°¨ ë¦¬ìŠ¤íŠ¸ [{"title": "...", "start_page": X, "end_page": Y}, ...]
                - chunks: LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            document_id: ìƒì„±ëœ ë¬¸ì„œ ID (ì‹¤íŒ¨ ì‹œ None)
        """
        try:
            # processed_data ê²€ì¦
            if not processed_data:
                raise ValueError("processed_dataê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            
            toc_sections = processed_data.get("toc_sections", [])
            chunks = processed_data.get("chunks", [])
            
            if not chunks:
                print("âš ï¸  ì—…ë¡œë“œí•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            filename = os.path.basename(file_path)
            
            print(f"\nğŸ“¤ Supabase ì—…ë¡œë“œ ì‹œì‘: {school_name} - {filename}")
            print(f"   ì„¹ì…˜ ìˆ˜: {len(toc_sections)}ê°œ")
            print(f"   ì²­í¬ ìˆ˜: {len(chunks)}ê°œ")
            
            # Step 1: documents í…Œì´ë¸”ì— ë¬¸ì„œ ë“±ë¡
            print("\n[Step 1] documents í…Œì´ë¸”ì— ë¬¸ì„œ ë“±ë¡ ì¤‘...")
            document_id = self._insert_document(school_name, filename, file_path)
            if not document_id:
                raise Exception("ë¬¸ì„œ ë“±ë¡ ì‹¤íŒ¨")
            print(f"   âœ… ë¬¸ì„œ ë“±ë¡ ì™„ë£Œ (ID: {document_id})")
            
            # Step 2: document_sections í…Œì´ë¸”ì— ì„¹ì…˜ ë“±ë¡
            print("\n[Step 2] document_sections í…Œì´ë¸”ì— ì„¹ì…˜ ë“±ë¡ ì¤‘...")
            section_map = self._insert_sections(document_id, toc_sections)
            print(f"   âœ… ì„¹ì…˜ ë“±ë¡ ì™„ë£Œ ({len(section_map)}ê°œ ì„¹ì…˜)")
            
            # Step 3: ì„ë² ë”© ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬)
            print("\n[Step 3] ì„ë² ë”© ìƒì„± ì¤‘...")
            embeddings_list = self._generate_embeddings(chunks)
            print(f"   âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ ({len(embeddings_list)}ê°œ)")
            
            # Step 4: document_chunks í…Œì´ë¸”ì— ì²­í¬ ë“±ë¡ (ë°°ì¹˜ ì²˜ë¦¬)
            print("\n[Step 4] document_chunks í…Œì´ë¸”ì— ì²­í¬ ë“±ë¡ ì¤‘...")
            chunks_inserted = self._insert_chunks(
                document_id,
                section_map,
                chunks,
                embeddings_list
            )
            print(f"   âœ… ì²­í¬ ë“±ë¡ ì™„ë£Œ ({chunks_inserted}ê°œ)")
            
            print(f"\nğŸ‰ Supabase ì—…ë¡œë“œ ì™„ë£Œ! (ë¬¸ì„œ ID: {document_id})")
            return document_id
            
        except Exception as e:
            print(f"\nâŒ Supabase ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            print(f"ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
            return None
    
    def _insert_document(
        self,
        school_name: str,
        filename: str,
        file_path: str
    ) -> Optional[int]:
        """
        documents í…Œì´ë¸”ì— ë¬¸ì„œ ë“±ë¡
        
        Args:
            school_name: í•™êµ ì´ë¦„
            filename: íŒŒì¼ëª…
            file_path: íŒŒì¼ ê²½ë¡œ
        
        Returns:
            document_id: ìƒì„±ëœ ë¬¸ì„œ ID
        """
        try:
            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            metadata = {
                "file_path": file_path,
                "uploaded_at": str(os.path.getmtime(file_path)) if os.path.exists(file_path) else None
            }
            
            # documents í…Œì´ë¸”ì— ì‚½ì…
            response = self.supabase.table("documents").insert({
                "school_name": school_name,
                "filename": filename,
                "metadata": metadata
            }).execute()
            
            if not response.data or len(response.data) == 0:
                raise Exception("ë¬¸ì„œ ì‚½ì… ì‹¤íŒ¨: ì‘ë‹µ ë°ì´í„° ì—†ìŒ")
            
            document_id = response.data[0]["id"]
            return document_id
            
        except Exception as e:
            print(f"   âŒ ë¬¸ì„œ ë“±ë¡ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def _insert_sections(
        self,
        document_id: int,
        toc_sections: List[Dict[str, Any]]
    ) -> Dict[str, int]:
        """
        document_sections í…Œì´ë¸”ì— ì„¹ì…˜ ë“±ë¡
        
        Args:
            document_id: ë¬¸ì„œ ID
            toc_sections: ëª©ì°¨ ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            section_map: {section_name: section_id} ë”•ì…”ë„ˆë¦¬
        """
        section_map = {}
        
        if not toc_sections:
            print("   âš ï¸  ë“±ë¡í•  ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
            return section_map
        
        try:
            # ì„¹ì…˜ ë°ì´í„° ì¤€ë¹„
            sections_data = []
            for section in toc_sections:
                section_name = section.get("title", "ì•Œ ìˆ˜ ì—†ìŒ")
                page_start = section.get("start_page", 1)
                page_end = section.get("end_page", 1)
                
                sections_data.append({
                    "document_id": document_id,
                    "section_name": section_name,
                    "page_start": page_start,
                    "page_end": page_end
                })
            
            # ë°°ì¹˜ ì‚½ì…
            response = self.supabase.table("document_sections").insert(
                sections_data
            ).execute()
            
            if not response.data:
                raise Exception("ì„¹ì…˜ ì‚½ì… ì‹¤íŒ¨: ì‘ë‹µ ë°ì´í„° ì—†ìŒ")
            
            # section_map ìƒì„±
            for section_data in response.data:
                section_name = section_data["section_name"]
                section_id = section_data["id"]
                section_map[section_name] = section_id
            
            return section_map
            
        except Exception as e:
            print(f"   âŒ ì„¹ì…˜ ë“±ë¡ ì˜¤ë¥˜: {str(e)}")
            return section_map
    
    def _generate_embeddings(self, chunks: List[Document]) -> List[List[float]]:
        """
        ì²­í¬ë“¤ì˜ ì„ë² ë”© ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬)
        
        Args:
            chunks: LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            embeddings_list: ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        try:
            # page_content ì¶”ì¶œ
            texts = [chunk.page_content for chunk in chunks]
            
            # ë°°ì¹˜ ì„ë² ë”© ìƒì„±
            print(f"   ğŸ“Š {len(texts)}ê°œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘...")
            embeddings_list = self.embeddings.embed_documents(texts)
            
            # ì„ë² ë”© ì°¨ì› ê²€ì¦ (768ì°¨ì›)
            if embeddings_list and len(embeddings_list) > 0:
                embedding_dim = len(embeddings_list[0])
                if embedding_dim != 768:
                    print(f"   âš ï¸  ì„ë² ë”© ì°¨ì›ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤: {embedding_dim} (ì˜ˆìƒ: 768)")
            
            return embeddings_list
            
        except Exception as e:
            print(f"   âŒ ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {str(e)}")
            raise
    
    def _insert_chunks(
        self,
        document_id: int,
        section_map: Dict[str, int],
        chunks: List[Document],
        embeddings_list: List[List[float]]
    ) -> int:
        """
        document_chunks í…Œì´ë¸”ì— ì²­í¬ ë“±ë¡ (100ê°œ ë‹¨ìœ„ ë°°ì¹˜ ì²˜ë¦¬)
        
        Args:
            document_id: ë¬¸ì„œ ID
            section_map: {section_name: section_id} ë”•ì…”ë„ˆë¦¬
            chunks: LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
            embeddings_list: ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        
        Returns:
            chunks_inserted: ì‚½ì…ëœ ì²­í¬ ìˆ˜
        """
        if len(chunks) != len(embeddings_list):
            raise ValueError(
                f"ì²­í¬ ìˆ˜({len(chunks)})ì™€ ì„ë² ë”© ìˆ˜({len(embeddings_list)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            )
        
        chunks_inserted = 0
        batch_size = 100
        
        # ì²­í¬ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for batch_start in range(0, len(chunks), batch_size):
            batch_end = min(batch_start + batch_size, len(chunks))
            batch_chunks = chunks[batch_start:batch_end]
            batch_embeddings = embeddings_list[batch_start:batch_end]
            
            try:
                # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
                chunks_data = []
                
                for idx, (chunk, embedding) in enumerate(zip(batch_chunks, batch_embeddings)):
                    # ì„¹ì…˜ ID ì°¾ê¸°
                    section_name = chunk.metadata.get("section_title", "ì•Œ ìˆ˜ ì—†ìŒ")
                    section_id = section_map.get(section_name)
                    
                    if not section_id:
                        print(f"   âš ï¸  ì„¹ì…˜ '{section_name}'ì— ëŒ€í•œ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì²­í¬ {batch_start + idx + 1})")
                        # ì„¹ì…˜ IDê°€ ì—†ìœ¼ë©´ Noneìœ¼ë¡œ ì„¤ì • (FK ì œì•½ ì¡°ê±´ì— ë”°ë¼ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŒ)
                        # ë˜ëŠ” ê¸°ë³¸ ì„¹ì…˜ì„ ì°¾ê±°ë‚˜ ìƒì„±í•´ì•¼ í•  ìˆ˜ ìˆìŒ
                        continue
                    
                    # Dual Chunking ë¡œì§ ì ìš©
                    content = chunk.page_content  # ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸
                    raw_data = chunk.metadata.get("raw_data")  # ë‹µë³€ìš© ì›ë³¸ ë°ì´í„°
                    
                    # raw_dataê°€ ì—†ìœ¼ë©´ page_content ì‚¬ìš©
                    if not raw_data:
                        raw_data = content
                    
                    # í˜ì´ì§€ ë²ˆí˜¸
                    page_number = chunk.metadata.get("page_number", 0)
                    
                    # ì²­í¬ íƒ€ì… ê²°ì •
                    chunk_type = chunk.metadata.get("chunk_type", "unknown")
                    if not chunk_type or chunk_type == "unknown":
                        # type ë©”íƒ€ë°ì´í„°ë¡œ íŒë‹¨
                        doc_type = chunk.metadata.get("type", "text")
                        if doc_type == "table":
                            chunk_type = "table"
                        else:
                            chunk_type = "text"
                    
                    chunks_data.append({
                        "document_id": document_id,
                        "section_id": section_id,
                        "content": content,
                        "raw_data": raw_data,
                        "embedding": embedding,  # pgvector í˜•ì‹ìœ¼ë¡œ ìë™ ë³€í™˜ë¨
                        "page_number": page_number,
                        "chunk_type": chunk_type
                    })
                
                if not chunks_data:
                    print(f"   âš ï¸  ë°°ì¹˜ {batch_start // batch_size + 1}ì— ì‚½ì…í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                # ë°°ì¹˜ ì‚½ì…
                response = self.supabase.table("document_chunks").insert(
                    chunks_data
                ).execute()
                
                if response.data:
                    chunks_inserted += len(response.data)
                    print(f"   ğŸ“¦ ë°°ì¹˜ {batch_start // batch_size + 1} ì‚½ì… ì™„ë£Œ ({len(response.data)}ê°œ)")
                else:
                    print(f"   âš ï¸  ë°°ì¹˜ {batch_start // batch_size + 1} ì‚½ì… ì‹¤íŒ¨: ì‘ë‹µ ë°ì´í„° ì—†ìŒ")
                
            except Exception as e:
                print(f"   âŒ ë°°ì¹˜ {batch_start // batch_size + 1} ì‚½ì… ì˜¤ë¥˜: {str(e)}")
                import traceback
                print(f"   ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
                continue
        
        return chunks_inserted


def upload_to_supabase(
    school_name: str,
    file_path: str,
    processed_data: Dict[str, Any]
) -> Optional[int]:
    """
    Supabaseì— ì „ì²˜ë¦¬ëœ PDF ë°ì´í„° ì—…ë¡œë“œ (í¸ì˜ í•¨ìˆ˜)
    
    Args:
        school_name: í•™êµ ì´ë¦„
        file_path: PDF íŒŒì¼ ê²½ë¡œ
        processed_data: ì „ì²˜ë¦¬ëœ ë°ì´í„°
            - toc_sections: ëª©ì°¨ ë¦¬ìŠ¤íŠ¸
            - chunks: LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        document_id: ìƒì„±ëœ ë¬¸ì„œ ID (ì‹¤íŒ¨ ì‹œ None)
    """
    uploader = SupabaseUploader()
    return uploader.upload_to_supabase(school_name, file_path, processed_data)

