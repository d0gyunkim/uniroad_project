import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_teddynote.prompts import load_prompt
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_community.vectorstores import FAISS
from langchain_core.runnables import RunnablePassthrough
from langchain_teddynote import logging
from dotenv import load_dotenv
import os
from pathlib import Path

# API KEY ì •ë³´ë¡œë“œ
load_dotenv()

# GEMINI_API_KEYë¥¼ GOOGLE_API_KEYë¡œ ë§¤í•‘
if os.getenv("GEMINI_API_KEY"):
    os.environ["GOOGLE_API_KEY"] = os.getenv("GEMINI_API_KEY")

# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.
logging.langsmith("[Project] Routed Agent RAG - Gemini")

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists(".cache"):
    os.mkdir(".cache")

if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")

if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")

st.title("ğŸ¯ ë¼ìš°íŒ… ê¸°ë°˜ ì…ì‹œ ì»¨ì„¤í„´íŠ¸")

# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ
if "messages" not in st.session_state:
    st.session_state["messages"] = []

if "agents" not in st.session_state:
    st.session_state["agents"] = None

# ì‚¬ì´ë“œë°” ìƒì„±
with st.sidebar:
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")
    
    # ë¶„í•  PDF í´ë” ê²½ë¡œ
    pdf_folder = st.text_input(
        "ë¶„í•  PDF í´ë” ê²½ë¡œ",
        value="ê²½í¬ëŒ€_ìˆ˜ì‹œìš”ê°•_ë¶„í• í‘œ"
    )
    
    load_btn = st.button("PDF ë¡œë“œ ë° ì—ì´ì „íŠ¸ ìƒì„±")
    
    selected_model = "gemini-2.5-flash-lite"
    
    st.markdown("---")
    st.markdown("### ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ")
    st.markdown("1. ì§ˆë¬¸ ë¶„ì„ (ë¼ìš°íŒ…)")
    st.markdown("2. ê´€ë ¨ ë¬¸ì„œ ìë™ ì„ íƒ")
    st.markdown("3. ê° ë¬¸ì„œë³„ ì—ì´ì „íŠ¸ ë‹µë³€")
    st.markdown("4. ì½”ë””ë„¤ì´í„°ê°€ ì •ë³´ ì¢…í•©")
    st.markdown("")
    st.markdown("âš¡ íš¨ìœ¨ + ì™„ì „í•œ ì •ë³´ í†µí•©")
    
    st.markdown("---")
    st.markdown("### âš™ï¸ ì‹œìŠ¤í…œ ìƒíƒœ")
    if st.session_state["agents"]:
        st.success(f"âœ… {len(st.session_state['agents'])}ê°œ ë¬¸ì„œ ë¡œë“œ")
    else:
        st.warning("â³ ë¬¸ì„œ ëŒ€ê¸° ì¤‘")

# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥
def print_messages():
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)

# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€
def add_message(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))

# ë‹¨ì¼ PDFë¥¼ ì„ë² ë”©í•˜ëŠ” í•¨ìˆ˜
def embed_single_pdf(pdf_path, pdf_name):
    """
    ë‹¨ì¼ PDFë¥¼ ì„ë² ë”©í•˜ì—¬ retriever ë°˜í™˜
    """
    # ë¬¸ì„œ ë¡œë“œ
    loader = PDFPlumberLoader(pdf_path)
    docs = loader.load()
    
    # ë¬¸ì„œ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=3000,
        chunk_overlap=600,
        length_function=len,
        separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
    )
    split_documents = text_splitter.split_documents(docs)
    
    # ì„ë² ë”© ìƒì„±
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    
    # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)
    
    # ê²€ìƒ‰ê¸° ìƒì„± (ë¼ìš°íŒ…ìš© - ë¹ ë¥¸ ê²€ìƒ‰)
    retriever_routing = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 3}  # ë¼ìš°íŒ…ìš©ì€ ì ì€ ìˆ˜
    )
    
    # ê²€ìƒ‰ê¸° ìƒì„± (ë‹µë³€ìš© - ìƒì„¸ ê²€ìƒ‰)
    retriever_answer = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": 20,
            "fetch_k": 50,
            "lambda_mult": 0.8
        }
    )
    
    return {
        "name": pdf_name,
        "path": pdf_path,
        "retriever_routing": retriever_routing,
        "retriever_answer": retriever_answer,
        "vectorstore": vectorstore
    }

# ë©€í‹° ì—ì´ì „íŠ¸ ë¡œë“œ (ìºì‹œ ë²„ì „ 2 - retriever_answer í¬í•¨)
@st.cache_resource(show_spinner="ë¬¸ì„œ ë¡œë“œ ì¤‘...")
def load_agents(folder_path, _cache_version=2):
    """
    í´ë” ë‚´ ëª¨ë“  PDFë¥¼ ë¡œë“œí•˜ì—¬ ì—ì´ì „íŠ¸ ìƒì„±
    """
    folder = Path(folder_path)
    
    if not folder.exists():
        st.error(f"í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}")
        return None
    
    # PDF íŒŒì¼ ì°¾ê¸°
    pdf_files = list(folder.glob("*.pdf"))
    
    if not pdf_files:
        st.error(f"í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {folder_path}")
        return None
    
    st.info(f"ğŸ“„ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ ë°œê²¬")
    
    agents = []
    
    # ê° PDFì— ëŒ€í•´ ì—ì´ì „íŠ¸ ìƒì„±
    for idx, pdf_file in enumerate(pdf_files, 1):
        with st.spinner(f"ë¬¸ì„œ {idx}/{len(pdf_files)} ì²˜ë¦¬ ì¤‘... ({pdf_file.name})"):
            agent = embed_single_pdf(str(pdf_file), pdf_file.stem)
            agents.append(agent)
            st.success(f"âœ… ë¬¸ì„œ {idx} ì¤€ë¹„ ì™„ë£Œ: {pdf_file.name}")
    
    return agents

# ë¼ìš°í„°: ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ìˆëŠ” ë¬¸ì„œ ì°¾ê¸°
def route_to_relevant_docs(question, agents, top_k=2):
    """
    ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ìˆëŠ” ë¬¸ì„œë¥¼ ì°¾ëŠ” ë¼ìš°í„°
    """
    st.info("ğŸ” ì§ˆë¬¸ ë¶„ì„ ë° ê´€ë ¨ ë¬¸ì„œ íƒìƒ‰ ì¤‘...")
    
    doc_scores = []
    
    # ê° ë¬¸ì„œì—ì„œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
    for agent in agents:
        # ë¹ ë¥¸ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ì„± í™•ì¸
        docs = agent["retriever_routing"].get_relevant_documents(question)
        
        # ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜ì™€ ë‚´ìš© ê¸¸ì´ë¡œ ì ìˆ˜ ê³„ì‚°
        if docs:
            # ê´€ë ¨ì„± ì ìˆ˜ = ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜ + í‰ê·  ë¬¸ì„œ ê¸¸ì´
            avg_length = sum(len(doc.page_content) for doc in docs) / len(docs)
            score = len(docs) * avg_length
        else:
            score = 0
        
        doc_scores.append({
            "agent": agent,
            "score": score,
            "sample_content": docs[0].page_content[:200] if docs else ""
        })
    
    # ì ìˆ˜ë¡œ ì •ë ¬
    doc_scores.sort(key=lambda x: x["score"], reverse=True)
    
    # ìƒìœ„ top_k ë¬¸ì„œ ì„ íƒ
    selected_agents = [item["agent"] for item in doc_scores[:top_k]]
    
    # ì„ íƒëœ ë¬¸ì„œ í‘œì‹œ
    with st.expander("ğŸ“‹ ì„ íƒëœ ê´€ë ¨ ë¬¸ì„œ"):
        for idx, item in enumerate(doc_scores[:top_k], 1):
            st.markdown(f"**{idx}. {item['agent']['name']}**")
            st.markdown(f"ê´€ë ¨ì„± ì ìˆ˜: {item['score']:.0f}")
            st.markdown(f"ìƒ˜í”Œ: {item['sample_content'][:150]}...")
            st.markdown("---")
    
    return selected_agents

# ë‹¨ì¼ ì—ì´ì „íŠ¸ ë‹µë³€ ìƒì„±
def create_agent_answer(question, agent, model_name="gemini-2.5-flash-lite"):
    """
    ë‹¨ì¼ ì—ì´ì „íŠ¸ê°€ ìì‹ ì˜ ë¬¸ì„œì—ì„œ ë‹µë³€ ìƒì„±
    """
    # ì—ì´ì „íŠ¸ êµ¬ì¡° í™•ì¸
    if "retriever_answer" not in agent:
        raise ValueError(f"ì—ì´ì „íŠ¸ '{agent.get('name', 'unknown')}'ì— 'retriever_answer'ê°€ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë‹¤ì‹œ ë¡œë“œí•´ì£¼ì„¸ìš”.")
    
    # ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰
    docs = agent["retriever_answer"].get_relevant_documents(question)
    context = "\n\n".join([doc.page_content for doc in docs])
    
    # ì—ì´ì „íŠ¸ ì „ìš© í”„ë¡¬í”„íŠ¸
    from langchain_teddynote.prompts import load_prompt
    prompt = load_prompt("prompts/pdf-rag-agent.yaml", encoding="utf-8")
    
    llm = ChatGoogleGenerativeAI(model=model_name, temperature=0)
    
    # ì²´ì¸ êµ¬ì„±
    chain = prompt | llm | StrOutputParser()
    
    # í”„ë¡¬í”„íŠ¸ì— í•„ìš”í•œ ë³€ìˆ˜ ì „ë‹¬
    response = chain.invoke({
        "context": context,
        "question": question
    })
    
    return response

# ì„ íƒëœ ë¬¸ì„œì—ì„œ ê° ì—ì´ì „íŠ¸ ë‹µë³€ ìƒì„±
def generate_answers_from_selected_agents(question, selected_agents, model_name="gemini-2.5-flash-lite"):
    """
    ì„ íƒëœ ê° ë¬¸ì„œì— ëŒ€í•´ ë…ë¦½ì ì¸ ì—ì´ì „íŠ¸ê°€ ë‹µë³€ ìƒì„±
    """
    st.info(f"ğŸ¤– ì„ íƒëœ {len(selected_agents)}ê°œ ë¬¸ì„œì—ì„œ ê°ê° ë‹µë³€ ìƒì„± ì¤‘...")
    
    agent_responses = []
    
    # ê° ì—ì´ì „íŠ¸ê°€ ë…ë¦½ì ìœ¼ë¡œ ë‹µë³€
    for idx, agent in enumerate(selected_agents, 1):
        with st.spinner(f"ì—ì´ì „íŠ¸ {idx}/{len(selected_agents)} ë‹µë³€ ìƒì„± ì¤‘... ({agent['name']})"):
            response = create_agent_answer(question, agent, model_name)
            agent_responses.append({
                "agent_name": agent["name"],
                "response": response
            })
            st.success(f"âœ… ì—ì´ì „íŠ¸ {idx} ë‹µë³€ ì™„ë£Œ: {agent['name']}")
    
    return agent_responses

# ì½”ë””ë„¤ì´í„°: ì—ì´ì „íŠ¸ ì •ë³´ ì¢…í•©
def synthesize_agent_answers(question, agent_responses, model_name="gemini-2.5-flash-lite"):
    """
    ì—¬ëŸ¬ ì—ì´ì „íŠ¸ì˜ ë‹µë³€ì„ ì¢…í•©í•˜ì—¬ ì™„ì „í•œ ë‹µë³€ ìƒì„±
    """
    st.info("ğŸ¯ ì½”ë””ë„¤ì´í„°ê°€ ì •ë³´ ì¢…í•© ì¤‘...")
    
    # ì—ì´ì „íŠ¸ ë‹µë³€ë“¤ í‘œì‹œ (í¼ì¹¨ ê°€ëŠ¥)
    with st.expander("ğŸ” ê° ì—ì´ì „íŠ¸ ë‹µë³€ ë³´ê¸°"):
        for idx, response in enumerate(agent_responses, 1):
            st.markdown(f"**[ì—ì´ì „íŠ¸ {idx}: {response['agent_name']}]**")
            st.markdown(response['response'])
            st.markdown("---")
    
    # ì¢…í•© í”„ë¡¬í”„íŠ¸
    synthesizer_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì´ë©°, ì—¬ëŸ¬ ì—ì´ì „íŠ¸ì˜ ì •ë³´ë¥¼ ì¢…í•©í•˜ëŠ” ì½”ë””ë„¤ì´í„°ì…ë‹ˆë‹¤.

ê° ì—ì´ì „íŠ¸ê°€ ìì‹ ì˜ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í–ˆìŠµë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì—­í• ì€ ì´ ë‹µë³€ë“¤ì„ ì¢…í•©í•˜ì—¬ í•™ìƒì—ê²Œ ì™„ì „í•˜ê³  ì²´ê³„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

**ì •ë³´ ì¶œì²˜ ê·œì¹™ (ì—„ê²©):**
1. ì˜¤ì§ ì—ì´ì „íŠ¸ë“¤ì´ ë¬¸ì„œì—ì„œ ì°¾ì€ êµ¬ì²´ì ì¸ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
2. ì—ì´ì „íŠ¸ê°€ "ì°¾ì„ ìˆ˜ ì—†ë‹¤"ê³  í•œ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
3. ì™¸ë¶€ ì§€ì‹ìœ¼ë¡œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ì •ë³´ë¥¼ ë³´ì¶©í•˜ì§€ ë§ˆì„¸ìš”
4. ì—¬ëŸ¬ ì—ì´ì „íŠ¸ì˜ ì •ë³´ë¥¼ í†µí•©í•˜ë˜, ìˆ«ì ì •ë³´ëŠ” ì ˆëŒ€ ì„ì˜ë¡œ ë”í•˜ì§€ ë§ê³ , ê° ì—ì´ì „íŠ¸ì˜ ë‹µë³€ì— ìˆëŠ” ìˆ«ìë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

**ì¢…í•© ê·œì¹™:**
1. **ì¤‘ë³µ ì œê±°**: ê°™ì€ ì •ë³´ê°€ ì—¬ëŸ¬ ì—ì´ì „íŠ¸ì— ìˆìœ¼ë©´ í•œ ë²ˆë§Œ ì–¸ê¸‰í•˜ì„¸ìš”
2. **ì •ë³´ í†µí•©**: ë³´ì™„ì ì¸ ì •ë³´ëŠ” ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ í†µí•©í•˜ì„¸ìš”
3. **ì²´ê³„ì  êµ¬ì„±**: ì •ë³´ë¥¼ ë…¼ë¦¬ì  ìˆœì„œë¡œ ì •ë¦¬í•˜ì„¸ìš” (ì˜ˆ: ì „í˜• ê°œìš” â†’ ì„¸ë¶€ì‚¬í•­ â†’ ì¡°ê±´)
4. **ì™„ì „ì„±**: ëª¨ë“  ì—ì´ì „íŠ¸ì˜ ê´€ë ¨ ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ í¬í•¨í•˜ì„¸ìš”
5. **ëª…í™•ì„±**: í•™ìƒê³¼ í•™ë¶€ëª¨ê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”

**ë‹µë³€ êµ¬ì„±:**
1. ì§ˆë¬¸ì— ëŒ€í•œ í•µì‹¬ ë‹µë³€ì„ ë¨¼ì € ì œì‹œí•˜ì„¸ìš”
2. ì„¸ë¶€ ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš” (ëª©ë¡, í‘œ ë“± í™œìš©)
3. ì—¬ëŸ¬ ì—ì´ì „íŠ¸ì—ì„œ ë‚˜ì˜¨ ë³´ì™„ ì •ë³´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©í•˜ì„¸ìš”
4. ì •ë³´ê°€ ë¶€ì¡±í•œ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ëª…ì‹œí•˜ì„¸ìš”

**ì¤‘ìš”:**
- ì—ì´ì „íŠ¸ë“¤ì˜ ì •ë³´ë¥¼ ìˆëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜, ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©í•˜ì„¸ìš”
- ìƒˆë¡œìš´ ì •ë³´ë¥¼ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”
- ëª¨ë“  êµ¬ì²´ì  ì •ë³´(ìˆ«ì, ë‚ ì§œ, ì´ë¦„ ë“±)ëŠ” ì—ì´ì „íŠ¸ ë‹µë³€ì—ì„œë§Œ ê°€ì ¸ì˜¤ì„¸ìš”

---

**í•™ìƒ ì§ˆë¬¸:**
{question}

---

**ì—ì´ì „íŠ¸ ë‹µë³€ë“¤:**

{agent_responses}

---

**ì¢…í•© ë‹µë³€:**
(ì—¬ëŸ¬ ì—ì´ì „íŠ¸ì˜ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì™„ì „í•˜ê³  ì²´ê³„ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”)
""")
    
    # ì—ì´ì „íŠ¸ ë‹µë³€ë“¤ì„ í¬ë§·íŒ…
    formatted_responses = "\n\n".join([
        f"[ì—ì´ì „íŠ¸ {idx}: {resp['agent_name']}]\n{resp['response']}"
        for idx, resp in enumerate(agent_responses, 1)
    ])
    
    llm = ChatGoogleGenerativeAI(model=model_name, temperature=0)
    chain = synthesizer_prompt | llm | StrOutputParser()
    
    synthesized_response = chain.invoke({
        "question": question,
        "agent_responses": formatted_responses
    })
    
    return synthesized_response

# PDF ë¡œë“œ ë²„íŠ¼ì´ ëˆŒë¦¬ë©´
if load_btn:
    # ìºì‹œ ì´ˆê¸°í™” (ì´ì „ ë²„ì „ ì œê±°)
    if hasattr(st, 'cache_resource'):
        # ìºì‹œ í´ë¦¬ì–´ë¥¼ ìœ„í•œ í”Œë˜ê·¸
        st.session_state["clear_cache"] = True
    
    agents = load_agents(pdf_folder)
    if agents:
        # ì—ì´ì „íŠ¸ êµ¬ì¡° ê²€ì¦
        for agent in agents:
            if "retriever_answer" not in agent:
                st.error(f"ì—ì´ì „íŠ¸ '{agent.get('name', 'unknown')}' êµ¬ì¡° ì˜¤ë¥˜. ìºì‹œë¥¼ ì´ˆê¸°í™”í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                st.stop()
        
        st.session_state["agents"] = agents
        st.success(f"ğŸ‰ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! ({len(agents)}ê°œ ë¬¸ì„œ)")
        st.rerun()

# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´
if clear_btn:
    st.session_state["messages"] = []

# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
print_messages()

# ì‚¬ìš©ìì˜ ì…ë ¥
user_input = st.chat_input("ì…ì‹œ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”!")

# ê²½ê³  ë©”ì‹œì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­
warning_msg = st.empty()

# ë§Œì•½ì— ì‚¬ìš©ì ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´...
if user_input:
    agents = st.session_state["agents"]
    
    if agents:
        # ì—ì´ì „íŠ¸ êµ¬ì¡° ê²€ì¦
        for agent in agents:
            if "retriever_answer" not in agent:
                warning_msg.error("ì—ì´ì „íŠ¸ êµ¬ì¡° ì˜¤ë¥˜ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. 'PDF ë¡œë“œ ë° ì—ì´ì „íŠ¸ ìƒì„±' ë²„íŠ¼ì„ ë‹¤ì‹œ í´ë¦­í•´ì£¼ì„¸ìš”.")
                st.stop()
        
        # ì‚¬ìš©ìì˜ ì…ë ¥
        st.chat_message("user").write(user_input)
        
        with st.chat_message("assistant"):
            try:
                # ë‹¨ê³„ 1: ê´€ë ¨ ë¬¸ì„œ ë¼ìš°íŒ… (ìƒìœ„ 2ê°œ ì„ íƒ)
                selected_agents = route_to_relevant_docs(user_input, agents, top_k=2)
                
                st.success(f"âœ… {len(selected_agents)}ê°œ ê´€ë ¨ ë¬¸ì„œ ì„ íƒ ì™„ë£Œ")
                
                # ë‹¨ê³„ 2: ê° ì„ íƒëœ ë¬¸ì„œì— ëŒ€í•´ ë…ë¦½ì ì¸ ì—ì´ì „íŠ¸ ë‹µë³€ ìƒì„±
                agent_responses = generate_answers_from_selected_agents(
                    user_input,
                    selected_agents,
                    selected_model
                )
                
                # ë‹¨ê³„ 3: ì½”ë””ë„¤ì´í„°ê°€ ì •ë³´ ì¢…í•©
                final_answer = synthesize_agent_answers(
                    user_input,
                    agent_responses,
                    selected_model
                )
                
                st.markdown("### ğŸ“ ì¢…í•© ë‹µë³€")
                st.markdown(final_answer)
                
                # ëŒ€í™”ê¸°ë¡ ì €ì¥
                add_message("user", user_input)
                add_message("assistant", final_answer)
                
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    else:
        warning_msg.error("ë¨¼ì € 'PDF ë¡œë“œ ë° ì—ì´ì „íŠ¸ ìƒì„±' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•˜ì„¸ìš”.")

