import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_teddynote.prompts import load_prompt
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_community.vectorstores import FAISS
from langchain_core.runnables import RunnablePassthrough
from langchain_teddynote import logging
from dotenv import load_dotenv
import os
import json
import re
from PyPDF2 import PdfReader, PdfWriter
from unstructured.partition.pdf import partition_pdf
from langchain.docstore.document import Document

# API KEY ì •ë³´ë¡œë“œ
load_dotenv()

# GEMINI_API_KEYë¥¼ GOOGLE_API_KEYë¡œ ë§¤í•‘
if os.getenv("GEMINI_API_KEY"):
    os.environ["GOOGLE_API_KEY"] = os.getenv("GEMINI_API_KEY")

# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.
logging.langsmith("[Project] TOC-Based Dynamic Routing RAG - Gemini")

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists(".cache"):
    os.mkdir(".cache")

if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")

if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")

if not os.path.exists(".cache/toc_sections"):
    os.mkdir(".cache/toc_sections")

st.title("ğŸ“‘ ëª©ì°¨ ê¸°ë°˜ ë™ì  ë¼ìš°íŒ… ì…ì‹œ ì»¨ì„¤í„´íŠ¸")

# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ
if "messages" not in st.session_state:
    st.session_state["messages"] = []

if "toc_index" not in st.session_state:
    st.session_state["toc_index"] = None

if "pdf_path" not in st.session_state:
    st.session_state["pdf_path"] = None

if "section_cache" not in st.session_state:
    st.session_state["section_cache"] = {}  # {section_key: documents}

if "section_vectorstores" not in st.session_state:
    st.session_state["section_vectorstores"] = {}  # {section_key: vectorstore}

# ì‚¬ì´ë“œë°” ìƒì„±
with st.sidebar:
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")
    
    # PDF íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader("ì›ë³¸ PDF ì—…ë¡œë“œ", type=["pdf"])
    
    selected_model = "gemini-2.5-flash-lite"
    
    # ì¬ì‹œë„ ì„¤ì •
    max_retries = st.slider("ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜", min_value=1, max_value=5, value=3)
    
    st.markdown("---")
    st.markdown("### ğŸ“‘ ëª©ì°¨ ê¸°ë°˜ ë™ì  ë¼ìš°íŒ…")
    st.markdown("1. ì›ë³¸ PDF ì—…ë¡œë“œ")
    st.markdown("2. ëª©ì°¨ ìë™ ê°ì§€ ë° íŒŒì‹±")
    st.markdown("3. ì¿¼ë¦¬ë³„ ê´€ë ¨ ì„¹ì…˜ ìë™ ì„ íƒ")
    st.markdown("4. í•´ë‹¹ ì„¹ì…˜ë§Œ ë™ì  ì²˜ë¦¬")
    st.markdown("5. **í‘œ ë³„ë„ ë¶„ì„ ë° ìš”ì•½**")
    st.markdown("6. **í’ˆì§ˆ í‰ê°€ ë° ì¬ì‹œë„**")
    st.markdown("")
    st.markdown("âœ… ì‚¬ì „ ë¶„í•  ë¶ˆí•„ìš”")
    st.markdown("âœ… ë™ì  ì„¹ì…˜ ì„ íƒ")
    st.markdown("âœ… í‘œ êµ¬ì¡° ì¸ì‹ ë° ìš”ì•½")
    st.markdown("âœ… íš¨ìœ¨ì  ì²˜ë¦¬")
    
    st.markdown("---")
    st.markdown("### âš™ï¸ ì‹œìŠ¤í…œ ìƒíƒœ")
    if st.session_state["toc_index"]:
        st.success(f"âœ… ëª©ì°¨ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
        st.info(f"ğŸ“„ ì„¹ì…˜ ìˆ˜: {len(st.session_state['toc_index'])}ê°œ")
        with st.expander("ğŸ“‹ ëª©ì°¨ ë³´ê¸°"):
            for section in st.session_state["toc_index"]:
                st.markdown(f"**{section['title']}** (í˜ì´ì§€ {section['start_page']}-{section['end_page']})")
    else:
        st.warning("â³ PDF ì—…ë¡œë“œ ëŒ€ê¸° ì¤‘")

# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥
def print_messages():
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)

# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€
def add_message(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))

# ëª©ì°¨ í˜ì´ì§€ ê°ì§€
def detect_toc_pages(pdf_path, max_pages_to_check=10):
    """
    PDFì˜ ì²˜ìŒ ëª‡ í˜ì´ì§€ì—ì„œ ëª©ì°¨ í˜ì´ì§€ë¥¼ ì°¾ìŒ
    """
    reader = PdfReader(pdf_path)
    toc_keywords = ["ëª©ì°¨", "ì°¨ë¡€", "contents", "table of contents", "index"]
    
    toc_pages = []
    
    # ì²˜ìŒ 10í˜ì´ì§€ í™•ì¸
    for page_num in range(min(max_pages_to_check, len(reader.pages))):
        page = reader.pages[page_num]
        text = page.extract_text().lower()
        
        # ëª©ì°¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        for keyword in toc_keywords:
            if keyword in text:
                toc_pages.append(page_num)
                break
    
    return toc_pages

# ëª©ì°¨ êµ¬ì¡° íŒŒì‹± (LLM ì‚¬ìš©)
def parse_toc_structure(pdf_path, toc_pages, model_name="gemini-2.5-flash-lite"):
    """
    ëª©ì°¨ í˜ì´ì§€ë¥¼ LLMìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì„¹ì…˜ êµ¬ì¡° ì¶”ì¶œ
    """
    reader = PdfReader(pdf_path)
    
    # ëª©ì°¨ í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    toc_text = ""
    for page_num in toc_pages:
        page = reader.pages[page_num]
        toc_text += f"\n--- í˜ì´ì§€ {page_num + 1} ---\n"
        toc_text += page.extract_text()
    
    # LLMìœ¼ë¡œ ëª©ì°¨ êµ¬ì¡° íŒŒì‹±
    parse_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ PDF ë¬¸ì„œì˜ ëª©ì°¨ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” PDF ë¬¸ì„œì˜ ëª©ì°¨ í˜ì´ì§€ì…ë‹ˆë‹¤. ëª©ì°¨ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ê° ì„¹ì…˜ì˜ ì œëª©ê³¼ í˜ì´ì§€ ë²”ìœ„ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

**ì¶œë ¥ í˜•ì‹ (JSON):**
[
  {{"title": "ì„¹ì…˜ ì œëª©", "start_page": ì‹œì‘í˜ì´ì§€ë²ˆí˜¸, "end_page": ëí˜ì´ì§€ë²ˆí˜¸}},
  ...
]

**ê·œì¹™:**
1. ê° ì„¹ì…˜ì˜ ì œëª©ì„ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”
2. í˜ì´ì§€ ë²ˆí˜¸ëŠ” 1ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤ (0ì´ ì•„ë‹Œ)
3. ì„¹ì…˜ì˜ ì‹œì‘ í˜ì´ì§€ì™€ ë í˜ì´ì§€ë¥¼ ì¶”ì •í•˜ì„¸ìš”
4. ë‹¤ìŒ ì„¹ì…˜ì´ ì‹œì‘ë˜ê¸° ì „ê¹Œì§€ê°€ í˜„ì¬ ì„¹ì…˜ì˜ ë í˜ì´ì§€ì…ë‹ˆë‹¤
5. ë§ˆì§€ë§‰ ì„¹ì…˜ì˜ ë í˜ì´ì§€ëŠ” ë¬¸ì„œì˜ ë§ˆì§€ë§‰ í˜ì´ì§€ë¡œ ì„¤ì •í•˜ì„¸ìš”
6. JSON í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš” (ì¶”ê°€ ì„¤ëª… ì—†ì´)

**ëª©ì°¨ í…ìŠ¤íŠ¸:**
{toc_text}

**JSON:**
""")
    
    llm = ChatGoogleGenerativeAI(model=model_name, temperature=0)
    chain = parse_prompt | llm | StrOutputParser()
    
    response = chain.invoke({"toc_text": toc_text})
    
    # JSON ì¶”ì¶œ (ì½”ë“œ ë¸”ë¡ ì œê±°)
    json_match = re.search(r'\[.*\]', response, re.DOTALL)
    if json_match:
        json_str = json_match.group(0)
        try:
            sections = json.loads(json_str)
            # ì„¹ì…˜ì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
            if sections and len(sections) > 0:
                return sections
        except json.JSONDecodeError as e:
            st.warning(f"JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
    
    # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ None ë°˜í™˜
    return None

# ì„¹ì…˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ (í‘œ êµ¬ì¡° ì¸ì‹, ì„ë² ë”© ìƒì„±)
def preprocess_section(section, pdf_path, model_name="gemini-2.5-flash-lite"):
    """
    ì„¹ì…˜ì„ ì „ì²˜ë¦¬í•˜ì—¬ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (í‘œ êµ¬ì¡° ì¸ì‹ í¬í•¨)
    """
    section_key = f"{section['start_page']}_{section['end_page']}"
    
    # ì„¹ì…˜ ì¶”ì¶œ
    section_path = extract_pdf_section(
        pdf_path,
        section["start_page"],
        section["end_page"]
    )
    
    # Multi-modal ë°©ì‹ìœ¼ë¡œ í‘œì™€ í…ìŠ¤íŠ¸ ë¶„ë¦¬ ì¶”ì¶œ
    raw_elements = extract_pdf_elements_multimodal(section_path)
    texts, tables = categorize_elements(raw_elements)
    
    # í‘œê°€ ìˆìœ¼ë©´ ìš”ì•½ ìƒì„±
    table_summaries = []
    if tables:
        table_summaries = generate_table_summaries(tables, model_name)
    
    # í…ìŠ¤íŠ¸ì™€ í‘œ ìš”ì•½ ê²°í•©
    all_texts = texts + table_summaries
    
    # Document ê°ì²´ë¡œ ë³€í™˜
    docs = []
    for idx, text in enumerate(all_texts):
        doc = Document(
            page_content=text,
            metadata={
                'section_title': section['title'],
                'section_start': section['start_page'],
                'section_end': section['end_page'],
                'is_table': idx >= len(texts)  # í‘œ ìš”ì•½ì¸ì§€ í‘œì‹œ
            }
        )
        docs.append(doc)
    
    # í…ìŠ¤íŠ¸ ë¶„í•  (í‘œ ìš”ì•½ì€ ì´ë¯¸ ìš”ì•½ëœ ìƒíƒœì´ë¯€ë¡œ ë¶„í• í•˜ì§€ ì•ŠìŒ)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=3000,
        chunk_overlap=600,
        length_function=len,
        separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
    )
    
    # í…ìŠ¤íŠ¸ë§Œ ë¶„í• í•˜ê³ , í‘œ ìš”ì•½ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
    split_docs = []
    for doc in docs:
        if doc.metadata.get('is_table', False):
            # í‘œ ìš”ì•½ì€ ë¶„í• í•˜ì§€ ì•ŠìŒ
            split_docs.append(doc)
        else:
            # í…ìŠ¤íŠ¸ëŠ” ë¶„í• 
            split_text_docs = text_splitter.split_documents([doc])
            split_docs.extend(split_text_docs)
    
    # ëª¨ë“  ë¬¸ì„œì— ë©”íƒ€ë°ì´í„° í™•ì¸
    for doc in split_docs:
        if 'section_title' not in doc.metadata:
            doc.metadata['section_title'] = section['title']
            doc.metadata['section_start'] = section['start_page']
            doc.metadata['section_end'] = section['end_page']
    
    # ì„ë² ë”© ìƒì„±
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vectorstore = FAISS.from_documents(documents=split_docs, embedding=embeddings)
    
    return {
        "vectorstore": vectorstore,
        "documents": split_docs,
        "table_count": len(tables)
    }

# ëª©ì°¨ ì¸ë±ìŠ¤ ìƒì„± ë° ëª¨ë“  ì„¹ì…˜ ì „ì²˜ë¦¬
@st.cache_resource(show_spinner="PDF ì „ì²˜ë¦¬ ì¤‘... (ëª©ì°¨ ë¶„ì„, í‘œ ì¸ì‹, ì„ë² ë”© ìƒì„±)")
def build_toc_index_and_preprocess(pdf_path, _cache_key):
    """
    PDFì˜ ëª©ì°¨ë¥¼ ë¶„ì„í•˜ê³  ëª¨ë“  ì„¹ì…˜ì„ ì „ì²˜ë¦¬í•˜ì—¬ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    """
    # 1ë‹¨ê³„: ëª©ì°¨ í˜ì´ì§€ ê°ì§€
    st.info("ğŸ” ëª©ì°¨ í˜ì´ì§€ ê°ì§€ ì¤‘...")
    toc_pages = detect_toc_pages(pdf_path)
    
    if not toc_pages:
        st.warning("âš ï¸ ëª©ì°¨ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜ì´ì§€ ìˆ˜ ê¸°ë°˜ ë¶„í• ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        # í˜ì´ì§€ ìˆ˜ ê¸°ë°˜ ë¶„í• 
        reader = PdfReader(pdf_path)
        total_pages = len(reader.pages)
        sections_per_part = max(1, total_pages // 4)
        
        sections = []
        for i in range(4):
            start = i * sections_per_part + 1
            end = (i + 1) * sections_per_part if i < 3 else total_pages
            sections.append({
                "title": f"ì„¹ì…˜ {i+1}",
                "start_page": start,
                "end_page": end
            })
    else:
        st.success(f"âœ… ëª©ì°¨ í˜ì´ì§€ ë°œê²¬: {[p+1 for p in toc_pages]}")
        
        # 2ë‹¨ê³„: ëª©ì°¨ êµ¬ì¡° íŒŒì‹±
        st.info("ğŸ“‹ ëª©ì°¨ êµ¬ì¡° íŒŒì‹± ì¤‘...")
        sections = parse_toc_structure(pdf_path, toc_pages)
        
        if not sections:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜ì´ì§€ ìˆ˜ ê¸°ë°˜ ë¶„í• 
            st.warning("âš ï¸ ëª©ì°¨ íŒŒì‹± ì‹¤íŒ¨. í˜ì´ì§€ ìˆ˜ ê¸°ë°˜ ë¶„í• ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            reader = PdfReader(pdf_path)
            total_pages = len(reader.pages)
            sections_per_part = max(1, total_pages // 4)
            
            sections = []
            for i in range(4):
                start = i * sections_per_part + 1
                end = (i + 1) * sections_per_part if i < 3 else total_pages
                sections.append({
                    "title": f"ì„¹ì…˜ {i+1}",
                    "start_page": start,
                    "end_page": end
                })
    
    # í˜ì´ì§€ ë²”ìœ„ ê²€ì¦ ë° ìˆ˜ì •
    reader = PdfReader(pdf_path)
    total_pages = len(reader.pages)
    
    for i, section in enumerate(sections):
        # í˜ì´ì§€ ë²ˆí˜¸ê°€ 1ë¶€í„° ì‹œì‘í•˜ë„ë¡ ë³´ì •
        section["start_page"] = max(1, min(section.get("start_page", 1), total_pages))
        if i < len(sections) - 1:
            section["end_page"] = min(section.get("end_page", total_pages), sections[i+1]["start_page"] - 1)
        else:
            section["end_page"] = min(section.get("end_page", total_pages), total_pages)
    
    st.success(f"âœ… {len(sections)}ê°œ ì„¹ì…˜ ì¶”ì¶œ ì™„ë£Œ")
    
    # 3ë‹¨ê³„: ëª¨ë“  ì„¹ì…˜ ì „ì²˜ë¦¬ (í‘œ êµ¬ì¡° ì¸ì‹, ì„ë² ë”© ìƒì„±)
    st.info(f"ğŸ“„ {len(sections)}ê°œ ì„¹ì…˜ ì „ì²˜ë¦¬ ì¤‘... (í‘œ êµ¬ì¡° ì¸ì‹ ë° ì„ë² ë”© ìƒì„±)")
    
    section_data = {}
    for idx, section in enumerate(sections, 1):
        with st.spinner(f"ì„¹ì…˜ {idx}/{len(sections)}: '{section['title']}' ì²˜ë¦¬ ì¤‘..."):
            section_key = f"{section['start_page']}_{section['end_page']}"
            result = preprocess_section(section, pdf_path)
            
            section_data[section_key] = {
                "vectorstore": result["vectorstore"],
                "documents": result["documents"],
                "section": section,
                "table_count": result["table_count"]
            }
            
            table_info = f" (í‘œ {result['table_count']}ê°œ)" if result['table_count'] > 0 else ""
            st.success(f"âœ… ì„¹ì…˜ {idx} ì™„ë£Œ: '{section['title']}'{table_info}")
    
    return {
        "sections": sections,
        "section_data": section_data
    }

# íŠ¹ì • í˜ì´ì§€ ë²”ìœ„ì˜ PDF ì¶”ì¶œ
def extract_pdf_section(pdf_path, start_page, end_page):
    """
    PDFì—ì„œ íŠ¹ì • í˜ì´ì§€ ë²”ìœ„ë§Œ ì¶”ì¶œí•˜ì—¬ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    """
    reader = PdfReader(pdf_path)
    writer = PdfWriter()
    
    # í˜ì´ì§€ëŠ” 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ -1
    for page_num in range(start_page - 1, min(end_page, len(reader.pages))):
        writer.add_page(reader.pages[page_num])
    
    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    temp_path = f".cache/toc_sections/section_{start_page}_{end_page}.pdf"
    with open(temp_path, "wb") as output_file:
        writer.write(output_file)
    
    return temp_path

# Multi-modal PDF ì²˜ë¦¬ í•¨ìˆ˜ (í‘œì™€ í…ìŠ¤íŠ¸ ë¶„ë¦¬)
def extract_pdf_elements_multimodal(file_path):
    """
    Unstructuredë¥¼ ì‚¬ìš©í•˜ì—¬ PDFì—ì„œ í‘œì™€ í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¦¬ ì¶”ì¶œ
    """
    raw_elements = partition_pdf(
        file_path,
        extract_images_in_pdf=False,  # ì´ë¯¸ì§€ëŠ” ì œì™¸ (ì†ë„ í–¥ìƒ)
        infer_table_structure=True,   # í‘œ êµ¬ì¡° ì¸ì‹
        chunking_strategy="by_title",  # ì œëª©ë³„ë¡œ ì²­í‚¹
        max_characters=4000,
        new_after_n_chars=3800,
        combine_text_under_n_chars=2000,
    )
    
    return raw_elements

def categorize_elements(raw_pdf_elements):
    """
    ì¶”ì¶œëœ ìš”ì†Œë¥¼ í‘œì™€ í…ìŠ¤íŠ¸ë¡œ ë¶„ë¥˜
    """
    tables = []
    texts = []
    
    for element in raw_pdf_elements:
        element_type = str(type(element))
        
        if "Table" in element_type:
            tables.append(str(element))
        elif "CompositeElement" in element_type or "Text" in element_type:
            texts.append(str(element))
    
    return texts, tables

def generate_table_summaries(tables, model_name="gemini-2.5-flash-lite"):
    """
    í‘œë¥¼ LLMìœ¼ë¡œ ìš”ì•½í•˜ì—¬ ê²€ìƒ‰ ìµœì í™”
    """
    if not tables:
        return []
    
    prompt_text = """ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ëª¨ì§‘ìš”ê°•ì˜ í‘œë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    
ì•„ë˜ í‘œì˜ ë‚´ìš©ì„ ìƒì„¸í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”. 
ëŒ€í•™ëª…, ì „í˜•ëª…, í•™ê³¼ëª…, ëª¨ì§‘ ì¸ì›, ì§€ì› ìê²©, ë°˜ì˜ë¹„ìœ¨ ë“± í‘œì— í¬í•¨ëœ ëª¨ë“  ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ í¬í•¨í•˜ì„¸ìš”.

í‘œ: {element}

ìš”ì•½:"""
    
    prompt = ChatPromptTemplate.from_template(prompt_text)
    model = ChatGoogleGenerativeAI(model=model_name, temperature=0)
    summarize_chain = {"element": lambda x: x} | prompt | model | StrOutputParser()
    
    # ë°°ì¹˜ë¡œ ì²˜ë¦¬
    table_summaries = summarize_chain.batch(tables, {"max_concurrency": 3})
    
    return table_summaries

# ê´€ë ¨ ì„¹ì…˜ ì°¾ê¸° (LLM ì‚¬ìš© - ì‚¬ê³  ê³¼ì • í¬í•¨)
def find_relevant_sections(question, toc_index, model_name="gemini-2.5-flash-lite"):
    """
    ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì„¹ì…˜ì„ ì°¾ìŒ (ì‚¬ê³  ê³¼ì • í¬í•¨)
    """
    # ì„¹ì…˜ ëª©ë¡ ìƒì„±
    sections_list = "\n".join([
        f"{idx+1}. {section['title']} (í˜ì´ì§€ {section['start_page']}-{section['end_page']})"
        for idx, section in enumerate(toc_index)
    ])
    
    routing_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ëª¨ì§‘ìš”ê°• ë¬¸ì„œì˜ ëª©ì°¨ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³ , ë‹µë³€ì„ ì°¾ê¸° ìœ„í•´ ì–´ë–¤ ì„¹ì…˜ì„ í™•ì¸í•´ì•¼ í• ì§€ ìƒê°í•´ë³´ì„¸ìš”.

**ì‚¬ìš©ì ì§ˆë¬¸:**
{question}

**ë¬¸ì„œì˜ ëª©ì°¨ (ì„¹ì…˜ ëª©ë¡):**
{sections_list}

**ë¶„ì„ ê³¼ì •:**
1. ì§ˆë¬¸ì˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì–´ë–¤ ì„¹ì…˜ì„ í™•ì¸í•´ì•¼ í• ì§€ ìƒê°í•´ë³´ì„¸ìš”, ë‹¨ ì§ˆë¬¸ì˜ ë‹¨ì ì¸ ë‚´ìš© ë¿ë§Œ ì•„ë‹ˆë¼, ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ì„¸ìš”
2. ê° ì„¹ì…˜ì˜ ì œëª©ì„ ë³´ê³  ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”,ë‹¨ ì§ˆë¬¸ì˜ í‚¤ì›Œë“œë‚˜ ë‹¨ì–´ì—ë§Œ ì§‘ì¤‘í•˜ì§€ ë§ˆì„¸ìš”
3. ê´€ë ¨ ì„¹ì…˜ì„ ì„ íƒí•˜ê³  ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”

**ì¶œë ¥ í˜•ì‹:**
ì‚¬ê³  ê³¼ì •: [ì§ˆë¬¸ ë¶„ì„ ë° ì„¹ì…˜ ì„ íƒ ì´ìœ ]
ì„ íƒí•œ ì„¹ì…˜: [ì„¹ì…˜ ë²ˆí˜¸ë“¤, ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: 1, 3]

**ì˜ˆì‹œ:**
ì‚¬ê³  ê³¼ì •: ì§ˆë¬¸ì´ "ë„¤ì˜¤ë¥´ë„¤ìƒìŠ¤ ì „í˜• ëª¨ì§‘ ì¸ì›"ì— ê´€í•œ ê²ƒì´ë¯€ë¡œ, ì „í˜• ê´€ë ¨ ì„¹ì…˜ê³¼ ëª¨ì§‘ ì¸ì› ê´€ë ¨ ì„¹ì…˜ì„ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.
ì„ íƒí•œ ì„¹ì…˜: 1, 2

---
**ë¶„ì„:**
""")
    
    llm = ChatGoogleGenerativeAI(model=model_name, temperature=0)
    chain = routing_prompt | llm | StrOutputParser()
    
    response = chain.invoke({
        "question": question,
        "sections_list": sections_list
    })
    
    # ì‚¬ê³  ê³¼ì •ê³¼ ì„¹ì…˜ ë²ˆí˜¸ ì¶”ì¶œ
    thinking_process = ""
    if "ì‚¬ê³  ê³¼ì •:" in response:
        thinking_part = response.split("ì‚¬ê³  ê³¼ì •:")[1]
        if "ì„ íƒí•œ ì„¹ì…˜:" in thinking_part:
            thinking_process = thinking_part.split("ì„ íƒí•œ ì„¹ì…˜:")[0].strip()
        else:
            thinking_process = thinking_part.strip()
    
    # ì„¹ì…˜ ë²ˆí˜¸ ì¶”ì¶œ
    section_numbers = []
    if "ì„ íƒí•œ ì„¹ì…˜:" in response:
        section_part = response.split("ì„ íƒí•œ ì„¹ì…˜:")[1]
        for num_str in re.findall(r'\d+', section_part):
            num = int(num_str)
            if 1 <= num <= len(toc_index):
                section_numbers.append(num - 1)  # 0-based index
    else:
        # ì§ì ‘ ìˆ«ì ì¶”ì¶œ
        for num_str in re.findall(r'\d+', response):
            num = int(num_str)
            if 1 <= num <= len(toc_index):
                section_numbers.append(num - 1)
    
    # ìµœì†Œ 1ê°œëŠ” ì„ íƒ
    if not section_numbers:
        section_numbers = [0]  # ì²« ë²ˆì§¸ ì„¹ì…˜ ì„ íƒ
    
    return section_numbers, thinking_process

# ë‹µë³€ í’ˆì§ˆ í‰ê°€
def evaluate_answer_quality(question, answer, model_name="gemini-2.5-flash-lite"):
    """
    ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì—¬ ì ì ˆí•œì§€ íŒë‹¨
    """
    evaluation_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ì»¨ì„¤í„´íŠ¸ì˜ ë‹µë³€ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**í‰ê°€ ê¸°ì¤€:**
1. **ê´€ë ¨ì„±** (0-10ì ): ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆëŠ”ê°€?
2. **ì™„ì „ì„±** (0-10ì ): ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ì¶©ë¶„íˆ ì™„ì „í•œê°€? (ì¤‘ìš” ì •ë³´ ëˆ„ë½ ì—†ìŒ)
3. **ì •í™•ì„±** (0-10ì ): ë‹µë³€ì— êµ¬ì²´ì ì¸ ì •ë³´(ìˆ«ì, ë‚ ì§œ, ì´ë¦„ ë“±)ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?
4. **ìœ ìš©ì„±** (0-10ì ): í•™ìƒ/í•™ë¶€ëª¨ì—ê²Œ ì‹¤ì œë¡œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì¸ê°€?

**ë¶ˆí•©ê²© ê¸°ì¤€ (ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¼ë„ í•´ë‹¹ë˜ë©´ ë¶ˆí•©ê²©):**
- "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤" ê°™ì€ ë¶ˆì™„ì „í•œ ë‹µë³€
- ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ë‹µë³€
- êµ¬ì²´ì ì¸ ì •ë³´ ì—†ì´ ì¶”ìƒì ì¸ ì„¤ëª…ë§Œ ìˆëŠ” ë‹µë³€
- ì´ì ì´ 30ì  ë¯¸ë§Œì¸ ê²½ìš°

**í‰ê°€ í˜•ì‹:**
ì´ì : [0-40ì ]
íŒì •: [í•©ê²©/ë¶ˆí•©ê²©]
ì´ìœ : [ê°„ë‹¨í•œ í‰ê°€ ì´ìœ ]

---
**ì§ˆë¬¸:**
{question}

**ë‹µë³€:**
{answer}

---
**í‰ê°€:**
""")
    
    llm = ChatGoogleGenerativeAI(model=model_name, temperature=0)
    chain = evaluation_prompt | llm | StrOutputParser()
    
    evaluation = chain.invoke({
        "question": question,
        "answer": answer
    })
    
    is_acceptable = "ë¶ˆí•©ê²©" not in evaluation and "í•©ê²©" in evaluation
    
    return {
        "is_acceptable": is_acceptable,
        "evaluation_text": evaluation
    }

# RAG ì²´ì¸ ìƒì„±
def create_rag_chain(retriever, model_name="gemini-2.5-flash-lite"):
    """
    RAG ì²´ì¸ ìƒì„±
    """
    prompt = load_prompt("prompts/pdf-rag.yaml", encoding="utf-8")
    llm = ChatGoogleGenerativeAI(model=model_name, temperature=0)
    
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return chain

# ì§ˆì˜ì‘ë‹µ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
def query_with_retry(question, pdf_path, toc_index, model_name="gemini-2.5-flash-lite", max_retries=3):
    """
    ëª©ì°¨ ê¸°ë°˜ ë™ì  ë¼ìš°íŒ…ìœ¼ë¡œ ì§ˆì˜ì‘ë‹µ ìˆ˜í–‰ (ì¬ì‹œë„ í¬í•¨)
    """
    for attempt in range(1, max_retries + 1):
        if attempt > 1:
            st.warning(f"ğŸ”„ {attempt}ë²ˆì§¸ ì‹œë„ ì¤‘... (ì´ì „ ë‹µë³€ í’ˆì§ˆ ê°œì„  í•„ìš”)")
        
        # 1ë‹¨ê³„: ê´€ë ¨ ì„¹ì…˜ ì°¾ê¸° (ì‚¬ê³  ê³¼ì • í¬í•¨)
        st.info(f"ğŸ¤” ì§ˆë¬¸ ë¶„ì„ ë° ê´€ë ¨ ì„¹ì…˜ íƒìƒ‰ ì¤‘... (ì‹œë„ {attempt}/{max_retries})")
        section_indices, thinking_process = find_relevant_sections(question, toc_index, model_name)
        
        selected_sections = [toc_index[idx] for idx in section_indices]
        
        # ì‚¬ê³  ê³¼ì • ë° ì„ íƒëœ ì„¹ì…˜ í‘œì‹œ
        with st.expander(f"ğŸ§  ì§ˆë¬¸ ë¶„ì„ ë° ì„¹ì…˜ ì„ íƒ (ì‹œë„ {attempt})"):
            st.markdown("**ì§ˆë¬¸ ë¶„ì„:**")
            if thinking_process:
                st.markdown(thinking_process)
            else:
                st.markdown("ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê´€ë ¨ ì„¹ì…˜ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤.")
            
            st.markdown("---")
            st.markdown("**ì„ íƒëœ ì„¹ì…˜:**")
            for idx, section in enumerate(selected_sections, 1):
                st.markdown(f"{idx}. **{section['title']}** (í˜ì´ì§€ {section['start_page']}-{section['end_page']})")
        
        # 2ë‹¨ê³„: ê° ì„¹ì…˜ë³„ë¡œ ê°œë³„ ê²€ìƒ‰ ìˆ˜í–‰
        st.info(f"ğŸ“„ ì„ íƒëœ {len(selected_sections)}ê°œ ì„¹ì…˜ì—ì„œ ê°œë³„ ê²€ìƒ‰ ì¤‘...")
        
        all_retrieved_docs = []  # ëª¨ë“  ì„¹ì…˜ì—ì„œ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
        
        for section in selected_sections:
            section_key = f"{section['start_page']}_{section['end_page']}"
            
            # ì´ë¯¸ ì „ì²˜ë¦¬ëœ ì„¹ì…˜ ë°ì´í„° ì‚¬ìš©
            if section_key in st.session_state["section_vectorstores"]:
                section_data = st.session_state["section_vectorstores"][section_key]
                section_vectorstore = section_data["vectorstore"]
                
                # ê° ì„¹ì…˜ë³„ë¡œ ê²€ìƒ‰ ìˆ˜í–‰
                with st.spinner(f"ì„¹ì…˜ '{section['title']}' ê²€ìƒ‰ ì¤‘..."):
                    try:
                        # MMR ê²€ìƒ‰ìœ¼ë¡œ ê° ì„¹ì…˜ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
                        section_retriever = section_vectorstore.as_retriever(
                            search_type="mmr",
                            search_kwargs={
                                "k": 10,  # ê° ì„¹ì…˜ë‹¹ 10ê°œì”© (ì—¬ëŸ¬ ì„¹ì…˜ì´ë©´ í•©ì³ì„œ 20ê°œ ì •ë„)
                                "fetch_k": 30,
                                "lambda_mult": 0.8
                            }
                        )
                        
                        # ê²€ìƒ‰ ìˆ˜í–‰
                        try:
                            section_docs = section_retriever.invoke(question)
                        except AttributeError:
                            try:
                                section_docs = section_retriever.get_relevant_documents(question)
                            except AttributeError:
                                # ì§ì ‘ vectorstoreì—ì„œ ê²€ìƒ‰
                                section_docs = section_vectorstore.similarity_search_with_score(question, k=10)
                                if section_docs and isinstance(section_docs[0], tuple):
                                    section_docs = [doc for doc, score in section_docs]
                        
                        all_retrieved_docs.extend(section_docs)
                        st.success(f"âœ… ì„¹ì…˜ '{section['title']}'ì—ì„œ {len(section_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
                        
                    except Exception as e:
                        st.warning(f"âš ï¸ ì„¹ì…˜ '{section['title']}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            else:
                st.warning(f"âš ï¸ ì„¹ì…˜ '{section['title']}'ì˜ ì „ì²˜ë¦¬ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        if not all_retrieved_docs:
            st.error("ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "answer": "ì˜¤ë¥˜: ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "evidence": [],
                "selected_sections": selected_sections
            }
        
        # 3ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±°
        st.info(f"ğŸ”— {len(all_retrieved_docs)}ê°œ ê²€ìƒ‰ ê²°ê³¼ í†µí•© ì¤‘...")
        
        # ì¤‘ë³µ ì œê±° (ê°™ì€ ë‚´ìš©ì˜ ë¬¸ì„œëŠ” í•˜ë‚˜ë§Œ ìœ ì§€)
        seen_contents = set()
        unique_docs = []
        for doc in all_retrieved_docs:
            content_hash = hash(doc.page_content[:100])  # ì²˜ìŒ 100ìë¡œ ì¤‘ë³µ íŒë‹¨
            if content_hash not in seen_contents:
                seen_contents.add(content_hash)
                unique_docs.append(doc)
        
        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬ (ì´ë¯¸ MMRë¡œ ì •ë ¬ë˜ì–´ ìˆì§€ë§Œ, ì—¬ëŸ¬ ì„¹ì…˜ ê²°ê³¼ë¥¼ í†µí•©í–ˆìœ¼ë¯€ë¡œ ì¬ì •ë ¬)
        # ìƒìœ„ 20ê°œë§Œ ì„ íƒ
        retrieved_docs = unique_docs[:20]
        
        st.success(f"âœ… ì´ {len(retrieved_docs)}ê°œ ë¬¸ì„œ ì„ íƒ ì™„ë£Œ")
        
        # 4ë‹¨ê³„: í†µí•©ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ ë‹µë³€ ìƒì„±
        st.info("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
        
        # í†µí•©ëœ ë¬¸ì„œë“¤ë¡œ ì„ì‹œ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (RAG ì²´ì¸ ì‚¬ìš©ì„ ìœ„í•´)
        if retrieved_docs:
            embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
            temp_vectorstore = FAISS.from_documents(documents=retrieved_docs, embedding=embeddings)
            
            retriever = temp_vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": len(retrieved_docs)}  # ëª¨ë“  ë¬¸ì„œ ì‚¬ìš©
            )
            
            # RAG ì²´ì¸ìœ¼ë¡œ ë‹µë³€ ìƒì„±
            chain = create_rag_chain(retriever, model_name)
            answer = chain.invoke(question)
        else:
            answer = "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # 5ë‹¨ê³„: í’ˆì§ˆ í‰ê°€
        st.info("ğŸ“Š ë‹µë³€ í’ˆì§ˆ í‰ê°€ ì¤‘...")
        quality_result = evaluate_answer_quality(question, answer, model_name)
        
        with st.expander("ğŸ“‹ í’ˆì§ˆ í‰ê°€ ê²°ê³¼"):
            st.markdown(quality_result["evaluation_text"])
        
        # ê·¼ê±° ë¬¸ì„œ ì •ë³´ ìˆ˜ì§‘
        evidence_docs = []
        for doc in retrieved_docs[:10]:  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
            # ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ì—ì„œ í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ
            page_info = ""
            section_info = ""
            
            if hasattr(doc, 'metadata') and doc.metadata:
                # í‘œ ì—¬ë¶€ í™•ì¸
                is_table = doc.metadata.get('is_table', False)
                table_label = " [í‘œ ìš”ì•½]" if is_table else ""
                
                # ì„¹ì…˜ ì •ë³´
                if 'section_title' in doc.metadata:
                    section_info = doc.metadata['section_title']
                    if 'section_start' in doc.metadata and 'section_end' in doc.metadata:
                        page_info = f"ì„¹ì…˜: {section_info} (í˜ì´ì§€ {doc.metadata['section_start']}-{doc.metadata['section_end']}){table_label}"
                    else:
                        page_info = f"ì„¹ì…˜: {section_info}{table_label}"
                elif 'page' in doc.metadata:
                    page_num = doc.metadata['page']
                    if isinstance(page_num, int):
                        page_info = f"í˜ì´ì§€ {page_num + 1}"  # 0-based to 1-based
                    else:
                        page_info = f"í˜ì´ì§€ {page_num}"
                elif 'source' in doc.metadata:
                    # íŒŒì¼ëª…ì—ì„œ í˜ì´ì§€ ë²”ìœ„ ì¶”ì¶œ ì‹œë„
                    source = doc.metadata['source']
                    if 'section_' in source:
                        # section_10_25.pdf í˜•ì‹ì—ì„œ ì¶”ì¶œ
                        match = re.search(r'section_(\d+)_(\d+)', source)
                        if match:
                            page_info = f"í˜ì´ì§€ {match.group(1)}-{match.group(2)}"
            
            evidence_docs.append({
                "content": doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content,
                "page_info": page_info,
                "section_info": section_info,
                "full_content": doc.page_content,
                "is_table": doc.metadata.get('is_table', False) if hasattr(doc, 'metadata') and doc.metadata else False
            })
        
        # í’ˆì§ˆì´ ì ì ˆí•˜ë©´ ë°˜í™˜
        if quality_result["is_acceptable"]:
            if attempt > 1:
                st.success(f"âœ… {attempt}ë²ˆì§¸ ì‹œë„ì—ì„œ ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤!")
            return {
                "answer": answer,
                "evidence": evidence_docs,
                "selected_sections": selected_sections
            }
        else:
            if attempt < max_retries:
                st.warning(f"âš ï¸ ë‹µë³€ í’ˆì§ˆì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ì¬ì‹œë„í•©ë‹ˆë‹¤... ({attempt}/{max_retries})")
            else:
                st.warning(f"âš ï¸ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜({max_retries})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
                return {
                    "answer": answer,
                    "evidence": evidence_docs,
                    "selected_sections": selected_sections
                }
    
    return {
        "answer": answer,
        "evidence": [],
        "selected_sections": []
    }

# PDF ì—…ë¡œë“œ ì²˜ë¦¬
if uploaded_file:
    current_pdf = st.session_state.get("pdf_path")
    if current_pdf is None or current_pdf != f".cache/files/{uploaded_file.name}":
        # íŒŒì¼ ì €ì¥
        file_content = uploaded_file.read()
        pdf_path = f".cache/files/{uploaded_file.name}"
        with open(pdf_path, "wb") as f:
            f.write(file_content)
        
        st.session_state["pdf_path"] = pdf_path
        st.session_state["section_cache"] = {}  # ìºì‹œ ì´ˆê¸°í™”
        st.session_state["section_vectorstores"] = {}  # ë²¡í„°ìŠ¤í† ì–´ ìºì‹œ ì´ˆê¸°í™”
        
        # ëª©ì°¨ ì¸ë±ìŠ¤ ìƒì„± ë° ëª¨ë“  ì„¹ì…˜ ì „ì²˜ë¦¬
        result = build_toc_index_and_preprocess(pdf_path, uploaded_file.name)
        
        # ëª©ì°¨ ì¸ë±ìŠ¤ ì €ì¥
        st.session_state["toc_index"] = result["sections"]
        
        # ê° ì„¹ì…˜ì˜ ë²¡í„°ìŠ¤í† ì–´ ì €ì¥
        for section_key, data in result["section_data"].items():
            st.session_state["section_vectorstores"][section_key] = data
        
        st.success("âœ… PDF ì—…ë¡œë“œ ë° ì „ì²˜ë¦¬ ì™„ë£Œ! (ëª©ì°¨ ë¶„ì„, í‘œ ì¸ì‹, ì„ë² ë”© ìƒì„±)")
        st.rerun()

# ì´ˆê¸°í™” ë²„íŠ¼
if clear_btn:
    st.session_state["messages"] = []

# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
print_messages()

# ì‚¬ìš©ì ì…ë ¥
user_input = st.chat_input("ì…ì‹œ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”!")

warning_msg = st.empty()

# ì§ˆì˜ì‘ë‹µ ì²˜ë¦¬
if user_input:
    toc_index = st.session_state.get("toc_index")
    pdf_path = st.session_state.get("pdf_path")
    
    if toc_index and pdf_path:
        st.chat_message("user").write(user_input)
        
        with st.chat_message("assistant"):
            try:
                result = query_with_retry(
                    user_input,
                    pdf_path,
                    toc_index,
                    selected_model,
                    max_retries
                )
                
                # ê²°ê³¼ê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸ (ìƒˆ í˜•ì‹)
                if isinstance(result, dict):
                    answer = result["answer"]
                    evidence = result.get("evidence", [])
                    selected_sections = result.get("selected_sections", [])
                else:
                    # ì´ì „ í˜•ì‹ í˜¸í™˜ì„±
                    answer = result
                    evidence = []
                    selected_sections = []
                
                st.markdown("### ğŸ“ ë‹µë³€")
                st.markdown(answer)
                
                # ê·¼ê±° ë¬¸ì„œ í‘œì‹œ
                if evidence:
                    st.markdown("---")
                    st.markdown("### ğŸ“š ë‹µë³€ ê·¼ê±° (ê²€ìƒ‰ëœ ë¬¸ì„œ)")
                    st.caption(f"ì´ {len(evidence)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œ ì²­í¬ë¥¼ ì°¸ê³ í–ˆìŠµë‹ˆë‹¤.")
                    
                    for idx, doc in enumerate(evidence, 1):
                        expander_title = f"ğŸ“„ ê·¼ê±° {idx}"
                        if doc['is_table']:
                            expander_title = f"ğŸ“Š í‘œ ìš”ì•½ {idx}"
                        if doc['page_info']:
                            expander_title += f" - {doc['page_info']}"
                        
                        with st.expander(expander_title):
                            if doc['is_table']:
                                st.markdown("**í‘œ ìš”ì•½ ë‚´ìš©:**")
                                st.info("ì´ ë‚´ìš©ì€ PDFì˜ í‘œë¥¼ AIê°€ ë¶„ì„í•˜ì—¬ ìš”ì•½í•œ ê²ƒì…ë‹ˆë‹¤.")
                            else:
                                st.markdown("**ë¬¸ì„œ ë‚´ìš©:**")
                            st.markdown(doc['content'])
                            if doc['page_info']:
                                st.caption(f"ğŸ“ ì¶œì²˜: {doc['page_info']}")
                            if len(doc['full_content']) > 500:
                                with st.expander("ì „ì²´ ë‚´ìš© ë³´ê¸°"):
                                    st.markdown(doc['full_content'])
                
                # ì„ íƒëœ ì„¹ì…˜ ì •ë³´ í‘œì‹œ
                if selected_sections:
                    st.markdown("---")
                    st.markdown("### ğŸ“‹ ì°¸ê³ í•œ ì„¹ì…˜")
                    for section in selected_sections:
                        st.markdown(f"- **{section['title']}** (í˜ì´ì§€ {section['start_page']}-{section['end_page']})")
                
                add_message("user", user_input)
                add_message("assistant", answer)
                
            except Exception as e:
                import traceback
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                with st.expander("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´"):
                    st.code(traceback.format_exc())
    else:
        if not pdf_path:
            warning_msg.error("ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        elif not toc_index:
            warning_msg.error("PDF ëª©ì°¨ ë¶„ì„ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

