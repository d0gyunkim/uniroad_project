"""
ê°œì„ ëœ RAG ì„œë¹„ìŠ¤ (ë¦¬íŒ©í† ë§ ë²„ì „)
ë‹¨ê³„ë³„ í•„í„°ë§ìœ¼ë¡œ íš¨ìœ¨ì„± í–¥ìƒ
"""
from services.supabase_client import supabase_service
from services.embedding_service import embedding_service
from services.gemini_service import gemini_service
from models.rag_models import SearchResult
from config.constants import IMPORTANT_KEYWORDS, TOP_CHUNKS_COUNT, KEYWORD_DENSITY_WEIGHT
from config.logging_config import rag_logger as logger
import re


class RAGService:
    """ê°œì„ ëœ RAG ê²€ìƒ‰ ì„œë¹„ìŠ¤"""
    
    @staticmethod
    async def search_documents(query: str) -> SearchResult:
        """
        ë‹¨ê³„ë³„ ë¬¸ì„œ ê²€ìƒ‰
        
        1ë‹¨ê³„: ë¬¸ì„œ ì œëª©/í‚¤ì›Œë“œë¡œ ê´€ë ¨ ë¬¸ì„œ í•„í„°ë§ (GPT íŒë‹¨)
        2ë‹¨ê³„: ì„ íƒëœ ë¬¸ì„œ ë‚´ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ê¸°ë°˜ ì²­í¬ ê²€ìƒ‰
        3ë‹¨ê³„: ê´€ë ¨ ì²­í¬ë§Œ ë°˜í™˜
        
        Returns:
            {
                'found': bool,
                'chunks': list,
                'source': str,
                'logs': list  # ê° ë‹¨ê³„ ë¡œê·¸
            }
        """
        logs = []
        
        # ============================================================
        # 1ë‹¨ê³„: ëª¨ë“  ë¬¸ì„œì˜ ìš”ì•½ë³¸ ê°€ì ¸ì˜¤ê¸°
        # ============================================================
        logs.append(f"ğŸ“‹ 1ë‹¨ê³„: ë¬¸ì„œ ìš”ì•½ë³¸ ì¡°íšŒ")
        
        client = supabase_service.get_client()
        response = client.table('policy_documents') \
            .select('id, metadata') \
            .execute()
        
        if not response.data:
            logs.append(f"   âŒ ì €ì¥ëœ ë¬¸ì„œ ì—†ìŒ")
            return SearchResult(
                found=False,
                chunks=[],
                source='',
                logs=logs
            )
        
        # ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í™” (ìš”ì•½ë³¸ í¬í•¨)
        docs_meta = {}
        for row in response.data:
            meta = row.get('metadata', {})
            filename = meta.get('fileName', 'unknown')
            if filename not in docs_meta:
                docs_meta[filename] = {
                    'title': meta.get('title', 'ì œëª© ì—†ìŒ'),
                    'summary': meta.get('summary', 'ìš”ì•½ ì—†ìŒ'),  # ìš”ì•½ë³¸ ì¶”ê°€
                    'keywords': meta.get('keywords', []),
                    'source': meta.get('source', 'ì¶œì²˜ ì—†ìŒ'),
                    'category': meta.get('categoryName', 'ë¯¸ë¶„ë¥˜')
                }
        
        logs.append(f"   âœ… ì´ {len(docs_meta)}ê°œ ê³ ìœ  ë¬¸ì„œ ë°œê²¬")
        for fname, meta in docs_meta.items():
            logs.append(f"      - {meta['title']}")
            logs.append(f"        ìš”ì•½: {meta['summary'][:80]}...")
        
        # ============================================================
        # 2ë‹¨ê³„: Geminië¡œ ìš”ì•½ë³¸ ê¸°ë°˜ ë¬¸ì„œ í•„í„°ë§
        # ============================================================
        logs.append(f"\nğŸ¤– 2ë‹¨ê³„: Gemini ìš”ì•½ë³¸ ê¸°ë°˜ ë¬¸ì„œ í•„í„°ë§")
        
        # ìš”ì•½ë³¸ ê¸°ë°˜ ë¬¸ì„œ ëª©ë¡
        docs_summary = "\n\n".join([
            f"{idx+1}. ì œëª©: {meta['title']}\n   ìš”ì•½: {meta['summary']}\n   ì¹´í…Œê³ ë¦¬: {meta['category']}"
            for idx, (fname, meta) in enumerate(docs_meta.items())
        ])
        
        filter_prompt = f"""ë‹¤ìŒ ë¬¸ì„œë“¤ì˜ ìš”ì•½ë³¸ì„ ì½ê³  ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆëŠ” ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{query}"

ë¬¸ì„œ ëª©ë¡ (ìš”ì•½ë³¸):
{docs_summary}

**ì„ íƒ ë°©ë²•:**
1. ê° ë¬¸ì„œì˜ ìš”ì•½ë³¸ì„ ì½ê³  ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„± íŒë‹¨
2. ê´€ë ¨ìˆëŠ” ë¬¸ì„œ ë²ˆí˜¸ë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´
3. ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ "ì—†ìŒ"ì´ë¼ê³  ë‹µë³€

ë‹µë³€ (ë²ˆí˜¸ë§Œ):"""
        
        try:
            # Gemini í˜¸ì¶œ
            filter_result = await gemini_service.generate(
                filter_prompt,
                "ë‹¹ì‹ ì€ ë¬¸ì„œ í•„í„°ë§ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
            )
            logs.append(f"   GPT ì‘ë‹µ: {filter_result}")
            
            if "ì—†ìŒ" in filter_result or not filter_result:
                logs.append(f"   âŒ ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ")
                return SearchResult(
                    found=False,
                    chunks=[],
                    source='',
                    logs=logs
                )
            
            # ë²ˆí˜¸ ì¶”ì¶œ
            selected_indices = [int(n.strip())-1 for n in re.findall(r'\d+', filter_result)]
            selected_files = [list(docs_meta.keys())[i] for i in selected_indices if i < len(docs_meta)]
            
            logs.append(f"   âœ… {len(selected_files)}ê°œ ë¬¸ì„œ ì„ íƒë¨")
            for fname in selected_files:
                logs.append(f"      - {docs_meta[fname]['title']}")
            
        except Exception as e:
            logs.append(f"   âš ï¸ GPT í•„í„°ë§ ì‹¤íŒ¨, ëª¨ë“  ë¬¸ì„œ ê²€ìƒ‰: {e}")
            selected_files = list(docs_meta.keys())
        
        # ============================================================
        # 3ë‹¨ê³„: ì„ íƒëœ ë¬¸ì„œì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        # ============================================================
        logs.append(f"\nğŸ”‘ 3ë‹¨ê³„: ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ")
        
        # ì§ˆë¬¸ì—ì„œ ëª…ì‚¬ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•)
        query_keywords = []
        for doc_meta in docs_meta.values():
            for keyword in doc_meta['keywords']:
                if keyword in query:
                    query_keywords.append(keyword)
        
        # ì¶”ê°€ë¡œ ì¤‘ìš” ë‹¨ì–´ë“¤
        for word in IMPORTANT_KEYWORDS:
            if word in query and word not in query_keywords:
                query_keywords.append(word)
        
        if not query_keywords:
            query_keywords = query.split()[:3]  # ìµœì†Œí•œ ì²˜ìŒ 3ë‹¨ì–´
        
        logs.append(f"   í‚¤ì›Œë“œ: {', '.join(query_keywords[:5])}")
        
        # ============================================================
        # 4ë‹¨ê³„: ì„ íƒëœ ë¬¸ì„œì˜ ì²­í¬ë§Œ ê°€ì ¸ì˜¤ê¸°
        # ============================================================
        logs.append(f"\nğŸ“„ 4ë‹¨ê³„: ì„ íƒëœ ë¬¸ì„œì˜ ì²­í¬ ì¡°íšŒ")
        
        all_chunks = []
        for fname in selected_files:
            response = client.table('policy_documents') \
                .select('content, metadata') \
                .eq('metadata->>fileName', fname) \
                .execute()
            
            if response.data:
                all_chunks.extend(response.data)
                logs.append(f"   {fname}: {len(response.data)}ê°œ ì²­í¬")
        
        logs.append(f"   âœ… ì´ {len(all_chunks)}ê°œ ì²­í¬")
        
        if not all_chunks:
            logs.append(f"   âŒ ì²­í¬ ì—†ìŒ")
            return SearchResult(
                found=False,
                chunks=[],
                source='',
                logs=logs
            )
        
        # ============================================================
        # 5ë‹¨ê³„: í‚¤ì›Œë“œ ê¸°ë°˜ ì²­í¬ ì ìˆ˜ ê³„ì‚°
        # ============================================================
        logs.append(f"\nâ­ 5ë‹¨ê³„: í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°")
        
        scored_chunks = []
        for chunk in all_chunks:
            content = chunk['content'].lower()
            score = 0
            
            # ê° í‚¤ì›Œë“œê°€ í¬í•¨ë˜ë©´ +1ì 
            for keyword in query_keywords:
                if keyword.lower() in content:
                    score += 1
            
            # í‚¤ì›Œë“œ ë°€ë„ ë³´ë„ˆìŠ¤ (í‚¤ì›Œë“œê°€ ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¤ë©´ ë” ë†’ì€ ì ìˆ˜)
            for keyword in query_keywords[:3]:  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë§Œ
                count = content.count(keyword.lower())
                score += count * KEYWORD_DENSITY_WEIGHT
            
            if score > 0:
                scored_chunks.append({
                    'chunk': chunk,
                    'score': score
                })
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        scored_chunks.sort(key=lambda x: x['score'], reverse=True)
        
        logs.append(f"   ë§¤ì¹­ëœ ì²­í¬: {len(scored_chunks)}ê°œ")
        for idx, item in enumerate(scored_chunks[:5], 1):
            logs.append(f"      {idx}. ì ìˆ˜ {item['score']:.1f}: {item['chunk']['content'][:80]}...")
        
        if not scored_chunks:
            logs.append(f"   âŒ í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨")
            return SearchResult(
                found=False,
                chunks=[],
                source='',
                logs=logs
            )
        
        # ============================================================
        # 6ë‹¨ê³„: ìƒìœ„ ì²­í¬ ë°˜í™˜
        # ============================================================
        logs.append(f"\nâœ… 6ë‹¨ê³„: ìƒìœ„ {TOP_CHUNKS_COUNT}ê°œ ì²­í¬ ì„ íƒ")

        top_chunks = [item['chunk'] for item in scored_chunks[:TOP_CHUNKS_COUNT]]
        source = top_chunks[0]['metadata'].get('source', 'ê³µì‹ ë¬¸ì„œ')
        
        logs.append(f"   ì¶œì²˜: {source}")
        logs.append(f"   ë°˜í™˜ ì²­í¬: {len(top_chunks)}ê°œ")
        
        result = SearchResult(
            found=True,
            chunks=top_chunks,
            source=source,
            logs=logs
        )

        logger.info(f"RAG ê²€ìƒ‰ ì™„ë£Œ - ì²­í¬ {len(top_chunks)}ê°œ ë°˜í™˜, ì¶œì²˜: {source}")

        return result


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
rag_service = RAGService()

