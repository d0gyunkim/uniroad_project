"""
Sub Agents
- ëŒ€í•™ë³„ Agent: Supabaseì—ì„œ í•´ë‹¹ ëŒ€í•™ í•´ì‹œíƒœê·¸ ë¬¸ì„œ ê²€ìƒ‰
- ì»¨ì„¤íŒ… Agent: ì„ì‹œ DBì—ì„œ ì…ê²°/í™˜ì‚°ì ìˆ˜ ë°ì´í„° ì¡°íšŒ
- ì„ ìƒë‹˜ Agent: í•™ìŠµ ê³„íš ë° ë©˜íƒˆ ê´€ë¦¬ ì¡°ì–¸
"""

import google.generativeai as genai
from typing import Dict, Any, List
import json
import os
import re
from dotenv import load_dotenv
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))))
from token_logger import log_token_usage

from services.supabase_client import supabase_service
from services.gemini_service import gemini_service
from services.score_converter import ScoreConverter
from services.data_standard import (
    korean_std_score_table,
    math_std_score_table,
    social_studies_data,
    science_inquiry_data,
    major_subjects_grade_cuts,
    english_grade_data,
    history_grade_data
)
from .mock_database import (
    get_admission_data_by_grade,
    get_jeongsi_data_by_percentile,
    get_score_conversion_info,
    get_all_universities_data,
    ADMISSION_DATA_SUSI,
    ADMISSION_DATA_JEONGSI
)

# ë¡œê·¸ ì½œë°± (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ìš©)
_log_callback = None

def set_log_callback(callback):
    """ë¡œê·¸ ì½œë°± ì„¤ì •"""
    global _log_callback
    _log_callback = callback

def _log(msg: str):
    """ë¡œê·¸ ì¶œë ¥ ë° ì½œë°± í˜¸ì¶œ"""
    if _log_callback:
        _log_callback(msg)
    else:
        print(msg)

load_dotenv()

# Gemini API ì„¤ì •
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)


class SubAgentBase:
    """Sub Agent ê¸°ë³¸ í´ë˜ìŠ¤"""

    def __init__(self, name: str, description: str, custom_system_prompt: str = None):
        self.name = name
        self.description = description
        self.custom_system_prompt = custom_system_prompt
        self.model = genai.GenerativeModel(
            model_name="gemini-3-flash-preview",
        )

    async def execute(self, query: str) -> Dict[str, Any]:
        """ì¿¼ë¦¬ ì‹¤í–‰ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        raise NotImplementedError


class UniversityAgent(SubAgentBase):
    """
    ëŒ€í•™ë³„ Agent - Supabaseì—ì„œ í•´ë‹¹ ëŒ€í•™ í•´ì‹œíƒœê·¸ ë¬¸ì„œ ê²€ìƒ‰
    
    ê²€ìƒ‰ ë¡œì§:
    1. í•´ì‹œíƒœê·¸ë¡œ 1ì°¨ íƒìƒ‰ (#{ëŒ€í•™ëª…})
    2. ìš”ì•½ë³¸(500ì) ë¶„ì„ìœ¼ë¡œ ì í•©í•œ ë¬¸ì„œ ì„ ë³„
    3. ì„ ë³„ëœ ë¬¸ì„œì˜ ì „ì²´ ë‚´ìš© ë¡œë“œ
    4. ì •ë³´ ì¶”ì¶œ í›„ ì¶œì²˜ì™€ í•¨ê»˜ ë°˜í™˜
    """

    SUPPORTED_UNIVERSITIES = ["ì„œìš¸ëŒ€", "ì—°ì„¸ëŒ€", "ê³ ë ¤ëŒ€", "ì„±ê· ê´€ëŒ€", "ê²½í¬ëŒ€"]

    def __init__(self, university_name: str, custom_system_prompt: str = None):
        self.university_name = university_name
        super().__init__(
            name=f"{university_name} agent",
            description=f"{university_name} ì…ì‹œ ì •ë³´(ì…ê²°, ëª¨ì§‘ìš”ê°•, ì „í˜•ë³„ ì •ë³´)ë¥¼ Supabaseì—ì„œ ê²€ìƒ‰í•˜ëŠ” ì—ì´ì „íŠ¸",
            custom_system_prompt=custom_system_prompt
        )

    async def execute(self, query: str) -> Dict[str, Any]:
        """ëŒ€í•™ ì •ë³´ ê²€ìƒ‰ ë° ì •ë¦¬"""
        _log("")
        _log("="*60)
        _log(f"ğŸ« {self.name} ì‹¤í–‰")
        _log("="*60)
        _log(f"ì¿¼ë¦¬: {query}")

        try:
            client = supabase_service.get_client()

            # ============================================================
            # 1ë‹¨ê³„: í•´ì‹œíƒœê·¸ë¡œ 1ì°¨ íƒìƒ‰
            # ============================================================
            _log("")
            _log(f"ğŸ“‹ [1ë‹¨ê³„] í•´ì‹œíƒœê·¸ ê²€ìƒ‰: #{self.university_name}")
            
            metadata_response = client.table('documents_metadata').select('*').execute()
            
            if not metadata_response.data:
                return {
                    "agent": self.name,
                    "status": "no_data",
                    "result": f"{self.university_name} ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "sources": [],
                    "source_urls": [],
                    "citations": []
                }

            # í•´ì‹œíƒœê·¸ í•„í„°ë§
            required_univ_tag = f"#{self.university_name}"
            
            # ì¶”ê°€ í•´ì‹œíƒœê·¸ ì¶”ì¶œ (ì—°ë„, ì „í˜• ë“±)
            optional_tags = []
            year_match = re.search(r'(2024|2025|2026|2027|2028)', query)
            if year_match:
                optional_tags.append(f"#{year_match.group()}")
            
            if 'ìˆ˜ì‹œ' in query:
                optional_tags.append('#ìˆ˜ì‹œ')
            if 'ì •ì‹œ' in query:
                optional_tags.append('#ì •ì‹œ')
            if any(word in query for word in ['ìš”ê°•', 'ëª¨ì§‘']):
                optional_tags.append('#ëª¨ì§‘ìš”ê°•')
            if any(word in query for word in ['ì…ê²°', 'ê²½ìŸë¥ ', 'ì»¤íŠ¸']):
                optional_tags.append('#ì…ê²°í†µê³„')

            # í•„í„°ë§
            relevant_docs = []
            for doc in metadata_response.data:
                doc_hashtags = doc.get('hashtags', []) or []
                
                # í•„ìˆ˜ ì¡°ê±´: ëŒ€í•™ íƒœê·¸ í¬í•¨
                if required_univ_tag not in doc_hashtags:
                    continue
                
                # ì ìˆ˜ ê³„ì‚°
                score = 10  # ëŒ€í•™ íƒœê·¸ ì¼ì¹˜ ê¸°ë³¸ ì ìˆ˜
                for tag in optional_tags:
                    if tag in doc_hashtags:
                        score += 5
                
                relevant_docs.append((score, doc))
            
            # ì ìˆ˜ìˆœ ì •ë ¬
            relevant_docs.sort(key=lambda x: x[0], reverse=True)
            relevant_docs = [doc for score, doc in relevant_docs]
            
            _log(f"   {self.university_name} ê´€ë ¨ ë¬¸ì„œ: {len(relevant_docs)}ê°œ")
            
            if not relevant_docs:
                return {
                    "agent": self.name,
                    "status": "no_match",
                    "result": f"{self.university_name} ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                    "sources": [],
                    "source_urls": [],
                    "citations": []
                }

            # ============================================================
            # 2ë‹¨ê³„: ìš”ì•½ë³¸ ë¶„ì„ (500ì ì´ë‚´)
            # ============================================================
            _log("")
            _log(f"ğŸ“‹ [2ë‹¨ê³„] ìš”ì•½ë³¸ ë¶„ì„")
            
            docs_summary_list = []
            for idx, doc in enumerate(relevant_docs[:10], 1):  # ìµœëŒ€ 10ê°œ
                title = doc.get('title', 'ì œëª© ì—†ìŒ')
                summary = doc.get('summary', 'ìš”ì•½ ì—†ìŒ')[:500]
                hashtags = doc.get('hashtags', [])
                docs_summary_list.append(
                    f"{idx}. ì œëª©: {title}\n   í•´ì‹œíƒœê·¸: {', '.join(hashtags) if hashtags else 'ì—†ìŒ'}\n   ìš”ì•½: {summary}"
                )
            
            docs_summary_text = "\n\n".join(docs_summary_list)
            
            filter_prompt = f"""ë‹¤ìŒ ë¬¸ì„œë“¤ì˜ ìš”ì•½ë³¸ì„ ì½ê³ , ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° í•„ìš”í•œ ë¬¸ì„œë§Œ ì„ íƒí•˜ì„¸ìš”.

ì§ˆë¬¸: "{query}"

ë¬¸ì„œ ëª©ë¡:
{docs_summary_text}

ì„ íƒ ê¸°ì¤€:
1. ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° í•„ìš”í•œ ì •ë³´ê°€ í¬í•¨ëœ ë¬¸ì„œë§Œ ì„ íƒ
2. ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ ì„ íƒ

ë‹µë³€ í˜•ì‹:
ê´€ë ¨ ë¬¸ì„œê°€ ìˆìœ¼ë©´: ë²ˆí˜¸ë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„ (ì˜ˆ: 1, 3)
ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìœ¼ë©´: ì—†ìŒ"""

            try:
                filter_result = await gemini_service.generate(
                    filter_prompt,
                    "ë¬¸ì„œ í•„í„°ë§ ì „ë¬¸ê°€"
                )
                
                if not filter_result.strip() or "ì—†ìŒ" in filter_result.lower():
                    # í•„í„°ë§ ì‹¤íŒ¨ì‹œ ìƒìœ„ 2ê°œ ì‚¬ìš©
                    selected_docs = relevant_docs[:2]
                else:
                    selected_indices = [int(n.strip())-1 for n in re.findall(r'\d+', filter_result)]
                    selected_docs = [relevant_docs[i] for i in selected_indices if i < len(relevant_docs)]
                    if not selected_docs:
                        selected_docs = relevant_docs[:2]
                        
            except Exception as e:
                _log(f"   âš ï¸ ìš”ì•½ë³¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
                selected_docs = relevant_docs[:2]
            
            _log(f"   ì„ ë³„ëœ ë¬¸ì„œ: {len(selected_docs)}ê°œ")

            # ============================================================
            # 3ë‹¨ê³„: ì „ì²´ ë‚´ìš© ë¡œë“œ
            # ============================================================
            _log("")
            _log(f"ğŸ“‹ [3ë‹¨ê³„] ë¬¸ì„œ ë‚´ìš© ë¡œë“œ")
            
            full_content = ""
            sources = []
            source_urls = []
            citations = []
            
            for doc in selected_docs:
                filename = doc['file_name']
                title = doc['title']
                file_url = doc.get('file_url') or ''
                
                sources.append(title)
                source_urls.append(file_url)
                
                _log(f"   ğŸ“„ {title}")
                
                # ì²­í¬ ê°€ì ¸ì˜¤ê¸°
                chunks_response = client.table('policy_documents')\
                    .select('id, content, metadata')\
                    .eq('metadata->>fileName', filename)\
                    .execute()
                
                if chunks_response.data:
                    sorted_chunks = sorted(
                        chunks_response.data,
                        key=lambda x: x.get('metadata', {}).get('chunkIndex', 0)
                    )
                    
                    full_content += f"\n\n{'='*60}\n"
                    full_content += f"ğŸ“„ {title}\n"
                    full_content += f"{'='*60}\n\n"
                    
                    # ì²­í¬ ì •ë³´ ì €ì¥ (ë‹µë³€ ì¶”ì ìš©)
                    for chunk in sorted_chunks:
                        chunk_content = chunk['content']
                        full_content += chunk_content
                        full_content += "\n\n"
                        
                        # ê° ì²­í¬ ì •ë³´ë¥¼ citationsì— ì €ì¥ (chunk í‚¤ë¡œ)
                        # citationsëŠ” ë‚˜ì¤‘ì— final_agentì—ì„œ ì¶”ì¶œë¨
                        chunk_info = {
                            "id": chunk.get('id'),
                            "content": chunk_content,
                            "title": title,
                            "source": doc.get('source', ''),
                            "file_url": file_url,
                            "metadata": chunk.get('metadata', {})
                        }
                        citations.append({
                            "chunk": chunk_info,
                            "source": title,  # ê¸°ì¡´ í˜•ì‹ ìœ ì§€
                            "url": file_url
                        })

            # ============================================================
            # 4ë‹¨ê³„: ì •ë³´ ì¶”ì¶œ
            # ============================================================
            _log("")
            _log(f"ğŸ“‹ [4ë‹¨ê³„] ì •ë³´ ì¶”ì¶œ")

            # ì‚¬ìš© ê°€ëŠ¥í•œ ì¶œì²˜ ëª©ë¡ ìƒì„±
            sources_list = "\n".join([f"- {s}" for s in sources])

            extract_prompt = f"""ë‹¤ìŒ ë¬¸ì„œì—ì„œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° í•„ìš”í•œ í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.

ì§ˆë¬¸: {query}

ì‚¬ìš© ê°€ëŠ¥í•œ ì¶œì²˜ ëª©ë¡:
{sources_list}

ë¬¸ì„œ ë‚´ìš©:
{full_content[:15000]}

ì¶œë ¥ ê·œì¹™:
1. í•µì‹¬ ì •ë³´ë§Œ ê°„ê²°í•˜ê²Œ ì¶”ì¶œ
2. ìˆ˜ì¹˜ ë°ì´í„°ëŠ” ì •í™•í•˜ê²Œ ìœ ì§€
3. ê° ì •ë³´ê°€ ì–´ëŠ ë¬¸ì„œì—ì„œ ì™”ëŠ”ì§€ [ì¶œì²˜: ë¬¸ì„œëª…] í˜•ì‹ìœ¼ë¡œ ë°˜ë“œì‹œ í‘œì‹œ
4. ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì™”ë‹¤ë©´, ê° ì •ë³´ë§ˆë‹¤ í•´ë‹¹ ì¶œì²˜ë¥¼ í‘œì‹œ
5. ë§ˆì§€ë§‰ì— "ì¶œì²˜: ë¬¸ì„œ1, ë¬¸ì„œ2, ..." í˜•íƒœë¡œ ìš”ì•½í•˜ì§€ ë§ê³ , ì •ë³´ë§ˆë‹¤ ê°œë³„ í‘œì‹œ
6. JSONì´ ì•„ë‹Œ ìì—°ì–´ë¡œ ì‘ì„±"""

            try:
                extracted_info = await gemini_service.generate(
                    extract_prompt,
                    "ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ ì „ë¬¸ê°€"
                )

                # citationsëŠ” ì´ë¯¸ ì²­í¬ ì •ë³´ì™€ í•¨ê»˜ ì¶”ê°€ë˜ì—ˆìœ¼ë¯€ë¡œ ì¶”ê°€ ì‘ì—… ë¶ˆí•„ìš”

            except Exception as e:
                extracted_info = f"ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}"
            
            _log(f"   ì¶”ì¶œëœ ì •ë³´ ê¸¸ì´: {len(extracted_info)}ì")
            _log("="*60)

            return {
                "agent": self.name,
                "status": "success",
                "query": query,
                "result": extracted_info,
                "sources": sources,
                "source_urls": source_urls,
                "citations": citations
            }

        except Exception as e:
            _log(f"âŒ {self.name} ì˜¤ë¥˜: {e}")
            return {
                "agent": self.name,
                "status": "error",
                "result": str(e),
                "sources": [],
                "source_urls": [],
                "citations": []
            }


class ConsultingAgent(SubAgentBase):
    """
    ì»¨ì„¤íŒ… Agent - ì„ì‹œ DBì—ì„œ ì…ê²°/í™˜ì‚°ì ìˆ˜ ë°ì´í„° ì¡°íšŒ
    5ê°œ ëŒ€í•™(ì„œìš¸ëŒ€/ì—°ì„¸ëŒ€/ê³ ë ¤ëŒ€/ì„±ê· ê´€ëŒ€/ê²½í¬ëŒ€) ë°ì´í„° ì‚¬ìš©
    
    ì ìˆ˜ ë³€í™˜ ê¸°ëŠ¥:
    - ë“±ê¸‰/í‘œì¤€ì ìˆ˜/ë°±ë¶„ìœ„/ì›ì ìˆ˜ -> ë“±ê¸‰-í‘œì¤€ì ìˆ˜-ë°±ë¶„ìœ„ ì •ê·œí™”
    - 2026 ìˆ˜ëŠ¥ ë°ì´í„° ê¸°ì¤€
    """

    def __init__(self, custom_system_prompt: str = None):
        super().__init__(
            name="ì»¨ì„¤íŒ… agent",
            description="5ê°œ ëŒ€í•™ í•©ê²© ë°ì´í„° ë¹„êµ ë¶„ì„, í•©ê²© ê°€ëŠ¥ì„± í‰ê°€",
            custom_system_prompt=custom_system_prompt
        )
        # ScoreConverter ì´ˆê¸°í™”
        self.score_converter = ScoreConverter()
        
        # 2026 ìˆ˜ëŠ¥ ë°ì´í„° ì¤€ë¹„
        self.score_data = {
            "êµ­ì–´": {
                "í‘œì¤€ì ìˆ˜_í…Œì´ë¸”": {str(k): v for k, v in korean_std_score_table.items()},
                "ì„ íƒê³¼ëª©_ë“±ê¸‰ì»·": major_subjects_grade_cuts.get("êµ­ì–´", {})
            },
            "ìˆ˜í•™": {
                "í‘œì¤€ì ìˆ˜_í…Œì´ë¸”": {str(k): v for k, v in math_std_score_table.items()},
                "ì„ íƒê³¼ëª©_ë“±ê¸‰ì»·": major_subjects_grade_cuts.get("ìˆ˜í•™", {})
            },
            "ì˜ì–´": english_grade_data,
            "í•œêµ­ì‚¬": history_grade_data,
            "ì‚¬íšŒíƒêµ¬": social_studies_data,
            "ê³¼í•™íƒêµ¬": science_inquiry_data
        }

    async def execute(self, query: str) -> Dict[str, Any]:
        """ì„±ì  ê¸°ë°˜ í•©ê²© ê°€ëŠ¥ ëŒ€í•™ ë¶„ì„"""
        _log("")
        _log("="*60)
        _log(f"ğŸ“Š ì»¨ì„¤íŒ… Agent ì‹¤í–‰")
        _log("="*60)
        _log(f"ì¿¼ë¦¬: {query}")

        # ì¿¼ë¦¬ì—ì„œ ì„±ì  ì •ë³´ ì¶”ì¶œ ë° ì •ê·œí™”
        raw_grade_info = self._extract_grade_from_query(query)
        _log(f"   ì¶”ì¶œëœ ì›ë³¸ ì„±ì : {raw_grade_info}")
        
        # ì ìˆ˜ ì •ê·œí™” (ë“±ê¸‰-í‘œì¤€ì ìˆ˜-ë°±ë¶„ìœ„)
        normalized_scores = self._normalize_scores(raw_grade_info)
        _log(f"   ì •ê·œí™”ëœ ì„±ì : {json.dumps(normalized_scores, ensure_ascii=False, indent=2)}")

        # DBì—ì„œ ë°ì´í„° ì¡°íšŒ
        susi_data = None
        jeongsi_data = None

        if raw_grade_info.get("ë‚´ì‹ "):
            susi_data = get_admission_data_by_grade(raw_grade_info["ë‚´ì‹ "])

        # ì •ê·œí™”ëœ ë°±ë¶„ìœ„ë¡œ ì •ì‹œ ë°ì´í„° ì¡°íšŒ
        avg_percentile = self._calculate_average_percentile(normalized_scores)
        if avg_percentile:
            jeongsi_data = get_jeongsi_data_by_percentile(avg_percentile)
            _log(f"   í‰ê·  ë°±ë¶„ìœ„: {avg_percentile}")

        # ì „ì²´ ë°ì´í„° í¬í•¨
        all_data = get_all_universities_data()
        
        # ì •ê·œí™”ëœ í•™ìƒ ì„±ì  ì¶”ê°€
        all_data["í•™ìƒ_ì •ê·œí™”_ì„±ì "] = normalized_scores
        all_data["í•™ìƒ_ì„±ì ë¶„ì„"] = {
            "ìˆ˜ì‹œ": susi_data,
            "ì •ì‹œ": jeongsi_data
        } if (susi_data or jeongsi_data) else None

        # Geminië¡œ ë¶„ì„
        if self.custom_system_prompt:
            system_prompt = self.custom_system_prompt.format(
                all_data=json.dumps(all_data, ensure_ascii=False, indent=2)[:8000]
            )
            print(f"ğŸ¨ Using custom system prompt for consulting agent")
        else:
            # ì •ê·œí™”ëœ ì„±ì  ì •ë³´ í¬ë§·íŒ…
            normalized_scores_text = self._format_normalized_scores(normalized_scores)
            
            system_prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì„±ì ì„ '2026 ìˆ˜ëŠ¥ ë°ì´í„°' ê¸°ì¤€ìœ¼ë¡œ í‘œì¤€í™”í•˜ì—¬ ë¶„ì„í•˜ê³ , íŒ©íŠ¸ ê¸°ë°˜ì˜ ë¶„ì„ ê²°ê³¼ë§Œ ì œê³µí•˜ì„¸ìš”.

## í•™ìƒì˜ ì •ê·œí™”ëœ ì„±ì  (ë“±ê¸‰-í‘œì¤€ì ìˆ˜-ë°±ë¶„ìœ„)
{normalized_scores_text}

## ê°€ìš© ì…ê²° ë°ì´í„°
{json.dumps(all_data, ensure_ascii=False, indent=2)[:6000]}

## ì¶œë ¥ ê·œì¹™ (í•„ìˆ˜)
1. **ì„±ì  ì •ê·œí™” ê²°ê³¼ ë¨¼ì € ì œì‹œ**: í•™ìƒì˜ ì…ë ¥ì„ ë“±ê¸‰-í‘œì¤€ì ìˆ˜-ë°±ë¶„ìœ„ë¡œ ë³€í™˜í•œ ê²°ê³¼ë¥¼ ëª…ì‹œ
2. ì¶”ì •ëœ ê³¼ëª©ì´ ìˆìœ¼ë©´ "(ì¶”ì •)" í‘œì‹œ
3. ì§ˆë¬¸ì— í•„ìš”í•œ í•µì‹¬ ë°ì´í„°ë§Œ ê°„ê²°í•˜ê²Œ ì œì‹œ
4. ìˆ˜ì¹˜ ë°ì´í„°ëŠ” ì •í™•í•˜ê²Œ í‘œê¸°
5. ê° ì •ë³´ ë’¤ì— [ì¶œì²˜: ì»¨ì„¤íŒ…DB] í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ í‘œì‹œ
6. JSONì´ ì•„ë‹Œ ìì—°ì–´ë¡œ ì¶œë ¥
7. ê²©ë ¤ë‚˜ ì¡°ì–¸ì€ í•˜ì§€ ë§ê³  ì˜¤ì§ ë°ì´í„°ë§Œ ì œê³µ
8. "í•©ê²©ê°€ëŠ¥", "ë„ì „ê°€ëŠ¥" ê°™ì€ íŒë‹¨ì€ í•˜ì§€ ë§ê³  ì‚¬ì‹¤ë§Œ ë‚˜ì—´
9. ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•(**, *, #, ##, ###) ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€
10. ê¸€ë¨¸ë¦¬ ê¸°í˜¸ëŠ” - ë˜ëŠ” â€¢ ë§Œ ì‚¬ìš©

## ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ
ã€í•™ìƒ ì„±ì  ì •ê·œí™”ã€‘
- êµ­ì–´(ì–¸ì–´ì™€ë§¤ì²´): 1ë“±ê¸‰ / í‘œì¤€ì ìˆ˜ 140 / ë°±ë¶„ìœ„ 98
- ìˆ˜í•™(ë¯¸ì ë¶„): 2ë“±ê¸‰ / í‘œì¤€ì ìˆ˜ 128 / ë°±ë¶„ìœ„ 92
- ì˜ì–´: 2ë“±ê¸‰ (ì¶”ì •)
[ì¶œì²˜: 2026 ìˆ˜ëŠ¥ ë°ì´í„°]

ã€ì…ê²° ë°ì´í„° ë¹„êµã€‘
- 2024í•™ë…„ë„ ì„œìš¸ëŒ€ ê¸°ê³„ê³µí•™ë¶€ ìˆ˜ì‹œ ì¼ë°˜ì „í˜• 70% ì»¤íŠ¸ë¼ì¸: ë‚´ì‹  1.5ë“±ê¸‰ [ì¶œì²˜: ì»¨ì„¤íŒ…DB]
- 2024í•™ë…„ë„ ì—°ì„¸ëŒ€ ê¸°ê³„ê³µí•™ë¶€ ì •ì‹œ 70% ì»¤íŠ¸ë¼ì¸: ë°±ë¶„ìœ„ 95.2 [ì¶œì²˜: ì»¨ì„¤íŒ…DB]"""

        try:
            response = self.model.generate_content(
                f"{system_prompt}\n\nì§ˆë¬¸: {query}\n\nìœ„ ë°ì´í„°ì—ì„œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° í•„ìš”í•œ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.",
                generation_config={"temperature": 0.1, "max_output_tokens": 1024},
                request_options=genai.types.RequestOptions(
                    retry=None,
                    timeout=120.0  # ë©€í‹°ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ì„ ìœ„í•´ 120ì´ˆë¡œ ì¦ê°€
                )
            )

            # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
            if hasattr(response, 'usage_metadata'):
                usage = response.usage_metadata
                print(f"ğŸ’° í† í° ì‚¬ìš©ëŸ‰ ({self.name}): {usage}")
                
                log_token_usage(
                    operation="ì…ê²°ë¹„êµì—ì´ì „íŠ¸",
                    prompt_tokens=getattr(usage, 'prompt_token_count', 0),
                    output_tokens=getattr(usage, 'candidates_token_count', 0),
                    total_tokens=getattr(usage, 'total_token_count', 0),
                    model="gemini-3-flash-preview",
                    details=self.name
                )

            result_text = response.text
            
            # citations êµ¬ì„±
            citations = [
                {
                    "text": "5ê°œ ëŒ€í•™ ì…ê²° ë°ì´í„° ë¶„ì„",
                    "source": "ì»¨ì„¤íŒ… DB (ì„œìš¸ëŒ€/ì—°ì„¸ëŒ€/ê³ ë ¤ëŒ€/ì„±ê· ê´€ëŒ€/ê²½í¬ëŒ€)",
                    "url": ""
                }
            ]
            
            # ì ìˆ˜ ë³€í™˜ì´ ì‹¤ì œë¡œ ì´ë£¨ì–´ì§„ ê²½ìš°ì—ë§Œ ì‚°ì¶œë°©ì‹ ë¬¸ì„œ ì¶”ê°€
            if normalized_scores and normalized_scores.get("ê³¼ëª©ë³„_ì„±ì "):
                citations.append({
                    "text": "í‘œì¤€ì ìˆ˜Â·ë°±ë¶„ìœ„ ì‚°ì¶œ ë°©ì‹",
                    "source": "ìœ ë‹ˆë¡œë“œ 2026 ìˆ˜ëŠ¥ í‘œì¤€ì ìˆ˜ ë° ë°±ë¶„ìœ„ ì‚°ì¶œ ë°©ì‹ ë¬¸ì„œ",
                    "url": "https://rnitmphvahpkosvxjshw.supabase.co/storage/v1/object/public/document/pdfs/5d5c4455-bf58-4ef5-9e7f-a82d602aaa51.pdf"
                })

            _log(f"   ë¶„ì„ ì™„ë£Œ")
            _log("="*60)

            # sources ëª©ë¡ êµ¬ì„±
            sources = ["ì»¨ì„¤íŒ… DB"]
            if normalized_scores and normalized_scores.get("ê³¼ëª©ë³„_ì„±ì "):
                sources.append("í‘œì¤€ì ìˆ˜Â·ë°±ë¶„ìœ„ ì‚°ì¶œ ë°©ì‹")
            
            return {
                "agent": self.name,
                "status": "success",
                "query": query,
                "result": result_text,
                "grade_info": raw_grade_info,
                "normalized_scores": normalized_scores,  # ì •ê·œí™”ëœ ì„±ì  ì¶”ê°€
                "sources": sources,
                "source_urls": [],
                "citations": citations
            }

        except Exception as e:
            _log(f"   âŒ ì»¨ì„¤íŒ… Agent ì˜¤ë¥˜: {e}")
            return {
                "agent": self.name,
                "status": "error",
                "result": str(e),
                "grade_info": raw_grade_info,
                "normalized_scores": normalized_scores,
                "sources": [],
                "source_urls": [],
                "citations": []
            }

    def _extract_grade_from_query(self, query: str) -> Dict[str, Any]:
        """
        ì¿¼ë¦¬ì—ì„œ ì„±ì  ì •ë³´ ì¶”ì¶œ
        
        ì§€ì› í˜•ì‹:
        - "ë“±ê¸‰ 132" -> êµ­ì–´ 1ë“±ê¸‰, ì˜ì–´ 3ë“±ê¸‰, ìˆ˜í•™ 2ë“±ê¸‰
        - "êµ­ì–´ 90ì  ìˆ˜í•™ ë¯¸ì ë¶„ 85ì "
        - "êµ­ì–´ 1ë“±ê¸‰ ìˆ˜í•™ í‘œì¤€ì ìˆ˜ 130"
        - "êµ­ì–´ ì–¸ì–´ì™€ë§¤ì²´ 92ì "
        """
        result = {
            "raw_input": query,
            "subjects": {},
            "ë‚´ì‹ ": None,
            "ì„ íƒê³¼ëª©_ì¶”ë¡ ": {}
        }

        # 1. "ë“±ê¸‰ XXX" íŒ¨í„´ ì²˜ë¦¬ (ì˜ˆ: "ë“±ê¸‰ 132", "13425", "ë‚˜ 13425ì•¼")
        # ìˆ«ìë§Œ 3~5ìë¦¬ì¸ íŒ¨í„´ ì°¾ê¸°
        compact_pattern = r'ë“±ê¸‰\s*(\d{3,5})|(\d{3,5})\s*ë“±ê¸‰|(?:ë‚˜|ì €)\s*(\d{3,5})|(\d{3,5})(?:ì•¼|ì´ì•¼|ì…ë‹ˆë‹¤|ìš”)'
        match = re.search(compact_pattern, query)
        if match:
            grade_str = match.group(1) or match.group(2) or match.group(3) or match.group(4)
            if grade_str and len(grade_str) >= 3:
                # êµ­/ìˆ˜/ì˜ ë˜ëŠ” êµ­/ìˆ˜/ì˜/íƒ1/íƒ2
                subjects_order = ["êµ­ì–´", "ìˆ˜í•™", "ì˜ì–´", "íƒêµ¬1", "íƒêµ¬2"]
                for i, char in enumerate(grade_str):
                    if i < len(subjects_order):
                        result["subjects"][subjects_order[i]] = {
                            "type": "ë“±ê¸‰",
                            "value": int(char)
                        }
        
        # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš°ë„ ì²˜ë¦¬ (ì˜ˆ: ë©”ì‹œì§€ì—ì„œ "13425" ê°™ì€ ìˆ«ìë§Œ)
        # ë‹¨, í‘œì¤€ì ìˆ˜/ë°±ë¶„ìœ„ í‚¤ì›Œë“œê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ
        if not result["subjects"] and "í‘œì¤€ì ìˆ˜" not in query and "ë°±ë¶„ìœ„" not in query and "ì " not in query:
            standalone_pattern = r'\b(\d{3,5})\b'
            matches = re.findall(standalone_pattern, query)
            for grade_str in matches:
                # ì—°ë„ê°€ ì•„ë‹Œì§€ í™•ì¸ (2024, 2025, 2026 ë“±)
                # ê·¸ë¦¬ê³  100 ì´ìƒì¸ ìˆ«ìëŠ” í‘œì¤€ì ìˆ˜ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ ì œì™¸
                if not (2020 <= int(grade_str) <= 2030) and int(grade_str) < 100:
                    subjects_order = ["êµ­ì–´", "ìˆ˜í•™", "ì˜ì–´", "íƒêµ¬1", "íƒêµ¬2"]
                    for i, char in enumerate(grade_str):
                        if i < len(subjects_order):
                            result["subjects"][subjects_order[i]] = {
                                "type": "ë“±ê¸‰",
                                "value": int(char)
                            }
                    break

        # 2. ê³¼ëª©ë³„ ì„±ì  ì¶”ì¶œ
        subject_keywords = {
            "êµ­ì–´": ["êµ­ì–´", "êµ­"],
            "ìˆ˜í•™": ["ìˆ˜í•™", "ìˆ˜"],
            "ì˜ì–´": ["ì˜ì–´", "ì˜"],
            "í•œêµ­ì‚¬": ["í•œêµ­ì‚¬", "í•œì‚¬"],
            "íƒêµ¬1": ["íƒêµ¬1"],
            "íƒêµ¬2": ["íƒêµ¬2"],
            # íƒêµ¬ ê³¼ëª©
            "ì‚¬íšŒë¬¸í™”": ["ì‚¬íšŒë¬¸í™”", "ì‚¬ë¬¸"],
            "ìƒí™œê³¼ìœ¤ë¦¬": ["ìƒí™œê³¼ìœ¤ë¦¬", "ìƒìœ¤"],
            "ìœ¤ë¦¬ì™€ì‚¬ìƒ": ["ìœ¤ë¦¬ì™€ì‚¬ìƒ", "ìœ¤ì‚¬"],
            "í•œêµ­ì§€ë¦¬": ["í•œêµ­ì§€ë¦¬", "í•œì§€"],
            "ì„¸ê³„ì§€ë¦¬": ["ì„¸ê³„ì§€ë¦¬", "ì„¸ì§€"],
            "ë™ì•„ì‹œì•„ì‚¬": ["ë™ì•„ì‹œì•„ì‚¬", "ë™ì•„ì‹œì•„"],
            "ì„¸ê³„ì‚¬": ["ì„¸ê³„ì‚¬"],
            "ì •ì¹˜ì™€ë²•": ["ì •ì¹˜ì™€ë²•", "ì •ë²•"],
            "ê²½ì œ": ["ê²½ì œ"],
            "ë¬¼ë¦¬í•™1": ["ë¬¼ë¦¬í•™1", "ë¬¼ë¦¬1", "ë¬¼1"],
            "ë¬¼ë¦¬í•™2": ["ë¬¼ë¦¬í•™2", "ë¬¼ë¦¬2", "ë¬¼2"],
            "í™”í•™1": ["í™”í•™1", "í™”1"],
            "í™”í•™2": ["í™”í•™2", "í™”2"],
            "ìƒëª…ê³¼í•™1": ["ìƒëª…ê³¼í•™1", "ìƒëª…1", "ìƒ1"],
            "ìƒëª…ê³¼í•™2": ["ìƒëª…ê³¼í•™2", "ìƒëª…2", "ìƒ2"],
            "ì§€êµ¬ê³¼í•™1": ["ì§€êµ¬ê³¼í•™1", "ì§€êµ¬1", "ì§€1"],
            "ì§€êµ¬ê³¼í•™2": ["ì§€êµ¬ê³¼í•™2", "ì§€êµ¬2", "ì§€2"],
        }

        # ì„ íƒê³¼ëª© í‚¤ì›Œë“œ
        elective_keywords = {
            "í™”ë²•ê³¼ì‘ë¬¸": ["í™”ë²•ê³¼ì‘ë¬¸", "í™”ì‘"],
            "ì–¸ì–´ì™€ë§¤ì²´": ["ì–¸ì–´ì™€ë§¤ì²´", "ì–¸ë§¤"],
            "í™•ë¥ ê³¼í†µê³„": ["í™•ë¥ ê³¼í†µê³„", "í™•í†µ"],
            "ë¯¸ì ë¶„": ["ë¯¸ì ë¶„", "ë¯¸ì "],
            "ê¸°í•˜": ["ê¸°í•˜"],
        }

        # ì„ íƒê³¼ëª© ì¶”ì¶œ
        detected_electives = {}
        for elective, keywords in elective_keywords.items():
            for kw in keywords:
                if kw in query:
                    if elective in ["í™”ë²•ê³¼ì‘ë¬¸", "ì–¸ì–´ì™€ë§¤ì²´"]:
                        detected_electives["êµ­ì–´"] = elective
                    else:
                        detected_electives["ìˆ˜í•™"] = elective
                    break
        
        result["ì„ íƒê³¼ëª©_ì¶”ë¡ "] = detected_electives

        # ê° ê³¼ëª©ë³„ ì ìˆ˜ ì¶”ì¶œ
        for subject, keywords in subject_keywords.items():
            if subject in result["subjects"]:
                continue  # ì´ë¯¸ ì¶”ì¶œëœ ê³¼ëª©ì€ ìŠ¤í‚µ
                
            for kw in keywords:
                # ë“±ê¸‰ íŒ¨í„´ (ë¨¼ì € ì²´í¬)
                grade_pattern = rf'{kw}\s*(\d)\s*ë“±ê¸‰|{kw}\s*ë“±ê¸‰\s*(\d)'
                match = re.search(grade_pattern, query)
                if match and subject not in result["subjects"]:
                    grade = match.group(1) or match.group(2)
                    result["subjects"][subject] = {
                        "type": "ë“±ê¸‰",
                        "value": int(grade)
                    }
                    break
                
                # í‘œì¤€ì ìˆ˜ íŒ¨í„´ (í‘œì¤€ì ìˆ˜, í‘œì  ëª…ì‹œ)
                std_pattern = rf'{kw}\s*(?:í‘œì¤€ì ìˆ˜|í‘œì )\s*(\d{{2,3}})'
                match = re.search(std_pattern, query)
                if match and subject not in result["subjects"]:
                    value = int(match.group(1))
                    result["subjects"][subject] = {"type": "í‘œì¤€ì ìˆ˜", "value": value}
                    break
                
                # ë°±ë¶„ìœ„ íŒ¨í„´
                pct_pattern = rf'{kw}\s*ë°±ë¶„ìœ„\s*(\d{{1,3}})'
                match = re.search(pct_pattern, query)
                if match and subject not in result["subjects"]:
                    result["subjects"][subject] = {
                        "type": "ë°±ë¶„ìœ„",
                        "value": int(match.group(1))
                    }
                    break
                
                # ì›ì ìˆ˜ íŒ¨í„´ (XXì )
                raw_pattern = rf'{kw}\s+(?:\w+\s+)?(\d{{2,3}})\s*ì '
                match = re.search(raw_pattern, query)
                if match and subject not in result["subjects"]:
                    value = int(match.group(1))
                    result["subjects"][subject] = {"type": "ì›ì ìˆ˜", "value": value}
                    break

        # 3. "íƒêµ¬ Xë“±ê¸‰" íŒ¨í„´ ì¶”ê°€ ì²˜ë¦¬ (íƒêµ¬1, íƒêµ¬2ê°€ ì•„ì§ ì¶”ì¶œë˜ì§€ ì•Šì€ ê²½ìš°)
        if "íƒêµ¬1" not in result["subjects"] or "íƒêµ¬2" not in result["subjects"]:
            # "íƒêµ¬" í‚¤ì›Œë“œ ë’¤ì— ë“±ê¸‰ì´ ì˜¤ëŠ” íŒ¨í„´ì„ ëª¨ë‘ ì°¾ê¸°
            inquiry_pattern = r'íƒêµ¬\s*(\d)\s*ë“±ê¸‰|íƒêµ¬\s*ë“±ê¸‰\s*(\d)'
            inquiry_matches = re.finditer(inquiry_pattern, query)
            
            inquiry_grades = []
            for match in inquiry_matches:
                grade_val = match.group(1) or match.group(2)
                inquiry_grades.append(int(grade_val))
            
            # ë°œê²¬ëœ íƒêµ¬ ë“±ê¸‰ì„ ìˆœì„œëŒ€ë¡œ íƒêµ¬1, íƒêµ¬2ì— í• ë‹¹
            if len(inquiry_grades) >= 1 and "íƒêµ¬1" not in result["subjects"]:
                result["subjects"]["íƒêµ¬1"] = {
                    "type": "ë“±ê¸‰",
                    "value": inquiry_grades[0]
                }
            if len(inquiry_grades) >= 2 and "íƒêµ¬2" not in result["subjects"]:
                result["subjects"]["íƒêµ¬2"] = {
                    "type": "ë“±ê¸‰",
                    "value": inquiry_grades[1]
                }

        # 4. ë‚´ì‹  ë“±ê¸‰ ì¶”ì¶œ
        grade_pattern = r'ë‚´ì‹ \s*(\d+\.?\d*)\s*ë“±ê¸‰?|(\d+\.?\d*)\s*ë“±ê¸‰\s*ë‚´ì‹ '
        match = re.search(grade_pattern, query)
        if match:
            grade = match.group(1) or match.group(2)
            result["ë‚´ì‹ "] = float(grade)

        # 5. ì„ íƒê³¼ëª© ê¸°ë³¸ê°’ ì¶”ë¡ 
        if "êµ­ì–´" not in result.get("ì„ íƒê³¼ëª©_ì¶”ë¡ ", {}):
            result["ì„ íƒê³¼ëª©_ì¶”ë¡ "]["êµ­ì–´"] = "í™”ë²•ê³¼ì‘ë¬¸"  # ê¸°ë³¸ê°’
        if "ìˆ˜í•™" not in result.get("ì„ íƒê³¼ëª©_ì¶”ë¡ ", {}):
            result["ì„ íƒê³¼ëª©_ì¶”ë¡ "]["ìˆ˜í•™"] = "í™•ë¥ ê³¼í†µê³„"  # ê¸°ë³¸ê°’
        
        # ìˆ˜í•™ ì„ íƒê³¼ëª©ì— ë”°ë¥¸ íƒêµ¬ ì¶”ë¡ 
        math_elective = result["ì„ íƒê³¼ëª©_ì¶”ë¡ "].get("ìˆ˜í•™", "í™•ë¥ ê³¼í†µê³„")
        if math_elective == "í™•ë¥ ê³¼í†µê³„":
            result["ì„ íƒê³¼ëª©_ì¶”ë¡ "]["íƒêµ¬_ì¶”ë¡ "] = "ì¸ë¬¸ê³„ (ì‚¬íšŒë¬¸í™”/ìƒí™œê³¼ìœ¤ë¦¬)"
        else:
            result["ì„ íƒê³¼ëª©_ì¶”ë¡ "]["íƒêµ¬_ì¶”ë¡ "] = "ìì—°ê³„ (ì§€êµ¬ê³¼í•™1/ìƒëª…ê³¼í•™1)"

        return result
    
    def _normalize_scores(self, raw_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì¶”ì¶œëœ ì„±ì ì„ ë“±ê¸‰-í‘œì¤€ì ìˆ˜-ë°±ë¶„ìœ„ë¡œ ì •ê·œí™”
        
        Args:
            raw_info: _extract_grade_from_queryì—ì„œ ì¶”ì¶œí•œ ì •ë³´
            
        Returns:
            ì •ê·œí™”ëœ ì„±ì  ì •ë³´
        """
        normalized = {
            "ê³¼ëª©ë³„_ì„±ì ": {},
            "ì¶”ì •_ê³¼ëª©": [],
            "ì„ íƒê³¼ëª©": raw_info.get("ì„ íƒê³¼ëª©_ì¶”ë¡ ", {})
        }
        
        subjects_data = raw_info.get("subjects", {})
        electives = raw_info.get("ì„ íƒê³¼ëª©_ì¶”ë¡ ", {})
        
        for subject, score_info in subjects_data.items():
            score_type = score_info.get("type")
            value = score_info.get("value")
            
            converted = None
            
            try:
                if subject in ["êµ­ì–´", "ìˆ˜í•™"]:
                    elective = electives.get(subject)
                    
                    if score_type == "ë“±ê¸‰":
                        # ë“±ê¸‰ -> í•´ë‹¹ ë“±ê¸‰ ì¤‘ê°„ ë°±ë¶„ìœ„ì˜ í‘œì¤€ì ìˆ˜ ì‚¬ìš©
                        converted = self._convert_grade_to_scores(subject, value)
                    elif score_type == "í‘œì¤€ì ìˆ˜":
                        converted = self.score_converter.convert_score(subject, standard_score=value)
                        if converted:
                            _log(f"   {subject} í‘œì¤€ì ìˆ˜ {value} -> ë“±ê¸‰ {converted.get('grade')}, ë°±ë¶„ìœ„ {converted.get('percentile')}")
                    elif score_type == "ë°±ë¶„ìœ„":
                        converted = self.score_converter.convert_score(subject, percentile=value)
                    elif score_type == "ì›ì ìˆ˜" and elective:
                        converted = self.score_converter.convert_score(
                            subject, raw_score=value, elective=elective
                        )
                        if converted:
                            _log(f"   {subject}({elective}) ì›ì ìˆ˜ {value} -> í‘œì¤€ì ìˆ˜ {converted.get('standard_score')}, ë“±ê¸‰ {converted.get('grade')}")
                
                elif subject == "ì˜ì–´":
                    # ì˜ì–´ëŠ” ì ˆëŒ€í‰ê°€
                    if score_type == "ë“±ê¸‰":
                        grade_data = english_grade_data.get(value, {})
                        converted = {
                            "standard_score": None,
                            "percentile": 100 - grade_data.get("ratio", 50),
                            "grade": value
                        }
                    elif score_type == "ì›ì ìˆ˜":
                        # ì›ì ìˆ˜ -> ë“±ê¸‰ ë³€í™˜
                        for grade, data in english_grade_data.items():
                            if value >= data.get("raw_cut", 0):
                                converted = {
                                    "standard_score": None,
                                    "percentile": 100 - data.get("ratio", 50),
                                    "grade": grade
                                }
                                break
                
                elif subject in self.score_converter.social_data:
                    if score_type == "ë“±ê¸‰":
                        converted = self._convert_grade_to_scores(subject, value)
                    elif score_type == "í‘œì¤€ì ìˆ˜":
                        converted = self.score_converter.convert_score(subject, standard_score=value)
                    elif score_type == "ë°±ë¶„ìœ„":
                        converted = self.score_converter.convert_score(subject, percentile=value)
                
                elif subject in self.score_converter.science_data:
                    if score_type == "ë“±ê¸‰":
                        converted = self._convert_grade_to_scores(subject, value)
                    elif score_type == "í‘œì¤€ì ìˆ˜":
                        converted = self.score_converter.convert_score(subject, standard_score=value)
                    elif score_type == "ë°±ë¶„ìœ„":
                        converted = self.score_converter.convert_score(subject, percentile=value)
                
                elif subject in ["íƒêµ¬1", "íƒêµ¬2"]:
                    # íƒêµ¬ ê³¼ëª©ì´ íŠ¹ì •ë˜ì§€ ì•Šì€ ê²½ìš°
                    if score_type == "ë“±ê¸‰":
                        converted = self._convert_grade_to_scores("íƒêµ¬_ê¸°ë³¸", value)
                
            except Exception as e:
                _log(f"   âš ï¸ {subject} ë³€í™˜ ì˜¤ë¥˜: {e}")
                converted = None
            
            if converted:
                normalized["ê³¼ëª©ë³„_ì„±ì "][subject] = {
                    "ì›ë³¸_ì…ë ¥": score_info,
                    "ë“±ê¸‰": converted.get("grade"),
                    "í‘œì¤€ì ìˆ˜": converted.get("standard_score"),
                    "ë°±ë¶„ìœ„": converted.get("percentile"),
                    "ì„ íƒê³¼ëª©": electives.get(subject) if subject in ["êµ­ì–´", "ìˆ˜í•™"] else None
                }
            else:
                # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì €ì¥
                normalized["ê³¼ëª©ë³„_ì„±ì "][subject] = {
                    "ì›ë³¸_ì…ë ¥": score_info,
                    "ë“±ê¸‰": value if score_type == "ë“±ê¸‰" else None,
                    "í‘œì¤€ì ìˆ˜": value if score_type == "í‘œì¤€ì ìˆ˜" else None,
                    "ë°±ë¶„ìœ„": value if score_type == "ë°±ë¶„ìœ„" else None,
                    "ë³€í™˜_ì‹¤íŒ¨": True
                }
        
        # ë¯¸ì…ë ¥ ê³¼ëª© ì¶”ì • (ë‹¤ë¥¸ ê³¼ëª©ë“¤ì˜ í‰ê·  ë°±ë¶„ìœ„ ê¸°ì¤€)
        normalized = self._estimate_missing_subjects(normalized)
        
        return normalized
    
    def _convert_grade_to_scores(self, subject: str, grade: int) -> Dict[str, Any]:
        """
        ë“±ê¸‰ì„ í‘œì¤€ì ìˆ˜/ë°±ë¶„ìœ„ë¡œ ë³€í™˜ (ë³´ìˆ˜ì  ì ‘ê·¼ - í•´ë‹¹ ë“±ê¸‰ ì¤‘ê°„ê°’ ì‚¬ìš©)
        
        ë“±ê¸‰ë³„ ë°±ë¶„ìœ„ ê¸°ì¤€:
        - 1ë“±ê¸‰: 96~100% -> ì¤‘ê°„ 98%
        - 2ë“±ê¸‰: 89~96% -> ì¤‘ê°„ 92.5%
        - 3ë“±ê¸‰: 77~89% -> ì¤‘ê°„ 83%
        - 4ë“±ê¸‰: 60~77% -> ì¤‘ê°„ 68.5%
        - 5ë“±ê¸‰: 40~60% -> ì¤‘ê°„ 50%
        - 6ë“±ê¸‰: 23~40% -> ì¤‘ê°„ 31.5%
        - 7ë“±ê¸‰: 11~23% -> ì¤‘ê°„ 17%
        - 8ë“±ê¸‰: 4~11% -> ì¤‘ê°„ 7.5%
        - 9ë“±ê¸‰: 0~4% -> ì¤‘ê°„ 2%
        """
        grade_to_mid_percentile = {
            1: 98,
            2: 92,
            3: 83,
            4: 68,
            5: 50,
            6: 31,
            7: 17,
            8: 7,
            9: 2
        }
        
        mid_percentile = grade_to_mid_percentile.get(grade, 50)
        
        # í•´ë‹¹ ë°±ë¶„ìœ„ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ í‘œì¤€ì ìˆ˜ ì°¾ê¸°
        result = self.score_converter.find_closest_by_percentile(subject, mid_percentile)
        
        if result:
            result["grade"] = grade  # ì›ë˜ ë“±ê¸‰ ìœ ì§€
            return result
        
        # íƒêµ¬ ê¸°ë³¸ê°’
        if subject == "íƒêµ¬_ê¸°ë³¸":
            # ì‚¬íšŒíƒêµ¬ ê¸°ë³¸ê°’ (ì‚¬íšŒë¬¸í™” ê¸°ì¤€)
            std_estimate = 50 + (mid_percentile - 50) * 0.2  # ëŒ€ëµì  ì¶”ì •
            return {
                "grade": grade,
                "standard_score": round(std_estimate),
                "percentile": mid_percentile
            }
        
        return {
            "grade": grade,
            "standard_score": None,
            "percentile": mid_percentile
        }
    
    def _estimate_missing_subjects(self, normalized: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë¯¸ì…ë ¥ ê³¼ëª©ì„ ë‹¤ë¥¸ ê³¼ëª©ë“¤ì˜ í‰ê·  ë°±ë¶„ìœ„ë¡œ ì¶”ì •
        """
        subjects = normalized.get("ê³¼ëª©ë³„_ì„±ì ", {})
        
        # ì…ë ¥ëœ ê³¼ëª©ë“¤ì˜ í‰ê·  ë°±ë¶„ìœ„ ê³„ì‚°
        percentiles = []
        for subj, data in subjects.items():
            pct = data.get("ë°±ë¶„ìœ„")
            if pct is not None:
                percentiles.append(pct)
        
        if not percentiles:
            return normalized
        
        avg_percentile = sum(percentiles) / len(percentiles)
        
        # í•„ìˆ˜ ê³¼ëª© í™•ì¸
        required = ["êµ­ì–´", "ìˆ˜í•™", "ì˜ì–´"]
        for subj in required:
            if subj not in subjects:
                # í‰ê·  ë°±ë¶„ìœ„ë¡œ ì¶”ì •
                if subj in ["êµ­ì–´", "ìˆ˜í•™"]:
                    estimated = self.score_converter.find_closest_by_percentile(subj, int(avg_percentile))
                    if estimated:
                        normalized["ê³¼ëª©ë³„_ì„±ì "][subj] = {
                            "ì›ë³¸_ì…ë ¥": None,
                            "ë“±ê¸‰": estimated.get("grade"),
                            "í‘œì¤€ì ìˆ˜": estimated.get("standard_score"),
                            "ë°±ë¶„ìœ„": estimated.get("percentile"),
                            "ì¶”ì •ë¨": True
                        }
                        normalized["ì¶”ì •_ê³¼ëª©"].append(subj)
                elif subj == "ì˜ì–´":
                    # ì˜ì–´ ë“±ê¸‰ ì¶”ì •
                    if avg_percentile >= 97:
                        est_grade = 1
                    elif avg_percentile >= 83:
                        est_grade = 2
                    elif avg_percentile >= 56:
                        est_grade = 3
                    elif avg_percentile >= 32:
                        est_grade = 4
                    else:
                        est_grade = 5
                    
                    normalized["ê³¼ëª©ë³„_ì„±ì "][subj] = {
                        "ì›ë³¸_ì…ë ¥": None,
                        "ë“±ê¸‰": est_grade,
                        "í‘œì¤€ì ìˆ˜": None,
                        "ë°±ë¶„ìœ„": avg_percentile,
                        "ì¶”ì •ë¨": True
                    }
                    normalized["ì¶”ì •_ê³¼ëª©"].append(subj)
        
        return normalized
    
    def _calculate_average_percentile(self, normalized: Dict[str, Any]) -> float:
        """ì •ê·œí™”ëœ ì„±ì ì—ì„œ í‰ê·  ë°±ë¶„ìœ„ ê³„ì‚°"""
        subjects = normalized.get("ê³¼ëª©ë³„_ì„±ì ", {})
        
        percentiles = []
        for subj, data in subjects.items():
            pct = data.get("ë°±ë¶„ìœ„")
            if pct is not None:
                percentiles.append(pct)
        
        if not percentiles:
            return None
        
        return sum(percentiles) / len(percentiles)
    
    def _format_normalized_scores(self, normalized: Dict[str, Any]) -> str:
        """ì •ê·œí™”ëœ ì„±ì ì„ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        lines = []
        
        subjects = normalized.get("ê³¼ëª©ë³„_ì„±ì ", {})
        electives = normalized.get("ì„ íƒê³¼ëª©", {})
        estimated = normalized.get("ì¶”ì •_ê³¼ëª©", [])
        
        for subj, data in subjects.items():
            grade = data.get("ë“±ê¸‰")
            std = data.get("í‘œì¤€ì ìˆ˜")
            pct = data.get("ë°±ë¶„ìœ„")
            elective = data.get("ì„ íƒê³¼ëª©") or electives.get(subj)
            is_estimated = data.get("ì¶”ì •ë¨", False) or subj in estimated
            
            # ê³¼ëª©ëª… í¬ë§·
            if elective:
                subj_name = f"{subj}({elective})"
            else:
                subj_name = subj
            
            # ì ìˆ˜ í¬ë§·
            parts = []
            if grade is not None:
                parts.append(f"{grade}ë“±ê¸‰")
            if std is not None:
                parts.append(f"í‘œì¤€ì ìˆ˜ {std}")
            elif subj == "ì˜ì–´":
                parts.append("í‘œì¤€ì ìˆ˜ ì—†ìŒ(ì ˆëŒ€í‰ê°€)")
            if pct is not None:
                parts.append(f"ë°±ë¶„ìœ„ {round(pct, 1)}")
            
            score_text = " / ".join(parts) if parts else "ì •ë³´ ì—†ìŒ"
            
            if is_estimated:
                score_text += " (ì¶”ì •)"
            
            lines.append(f"- {subj_name}: {score_text}")
        
        if not lines:
            return "ì„±ì  ì •ë³´ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        return "\n".join(lines)


class TeacherAgent(SubAgentBase):
    """ì„ ìƒë‹˜ Agent - í•™ìŠµ ê³„íš ë° ë©˜íƒˆ ê´€ë¦¬ ì¡°ì–¸"""

    def __init__(self, custom_system_prompt: str = None):
        super().__init__(
            name="ì„ ìƒë‹˜ agent",
            description="í˜„ì‹¤ì ì¸ ëª©í‘œ ì„¤ì • ë° ê³µë¶€ ê³„íš ìˆ˜ë¦½, ë©˜íƒˆ ê´€ë¦¬",
            custom_system_prompt=custom_system_prompt
        )

    async def execute(self, query: str) -> Dict[str, Any]:
        """í•™ìŠµ ê³„íš ë° ì¡°ì–¸ ì œê³µ"""
        _log("")
        _log("="*60)
        _log(f"ğŸ‘¨â€ğŸ« ì„ ìƒë‹˜ Agent ì‹¤í–‰")
        _log("="*60)
        _log(f"ì¿¼ë¦¬: {query}")

        if self.custom_system_prompt:
            system_prompt = self.custom_system_prompt
            print(f"ğŸ¨ Using custom system prompt for teacher agent")
        else:
            system_prompt = """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ì…ì‹œ ì „ë¬¸ ì„ ìƒë‹˜ì…ë‹ˆë‹¤.
í•™ìƒì˜ ìƒí™©ì„ íŒŒì•…í•˜ê³  í˜„ì‹¤ì ì´ë©´ì„œë„ í¬ë§ì„ ìƒì§€ ì•ŠëŠ” ì¡°ì–¸ì„ í•´ì£¼ì„¸ìš”.

## ì¡°ì–¸ ì›ì¹™
1. í˜„ì‹¤ì ì¸ ëª©í‘œ ì„¤ì • (ë¬´ë¦¬í•œ ëª©í‘œëŠ” ì§€ì )
2. êµ¬ì²´ì ì¸ ì‹œê°„í‘œì™€ ê³„íš ì œì‹œ
3. ë©˜íƒˆ ê´€ë¦¬ ì¡°ì–¸ í¬í•¨
4. ë‹¨ê¸°/ì¤‘ê¸°/ì¥ê¸° ëª©í‘œ êµ¬ë¶„
5. í¬ê¸°í•˜ì§€ ì•Šë„ë¡ ê²©ë ¤í•˜ë˜, ê±°ì§“ í¬ë§ì€ ì£¼ì§€ ì•Šê¸°

## ì¶œë ¥ í˜•ì‹
- ìì—°ì–´ë¡œ ì¹œê·¼í•˜ê²Œ ì‘ì„±
- í•„ìš”ì‹œ ë¦¬ìŠ¤íŠ¸ë‚˜ í‘œ ì‚¬ìš©
- ì¡´ëŒ“ë§ ì‚¬ìš©"""

        try:
            response = self.model.generate_content(
                f"{system_prompt}\n\ní•™ìƒ ì§ˆë¬¸: {query}\n\nì„ ìƒë‹˜ìœ¼ë¡œì„œ ì¡°ì–¸í•´ì£¼ì„¸ìš”.",
                generation_config={"temperature": 0.7},
                request_options=genai.types.RequestOptions(
                    retry=None,
                    timeout=120.0  # ë©€í‹°ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ì„ ìœ„í•´ 120ì´ˆë¡œ ì¦ê°€
                )
            )

            # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
            if hasattr(response, 'usage_metadata'):
                usage = response.usage_metadata
                print(f"ğŸ’° í† í° ì‚¬ìš©ëŸ‰ ({self.name}): {usage}")
                
                log_token_usage(
                    operation="ì„ ìƒë‹˜ì—ì´ì „íŠ¸",
                    prompt_tokens=getattr(usage, 'prompt_token_count', 0),
                    output_tokens=getattr(usage, 'candidates_token_count', 0),
                    total_tokens=getattr(usage, 'total_token_count', 0),
                    model="gemini-3-flash-preview",
                    details=self.name
                )

            _log(f"   ì¡°ì–¸ ì™„ë£Œ")
            _log("="*60)

            return {
                "agent": self.name,
                "status": "success",
                "query": query,
                "result": response.text,
                "sources": [],
                "source_urls": [],
                "citations": []
            }

        except Exception as e:
            return {
                "agent": self.name,
                "status": "error",
                "result": str(e),
                "sources": [],
                "source_urls": [],
                "citations": []
            }


# ============================================================
# Agent Factory
# ============================================================

def get_agent(agent_name: str) -> SubAgentBase:
    """ì—ì´ì „íŠ¸ ì´ë¦„ìœ¼ë¡œ ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    agent_name_lower = agent_name.lower()

    # ëŒ€í•™ë³„ Agent
    for univ in UniversityAgent.SUPPORTED_UNIVERSITIES:
        if univ in agent_name:
            return UniversityAgent(univ)

    # ì»¨ì„¤íŒ… Agent
    if "ì»¨ì„¤íŒ…" in agent_name or "ì»¨ì„¤í„´íŠ¸" in agent_name:
        return ConsultingAgent()

    # ì„ ìƒë‹˜ Agent
    if "ì„ ìƒë‹˜" in agent_name or "ì„ ìƒ" in agent_name:
        return TeacherAgent()

    raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ì—ì´ì „íŠ¸: {agent_name}")


async def execute_sub_agents(execution_plan: list) -> Dict[str, Any]:
    """
    Execution Planì— ë”°ë¼ Sub Agentë“¤ ì‹¤í–‰
    
    Args:
        execution_plan: Orchestration Agentê°€ ìƒì„±í•œ ì‹¤í–‰ ê³„íš
        
    Returns:
        {
            "Step1_Result": {...},
            "Step2_Result": {...},
            ...
        }
    """
    results = {}

    for step in execution_plan:
        step_num = step.get("step")
        agent_name = step.get("agent")
        query = step.get("query")

        _log(f"   Step {step_num}: {agent_name}")
        _log(f"   Query: {query}")

        try:
            agent = get_agent(agent_name)
            result = await agent.execute(query)
            results[f"Step{step_num}_Result"] = result
            
            status_icon = "âœ…" if result.get('status') == 'success' else "âŒ"
            _log(f"   {status_icon} Status: {result.get('status')}")
            sources_count = len(result.get('sources', []))
            if sources_count > 0:
                _log(f"   ì¶œì²˜: {sources_count}ê°œ")
            
        except Exception as e:
            _log(f"   âŒ Error: {e}")
            results[f"Step{step_num}_Result"] = {
                "agent": agent_name,
                "status": "error",
                "result": str(e),
                "sources": [],
                "source_urls": [],
                "citations": []
            }

    return results
