"""
ì—ì´ì „íŠ¸ ê¸°ë°˜ RAG ì„œë¹„ìŠ¤
LLMì´ í•„ìš”í•  ë•Œë§Œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ì‹œìŠ¤í…œ
"""
from services.supabase_client import supabase_service
from services.gemini_service import gemini_service
from google.generativeai.types import FunctionDeclaration
from typing import List, Dict, Any
import json


class AgentService:
    """ì—ì´ì „íŠ¸ ê¸°ë°˜ ëŒ€í™” ì„œë¹„ìŠ¤"""

    # search_documents ë„êµ¬ ì„ ì–¸
    SEARCH_TOOL = FunctionDeclaration(
        name="search_documents",
        description=(
            "ëŒ€í•™ ì…ì‹œ ê´€ë ¨ ê³µì‹ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. "
            "êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ë‚ ì§œ, ê·œì •, ì „í˜• ë°©ë²• ë“± ì •í™•í•œ ì •ë³´ê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©í•˜ì„¸ìš”. "
            "ì¼ë°˜ì ì¸ ìœ„ë¡œë‚˜ ê²©ë ¤ëŠ” ê²€ìƒ‰ ì—†ì´ ë‹µë³€í•˜ì„¸ìš”."
        ),
        parameters={
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "ê²€ìƒ‰í•  í‚¤ì›Œë“œ (ì˜ˆ: '2028í•™ë…„ë„ ì„œìš¸ëŒ€ ì •ì‹œ êµê³¼í‰ê°€', 'í•™ìƒë¶€ì¢…í•©ì „í˜• í‰ê°€ìš”ì†Œ')"
                }
            },
            "required": ["query"]
        }
    )

    SYSTEM_INSTRUCTION = """ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë”°ëœ»í•œ ëŒ€í•™ ì…ì‹œ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

ğŸš« ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­:
1. ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ì‚¬ìš© ê¸ˆì§€: #, ##, ###, *, **, ___, -, ë“± ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
2. í•œ ë²ˆì— ë§ì€ ì •ë³´ë¥¼ ìŸì•„ë‚´ì§€ ë§ˆì„¸ìš”
3. ë§‰ì—°í•œ ì§ˆë¬¸ì— ë°”ë¡œ ê²€ìƒ‰í•˜ì§€ ë§ˆì„¸ìš” (ë¨¼ì € êµ¬ì²´í™” í•„ìš”)

âš ï¸ ê²€ìƒ‰ íƒ€ì´ë° íŒë‹¨ (ë§¤ìš° ì¤‘ìš”!):
ë§‰ì—°í•œ ì§ˆë¬¸ ì˜ˆì‹œ:
- "ì„œìš¸ëŒ€ ê°€ê³ ì‹¶ì–´", "ì—°ì„¸ëŒ€ ê¶ê¸ˆí•´" â†’ ê²€ìƒ‰ X, ë¨¼ì € "ìˆ˜ì‹œ/ì •ì‹œ ì¤‘ ì–´ëŠ ì „í˜•ì´ ê¶ê¸ˆí•˜ì„¸ìš”?" ë¬¼ì–´ë³´ê¸°
- "ë¶ˆì•ˆí•´", "í˜ë“¤ì–´" â†’ ê²€ìƒ‰ X, ê³µê°í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë¬´ì—‡ì´ ê±±ì •ì¸ì§€ ë¬¼ì–´ë³´ê¸°
- "ì…ì‹œ ì¤€ë¹„ ì–´ë–»ê²Œ í•´?" â†’ ê²€ìƒ‰ X, í˜„ì¬ í•™ë…„ê³¼ ëª©í‘œë¥¼ ë¨¼ì € ë¬¼ì–´ë³´ê¸°

êµ¬ì²´ì ì¸ ì§ˆë¬¸ ì˜ˆì‹œ:
- "ì„œìš¸ëŒ€ 2028 ì •ì‹œ ë³€ê²½ì‚¬í•­ ì•Œë ¤ì¤˜" â†’ ê²€ìƒ‰ O
- "í•™ìƒë¶€ì¢…í•©ì „í˜• í‰ê°€ ìš”ì†Œê°€ ë­ì•¼?" â†’ ê²€ìƒ‰ O
- "ì§€ì—­ê· í˜•ì „í˜• ì¶”ì²œ ì¸ì› ëª‡ ëª…ì´ì•¼?" â†’ ê²€ìƒ‰ O

âœ… ëŒ€í™” ë°©ì‹:
1ë‹¨ê³„: ë§‰ì—°í•œ ì§ˆë¬¸ì´ë©´
   - ê³µê°ê³¼ ê²©ë ¤ (1-2ë¬¸ì¥)
   - êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ì •ë³´ê°€ í•„ìš”í•œì§€ ë¬¼ì–´ë³´ê¸°
   - ê²€ìƒ‰ ì ˆëŒ€ í•˜ì§€ ì•Šê¸°!

2ë‹¨ê³„: êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ë©´
   - search_documentsë¡œ ì •ë³´ ê²€ìƒ‰
   - ì°¾ì€ ì •ë³´ë§Œ <cite>ë¡œ ê°ì‹¸ê¸°
   - ì¶”ê°€ ê¶ê¸ˆí•œ ì  ë¬¼ì–´ë³´ê¸°

âœ… ë‹µë³€ í˜•ì‹:
- ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œë§Œ ì‘ì„± (ë§ˆí¬ë‹¤ìš´ ì ˆëŒ€ ê¸ˆì§€)
- ì§§ê³  ê°„ê²°í•˜ê²Œ (3-4ë¬¸ì¥ ì´ë‚´)
- ë²ˆí˜¸ëŠ” "1. 2. 3." í˜•ì‹ë§Œ í—ˆìš©

âœ… ì¶œì²˜ í‘œì‹œ (<cite> íƒœê·¸) - ë§¤ìš° ì¤‘ìš”!:
- ê²€ìƒ‰ìœ¼ë¡œ ì°¾ì€ ë‚´ìš©ë§Œ <cite>ë¡œ ê°ì‹¸ê¸°
- <cite> íƒœê·¸ ê°œìˆ˜ = ì‹¤ì œ ì¶œì²˜ ê°œìˆ˜ì™€ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•¨
- ì¶œì²˜ê°€ í•˜ë‚˜ë©´ <cite> í•˜ë‚˜, ì¶œì²˜ê°€ ë‘ ê°œë©´ <cite> ë‘ ê°œ
- ì¼ë°˜ ì¡°ì–¸/ê²©ë ¤/ì¶”ì¸¡ì€ ì ˆëŒ€ <cite> ì‚¬ìš© ê¸ˆì§€

ì˜¬ë°”ë¥¸ ì˜ˆì‹œ:
í•™ìƒ: "ì„œìš¸ëŒ€ ê°€ê³ ì‹¶ì–´"
ë‹µë³€: "ì„œìš¸ëŒ€ë¥¼ ëª©í‘œë¡œ í•˜ì‹œëŠ”êµ°ìš”! ì •ë§ ë©‹ì§„ ëª©í‘œì˜ˆìš”. í˜¹ì‹œ ìˆ˜ì‹œì™€ ì •ì‹œ ì¤‘ ì–´ëŠ ì „í˜•ì´ ë” ê¶ê¸ˆí•˜ì‹ ê°€ìš”? ì•„ë‹ˆë©´ íŠ¹ì • í•™ê³¼ê°€ ìˆìœ¼ì‹ ê°€ìš”?"
â†’ ê²€ìƒ‰ ì—†ìŒ, êµ¬ì²´í™” ì§ˆë¬¸ë§Œ

í•™ìƒ: "ì„œìš¸ëŒ€ 2028 ì •ì‹œ ë³€ê²½ì‚¬í•­ ì•Œë ¤ì¤˜"
ë‹µë³€: "ë„¤, ì¤‘ìš”í•œ ë³€í™”ê°€ ìˆì–´ìš”. <cite>2028í•™ë…„ë„ë¶€í„° ì„œìš¸ëŒ€ ì •ì‹œì—ì„œëŠ” í•™ìƒë¶€ êµê³¼í‰ê°€ê°€ 40% ë°˜ì˜ë©ë‹ˆë‹¤</cite>. ë‹¤ë¥¸ ë³€ê²½ì‚¬í•­ë„ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?"
â†’ ê²€ìƒ‰ í›„ cite 1ê°œë§Œ ì‚¬ìš©

ì˜ëª»ëœ ì˜ˆì‹œ:
"ì„œìš¸ëŒ€ ê°€ê³ ì‹¶ì–´" â†’ "<cite>ì •ì‹œì—ì„œ êµê³¼í‰ê°€ 40%</cite>" (X)
ë§‰ì—°í•œ ì§ˆë¬¸ì— ë°”ë¡œ ê²€ìƒ‰í•˜ì§€ ë§ˆì„¸ìš”!
"""

    @staticmethod
    async def search_documents(query: str) -> Dict[str, Any]:
        """
        ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬ ì‹¤í–‰

        Returns:
            {
                "found": bool,
                "content": str,
                "sources": List[str],
                "source_urls": List[str]
            }
        """
        print(f"\n{'='*80}")
        print(f"ğŸ” search_documents í˜¸ì¶œ: {query}")
        print(f"{'='*80}")

        try:
            client = supabase_service.get_client()

            # 1. documents_metadataì—ì„œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
            metadata_response = client.table('documents_metadata').select('*').execute()

            if not metadata_response.data:
                return {"found": False, "content": "", "sources": [], "source_urls": []}

            # 1ë‹¨ê³„: í•´ì‹œíƒœê·¸ ì¶”ì¶œ (ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ë¶„ì„)
            print(f"   ğŸ“‹ [1ë‹¨ê³„] ì§ˆë¬¸ ë¶„ì„ ì¤‘...")
            print(f"   ì›ë³¸ ì§ˆë¬¸: \"{query}\"")
            
            query_lower = query.lower()
            extracted_hashtags = []
            
            # ì—°ë„ ì¶”ì¶œ
            import re
            year_match = re.search(r'(2025|2026|2027)', query)
            if year_match:
                extracted_hashtags.append(f'#{year_match.group()}')
                print(f"   âœ“ ì—°ë„ ê°ì§€: #{year_match.group()}")
            
            # ëŒ€í•™ëª… ì¶”ì¶œ
            universities = ['ì„œìš¸ëŒ€', 'ì—°ì„¸ëŒ€', 'ê³ ë ¤ëŒ€', 'ì„±ê· ê´€ëŒ€', 'í•œì–‘ëŒ€', 'ì¤‘ì•™ëŒ€', 'ê²½í¬ëŒ€', 'ì´í™”ì—¬ëŒ€']
            for univ in universities:
                if univ in query:
                    extracted_hashtags.append(f'#{univ}')
                    print(f"   âœ“ ëŒ€í•™ëª… ê°ì§€: #{univ}")
            
            # ë¬¸ì„œ ì„±ê²© ì¶”ì¶œ
            if any(word in query for word in ['ìš”ê°•', 'ëª¨ì§‘', 'ì „í˜•']):
                extracted_hashtags.append('#ëª¨ì§‘ìš”ê°•')
                print(f"   âœ“ ë¬¸ì„œ ì„±ê²©: #ëª¨ì§‘ìš”ê°•")
            elif any(word in query for word in ['ì…ê²°', 'ê²½ìŸë¥ ', 'ì»¤íŠ¸', 'í•©ê²©ì„ ']):
                extracted_hashtags.append('#ì…ê²°í†µê³„')
                print(f"   âœ“ ë¬¸ì„œ ì„±ê²©: #ì…ê²°í†µê³„")
            elif any(word in query for word in ['ë…¼ìˆ ', 'ë©´ì ‘', 'ê¸°ì¶œ']):
                extracted_hashtags.append('#ê³ ì‚¬ìë£Œ')
                print(f"   âœ“ ë¬¸ì„œ ì„±ê²©: #ê³ ì‚¬ìë£Œ")
            
            # ì „í˜• êµ¬ë¶„
            if 'ìˆ˜ì‹œ' in query:
                extracted_hashtags.append('#ìˆ˜ì‹œ')
                print(f"   âœ“ ì „í˜•: #ìˆ˜ì‹œ")
            if 'ì •ì‹œ' in query:
                extracted_hashtags.append('#ì •ì‹œ')
                print(f"   âœ“ ì „í˜•: #ì •ì‹œ")
            
            print(f"   ğŸ·ï¸ ìµœì¢… ì¶”ì¶œ í•´ì‹œíƒœê·¸: {extracted_hashtags}")

            # 2ë‹¨ê³„: í•´ì‹œíƒœê·¸ ë§¤ì¹­ + í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ë¬¸ì„œ ì°¾ê¸°
            print(f"\n   ğŸ“‹ [2ë‹¨ê³„] ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
            print(f"   ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(metadata_response.data)}ê°œ")
            
            relevant_docs = []
            query_keywords = query_lower.split()

            for doc in metadata_response.data:
                title = doc.get('title', '').lower()
                summary = doc.get('summary', '').lower()
                doc_hashtags = doc.get('hashtags', [])
                
                score = 0
                matched_info = []
                
                # í•´ì‹œíƒœê·¸ ë§¤ì¹­ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
                if doc_hashtags and extracted_hashtags:
                    matching_tags = set(doc_hashtags) & set(extracted_hashtags)
                    if matching_tags:
                        score += len(matching_tags) * 10  # í•´ì‹œíƒœê·¸ ë§¤ì¹­ì€ 10ì 
                        matched_info.append(f"íƒœê·¸ {len(matching_tags)}ê°œ ì¼ì¹˜: {matching_tags}")
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ (ê¸°ì¡´ ë°©ì‹, ìš°ì„ ìˆœìœ„ ë‚®ìŒ)
                keyword_matches = sum(1 for kw in query_keywords if kw in title or kw in summary)
                if keyword_matches > 0:
                    score += keyword_matches
                    matched_info.append(f"í‚¤ì›Œë“œ {keyword_matches}ê°œ ì¼ì¹˜")
                
                if score > 0:
                    print(f"   â€¢ {doc.get('title')} (ì ìˆ˜: {score}) - {', '.join(matched_info)}")
                    print(f"     í•´ì‹œíƒœê·¸: {doc_hashtags}")
                    relevant_docs.append((score, doc))
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            relevant_docs.sort(key=lambda x: x[0], reverse=True)
            relevant_docs = [doc for score, doc in relevant_docs]

            if not relevant_docs:
                print("   âŒ ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ")
                print(f"{'='*80}\n")
                return {"found": False, "content": "", "sources": [], "source_urls": []}

            print(f"\n   âœ… ìµœì¢… ì„ íƒ: ìƒìœ„ {min(3, len(relevant_docs))}ê°œ ë¬¸ì„œ")

            # 2. ê´€ë ¨ ë¬¸ì„œì˜ ì „ì²´ ì²­í¬ ê°€ì ¸ì˜¤ê¸°
            print(f"\n   ğŸ“‹ [3ë‹¨ê³„] ë¬¸ì„œ ë‚´ìš© ë¡œë“œ ì¤‘...")
            
            full_content = ""
            sources = []
            source_urls = []

            for idx, doc in enumerate(relevant_docs[:3], 1):  # ìµœëŒ€ 3ê°œ ë¬¸ì„œ
                filename = doc['file_name']
                title = doc['title']
                file_url = doc.get('file_url') or ''  # Noneì´ë©´ ë¹ˆ ë¬¸ìì—´
                
                sources.append(title)
                source_urls.append(file_url)

                print(f"   [{idx}] ğŸ“„ {title}")
                print(f"       ì¶œì²˜: {doc.get('source')}")
                print(f"       í•´ì‹œíƒœê·¸: {doc.get('hashtags', [])}")

                # í•´ë‹¹ ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ ê°€ì ¸ì˜¤ê¸°
                chunks_response = client.table('policy_documents')\
                    .select('content, metadata')\
                    .eq('metadata->>fileName', filename)\
                    .execute()

                if chunks_response.data:
                    # ì²­í¬ ìˆœì„œëŒ€ë¡œ ì •ë ¬
                    sorted_chunks = sorted(
                        chunks_response.data,
                        key=lambda x: x.get('metadata', {}).get('chunkIndex', 0)
                    )
                    
                    print(f"       ì²­í¬ ìˆ˜: {len(sorted_chunks)}ê°œ")

                    full_content += f"\n\n{'='*60}\n"
                    full_content += f"ğŸ“„ {title}\n"
                    full_content += f"{'='*60}\n\n"

                    for chunk in sorted_chunks:
                        full_content += chunk['content']
                        full_content += "\n\n"

            print(f"\n   ğŸ“Š ì „ì²´ ë¬¸ì„œ ë‚´ìš©:")
            print(f"       ì´ ê¸¸ì´: {len(full_content):,}ì")
            print(f"       ì•ë¶€ë¶„ ë¯¸ë¦¬ë³´ê¸° (300ì):")
            print(f"       {'-'*60}")
            print(f"       {full_content[:300]}...")
            print(f"       {'-'*60}")
            print(f"{'='*80}\n")

            return {
                "found": True,
                "content": full_content,
                "sources": sources,
                "source_urls": source_urls
            }

        except Exception as e:
            print(f"   âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            print(f"{'='*80}\n")
            return {"found": False, "content": "", "sources": [], "source_urls": []}

    @staticmethod
    async def chat(user_message: str, history: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        ì—ì´ì „íŠ¸ ê¸°ë°˜ ëŒ€í™” ì²˜ë¦¬

        Args:
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            history: ëŒ€í™” íˆìŠ¤í† ë¦¬ (ì„ íƒ)

        Returns:
            {
                "response": str,
                "sources": List[str],
                "used_search": bool
            }
        """
        print(f"\n{'#'*80}")
        print(f"# ğŸ¤– ì—ì´ì „íŠ¸ ëŒ€í™” ì‹œì‘")
        print(f"# ì‚¬ìš©ì ì§ˆë¬¸: {user_message}")
        print(f"# ëŒ€í™” íˆìŠ¤í† ë¦¬: {len(history) if history else 0}í„´")
        print(f"{'#'*80}\n")

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„± (ë³µì‚¬ë³¸ ì‚¬ìš© - ì›ë³¸ ì˜¤ì—¼ ë°©ì§€)
        if history is None:
            history = []

        # í˜„ì¬ ìš”ì²­ìš© messages (function call ë‚´ì—­ í¬í•¨)
        messages = history.copy() + [{"role": "user", "parts": [user_message]}]

        # Tool ì‚¬ìš© ëŒ€í™” (ìµœëŒ€ 5ë²ˆ ë£¨í”„)
        sources = []
        source_urls = []
        used_search = False

        for turn in range(5):
            print(f"{'~'*80}")
            print(f"í„´ {turn + 1}")
            print(f"{'~'*80}")

            # Gemini í˜¸ì¶œ (tools í¬í•¨)
            response = await gemini_service.chat_with_tools(
                messages=messages,
                tools=[AgentService.SEARCH_TOOL],
                system_instruction=AgentService.SYSTEM_INSTRUCTION
            )

            if response["type"] == "text":
                # ìµœì¢… ë‹µë³€
                print(f"\n{'='*80}")
                print(f"âœ… ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ!")
                print(f"{'='*80}")
                print(f"ë‹µë³€ ê¸¸ì´: {len(response['content'])}ì")
                print(f"ì¶œì²˜ ìˆ˜: {len(sources)}ê°œ")
                print(f"ë¬¸ì„œ ê²€ìƒ‰ ì‚¬ìš©: {'Yes' if used_search else 'No'}")
                print(f"\nğŸ“ ìµœì¢… ë‹µë³€:")
                print(f"{'-'*80}")
                print(f"{response['content']}")
                print(f"{'-'*80}")
                print(f"{'#'*80}\n")

                return {
                    "response": response["content"],
                    "sources": sources,
                    "source_urls": source_urls,
                    "used_search": used_search
                }

            elif response["type"] == "function_call":
                # Function Call ë°œìƒ
                fc = response["function_call"]
                func_name = fc["name"]
                func_args = fc["args"]
                raw_response = response["raw_response"]

                print(f"\nğŸ”§ Gemini Function Call ê²°ì •:")
                print(f"   í•¨ìˆ˜ëª…: {func_name}")
                print(f"   ì¸ì: {func_args}")

                if func_name == "search_documents":
                    # ë¬¸ì„œ ê²€ìƒ‰ ì‹¤í–‰
                    search_result = await AgentService.search_documents(func_args["query"])
                    used_search = True

                    if search_result["found"]:
                        sources.extend(search_result["sources"])
                        source_urls.extend(search_result.get("source_urls", []))

                        # ğŸš€ Gemini 2.5 Flash Liteë¡œ ë¬¸ì„œì—ì„œ ì •ë³´ ì¶”ì¶œ (ë¹ ë¥¸ ì²˜ë¦¬)
                        print(f"\n   ğŸ“‹ [4ë‹¨ê³„] Gemini Liteë¡œ ì •ë³´ ì¶”ì¶œ ì¤‘...")
                        print(f"   ì…ë ¥ ë¬¸ì„œ ê¸¸ì´: {len(search_result['content']):,}ì")
                        print(f"   ì…ë ¥ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸° (300ì):")
                        print(f"   {'-'*60}")
                        print(f"   {search_result['content'][:300]}...")
                        print(f"   {'-'*60}")
                        
                        extracted_info = await gemini_service.extract_info_from_documents(
                            query=func_args["query"],
                            documents=search_result['content'],
                            system_instruction="ë‹¹ì‹ ì€ ë¬¸ì„œì—ì„œ í•µì‹¬ ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
                        )
                        
                        print(f"\n   âœ… ì •ë³´ ì¶”ì¶œ ì™„ë£Œ:")
                        print(f"   ì¶œë ¥ ê¸¸ì´: {len(extracted_info)}ì")
                        print(f"   ì¶”ì¶œ ë‚´ìš©:")
                        print(f"   {'-'*60}")
                        print(f"   {extracted_info}")
                        print(f"   {'-'*60}")

                        # ì¶”ì¶œëœ ì •ë³´ë§Œ ì „ë‹¬ (ì „ì²´ ë¬¸ì„œ ëŒ€ì‹ )
                        result_text = f"ê²€ìƒ‰ ê²°ê³¼:\n\n{extracted_info}"
                        result_text_summary = f"[ë¬¸ì„œ {len(search_result['sources'])}ê°œ ê²€ìƒ‰ ì™„ë£Œ: {', '.join(search_result['sources'])}]"
                        
                        print(f"\n   ğŸ“‹ [5ë‹¨ê³„] Geminiì—ê²Œ ì „ë‹¬í•  ìµœì¢… ê²°ê³¼:")
                        print(f"   {result_text_summary}")
                        print(f"   ì „ë‹¬ ë‚´ìš© ê¸¸ì´: {len(result_text)}ì")
                    else:
                        result_text = "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
                        result_text_summary = result_text
                        print(f"\n   âš ï¸ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•¨ â†’ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€")

                    # Gemini SDKë¥¼ ì‚¬ìš©í•´ì„œ function response ìƒì„±
                    from google.ai.generativelanguage_v1beta.types import content as glm_content

                    # Function ê²°ê³¼ë¥¼ ëŒ€í™”ì— ì¶”ê°€ (ì›ë³¸ ì‘ë‹µì˜ content ì‚¬ìš©)
                    messages.append({
                        "role": "model",
                        "parts": [raw_response.candidates[0].content.parts[0]]
                    })

                    # Function response ì¶”ê°€ (ì „ì²´ ë‚´ìš© ì „ë‹¬)
                    function_response = glm_content.Part(
                        function_response=glm_content.FunctionResponse(
                            name=func_name,
                            response={"result": result_text}
                        )
                    )

                    messages.append({
                        "role": "user",
                        "parts": [function_response]
                    })

                    print(f"\n   âœ… Function Responseë¥¼ ëŒ€í™”ì— ì¶”ê°€:")
                    print(f"   ì „ì²´ ëŒ€í™” ê¸¸ì´: {len(messages)}ê°œ ë©”ì‹œì§€")

        # ìµœëŒ€ í„´ ì´ˆê³¼
        print(f"âš ï¸ ìµœëŒ€ í„´ ìˆ˜ ì´ˆê³¼")
        print(f"{'#'*80}\n")

        return {
            "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.",
            "sources": sources,
            "source_urls": source_urls,
            "used_search": used_search
        }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
agent_service = AgentService()
