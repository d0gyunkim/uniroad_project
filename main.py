"""
ëª©ì°¨ ê¸°ë°˜ ë™ì  ë¼ìš°íŒ… RAG ì‹œìŠ¤í…œ - ë©”ì¸ ì‹¤í–‰ íŒŒì¼
Streamlit ì•± ì§„ì…ì 
"""
import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_teddynote import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
import os

from core import (
    TOCProcessor,
    SectionPreprocessor,
    RAGSystem,
    SearchEngine
)
import config

# í”„ë¡œì íŠ¸ ì´ë¦„ ì„¤ì •
logging.langsmith("[Project] TOC-Based Dynamic Routing RAG - Gemini")

# Streamlit í˜ì´ì§€ ì„¤ì •
st.title("ğŸ“‘ ëª©ì°¨ ê¸°ë°˜ ë™ì  ë¼ìš°íŒ… ì…ì‹œ ì»¨ì„¤í„´íŠ¸")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []

if "toc_index" not in st.session_state:
    st.session_state["toc_index"] = None

if "pdf_path" not in st.session_state:
    st.session_state["pdf_path"] = None

if "section_cache" not in st.session_state:
    st.session_state["section_cache"] = {}

if "section_vectorstores" not in st.session_state:
    st.session_state["section_vectorstores"] = {}

# ì‚¬ì´ë“œë°”
with st.sidebar:
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")
    
    uploaded_file = st.file_uploader("ì›ë³¸ PDF ì—…ë¡œë“œ", type=["pdf"])
    
    selected_model = config.DEFAULT_LLM_MODEL
    
    max_retries = st.slider("ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜", min_value=1, max_value=5, value=config.DEFAULT_MAX_RETRIES)
    
    st.markdown("---")
    st.markdown("### ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    extract_only_mode = st.checkbox(
        "ì›ë³¸ ì •ë³´ë§Œ ì¶”ì¶œ (LLM ë‹µë³€ ìƒì„± ì—†ìŒ)",
        value=False,
        help="ì²´í¬í•˜ë©´ ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ì›ë³¸ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ í‘œì‹œí•©ë‹ˆë‹¤. LLM ê¸°ë°˜ ë‹µë³€ ìƒì„±ì€ ê±´ë„ˆëœë‹ˆë‹¤."
    )
    clean_extract_mode = st.checkbox(
        "ìˆœìˆ˜ ì •ë³´ë§Œ ì¶œë ¥ (ë©”íƒ€ë°ì´í„° ì œì™¸)",
        value=False,
        help="ì²´í¬í•˜ë©´ ë©”íƒ€ë°ì´í„°ë‚˜ ì¶”ê°€ ì •ë³´ ì—†ì´ ì¿¼ë¦¬ì— ì ì ˆí•œ ì •ë³´ë§Œ ê¹”ë”í•˜ê²Œ ì¶œë ¥í•©ë‹ˆë‹¤."
    )
    
    st.markdown("---")
    st.markdown("### ğŸ“‘ ëª©ì°¨ ê¸°ë°˜ ë™ì  ë¼ìš°íŒ…")
    st.markdown("1. ì›ë³¸ PDF ì—…ë¡œë“œ")
    st.markdown("2. ëª©ì°¨ ìë™ ê°ì§€ ë° íŒŒì‹±")
    st.markdown("3. ì¿¼ë¦¬ë³„ ê´€ë ¨ ì„¹ì…˜ ìë™ ì„ íƒ")
    st.markdown("4. í•´ë‹¹ ì„¹ì…˜ë§Œ ë™ì  ì²˜ë¦¬")
    st.markdown("5. **í‘œ ë³„ë„ ë¶„ì„ ë° ìš”ì•½**")
    st.markdown("6. **í’ˆì§ˆ í‰ê°€ ë° ì¬ì‹œë„**")
    st.markdown("")
    st.markdown("âœ… ì‚¬ì „ ë¶„í•  ë¶ˆí•„ìš”")
    st.markdown("âœ… ë™ì  ì„¹ì…˜ ì„ íƒ")
    st.markdown("âœ… í‘œ êµ¬ì¡° ì¸ì‹ ë° ìš”ì•½")
    st.markdown("âœ… íš¨ìœ¨ì  ì²˜ë¦¬")
    
    st.markdown("---")
    st.markdown("### âš™ï¸ ì‹œìŠ¤í…œ ìƒíƒœ")
    if st.session_state["toc_index"]:
        st.success(f"âœ… ëª©ì°¨ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
        st.info(f"ğŸ“„ ì„¹ì…˜ ìˆ˜: {len(st.session_state['toc_index'])}ê°œ")
        with st.expander("ğŸ“‹ ëª©ì°¨ ë³´ê¸°"):
            for section in st.session_state["toc_index"]:
                st.markdown(f"**{section['title']}** (í˜ì´ì§€ {section['start_page']}-{section['end_page']})")
    else:
        st.warning("â³ PDF ì—…ë¡œë“œ ëŒ€ê¸° ì¤‘")


def print_messages():
    """ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥"""
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)


def add_message(role, message):
    """ìƒˆë¡œìš´ ë©”ì‹œì§€ ì¶”ê°€"""
    st.session_state["messages"].append(ChatMessage(role=role, content=message))


@st.cache_resource(show_spinner="PDF ì „ì²˜ë¦¬ ì¤‘... (ëª©ì°¨ ë¶„ì„, í‘œ ì¸ì‹, ì„ë² ë”© ìƒì„±)")
def build_toc_index_and_preprocess(pdf_path, _cache_key, _file_mtime):
    """
    ëª©ì°¨ ì¸ë±ìŠ¤ ìƒì„± ë° ëª¨ë“  ì„¹ì…˜ ì „ì²˜ë¦¬
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        _cache_key: ìºì‹œ í‚¤ (íŒŒì¼ëª…)
        _file_mtime: íŒŒì¼ ìˆ˜ì • ì‹œê°„ (ìºì‹œ ë¬´íš¨í™”ìš©)
        
    Returns:
        {
            "sections": ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸,
            "section_data": ì„¹ì…˜ë³„ ì „ì²˜ë¦¬ ê²°ê³¼
        }
    """
    toc_processor = TOCProcessor(selected_model)
    preprocessor = SectionPreprocessor(selected_model)
    
    # 1ë‹¨ê³„: ëª©ì°¨ í˜ì´ì§€ ê°ì§€
    st.info("ğŸ” ëª©ì°¨ í˜ì´ì§€ ê°ì§€ ì¤‘...")
    toc_pages = toc_processor.detect_toc_pages(pdf_path)
    
    if not toc_pages:
        st.warning("âš ï¸ ëª©ì°¨ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜ì´ì§€ ìˆ˜ ê¸°ë°˜ ë¶„í• ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        sections = toc_processor.create_default_sections(pdf_path)
    else:
        st.success(f"âœ… ëª©ì°¨ í˜ì´ì§€ ë°œê²¬: {[p+1 for p in toc_pages]}")
        
        # 2ë‹¨ê³„: ëª©ì°¨ êµ¬ì¡° íŒŒì‹±
        st.info("ğŸ“‹ ëª©ì°¨ êµ¬ì¡° íŒŒì‹± ì¤‘...")
        sections = toc_processor.parse_toc_structure(pdf_path, toc_pages)
        
        if not sections:
            st.warning("âš ï¸ ëª©ì°¨ íŒŒì‹± ì‹¤íŒ¨. í˜ì´ì§€ ìˆ˜ ê¸°ë°˜ ë¶„í• ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            sections = toc_processor.create_default_sections(pdf_path)
    
    # 3ë‹¨ê³„: í˜ì´ì§€ ë²”ìœ„ ê²€ì¦
    sections = toc_processor.validate_and_fix_sections(sections, pdf_path)
    st.success(f"âœ… {len(sections)}ê°œ ì„¹ì…˜ ì¶”ì¶œ ì™„ë£Œ")
    
    # 4ë‹¨ê³„: ë³‘ë ¬ ì „ì²˜ë¦¬
    st.info(f"ğŸ“„ {len(sections)}ê°œ ì„¹ì…˜ ë³‘ë ¬ ì „ì²˜ë¦¬ ì¤‘... (í‘œ êµ¬ì¡° ì¸ì‹ ë° ì„ë² ë”© ìƒì„±)")
    
    section_data = {}
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    def process_section(section):
        """ì„¹ì…˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ (ë³‘ë ¬ ì‹¤í–‰ìš©)"""
        section_key = f"{section['start_page']}_{section['end_page']}"
        result = preprocessor.preprocess_section(section, pdf_path)
        return {
            "section_key": section_key,
            "result": result,
            "section": section
        }
    
    with ThreadPoolExecutor(max_workers=config.MAX_WORKERS) as executor:
        future_to_section = {
            executor.submit(process_section, section): idx
            for idx, section in enumerate(sections, 1)
        }
        
        completed = 0
        total = len(sections)
        
        for future in as_completed(future_to_section):
            idx = future_to_section[future]
            try:
                data = future.result()
                
                # dataê°€ Noneì´ê±°ë‚˜ í•„ìˆ˜ í‚¤ê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                if not data:
                    st.error(f"âŒ ì„¹ì…˜ {idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤")
                    continue
                    
                section_key = data.get("section_key", f"section_{idx}")
                result = data.get("result", {})
                section = data.get("section", {"title": f"ì„¹ì…˜ {idx}"})
                
                # resultê°€ Noneì´ê±°ë‚˜ í•„ìˆ˜ í‚¤ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                if not result:
                    result = {
                        "vectorstore": None,
                        "documents": [],
                        "table_count": 0
                    }
                
                section_data[section_key] = {
                    "vectorstore": result.get("vectorstore"),
                    "documents": result.get("documents", []),
                    "section": section,
                    "table_count": result.get("table_count", 0)
                }
                
                completed += 1
                progress = completed / total
                progress_bar.progress(progress)
                
                table_count = result.get("table_count", 0)
                table_info = f" (í‘œ {table_count}ê°œ)" if table_count > 0 else ""
                section_title = section.get("title", f"ì„¹ì…˜ {idx}")
                status_text.text(f"âœ… {completed}/{total} ì™„ë£Œ: '{section_title}'{table_info}")
                st.success(f"âœ… ì„¹ì…˜ {idx} ì™„ë£Œ: '{section_title}'{table_info}")
                
            except Exception as e:
                import traceback
                error_details = traceback.format_exc()
                print(f"âŒ ì„¹ì…˜ {idx} ì²˜ë¦¬ ì¤‘ ìƒì„¸ ì˜¤ë¥˜:\n{error_details}")
                st.error(f"âŒ ì„¹ì…˜ {idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                # ì˜¤ë¥˜ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰
                completed += 1
                progress = completed / total
                progress_bar.progress(progress)
    
    progress_bar.empty()
    status_text.empty()
    st.success(f"ğŸ‰ ëª¨ë“  ì„¹ì…˜ ì „ì²˜ë¦¬ ì™„ë£Œ! ({len(sections)}ê°œ)")
    
    return {
        "sections": sections,
        "section_data": section_data
    }


def extract_clean_information(retrieved_docs, question):
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ìˆœìˆ˜ ì •ë³´ë§Œ ì¶”ì¶œ (ë©”íƒ€ë°ì´í„° ì œì™¸, Elbow Method ê¸°ë°˜ ë™ì  ì»·ì˜¤í”„ ì ìš©)
    
    Args:
        retrieved_docs: Document ë¦¬ìŠ¤íŠ¸ (similarity_scoreê°€ ë©”íƒ€ë°ì´í„°ì— í¬í•¨ë¨)
        question: ì‚¬ìš©ì ì§ˆë¬¸
        
    Returns:
        í¬ë§·íŒ…ëœ ìˆœìˆ˜ ì •ë³´ ë¬¸ìì—´ (ë©”íƒ€ë°ì´í„° ì—†ìŒ)
    """
    if not retrieved_docs:
        return "ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
    
    # Elbow Method ê¸°ë°˜ ë™ì  ì»·ì˜¤í”„ ì ìš©
    from core.rag_system import RAGSystem
    rag_system = RAGSystem()
    filtered_docs = rag_system._apply_dynamic_cutoff(retrieved_docs)
    
    if not filtered_docs:
        return "ë™ì  ì»·ì˜¤í”„ ì ìš© í›„ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # ìˆœìˆ˜ ì •ë³´ë§Œ ì¶”ì¶œ (ë©”íƒ€ë°ì´í„° ì—†ì´)
    output_parts = []
    
    for doc in filtered_docs:
        # ì›ë³¸ ë‚´ìš©ë§Œ ì¶”ì¶œ
        if doc.metadata.get('is_table', False) and doc.metadata.get('raw_data'):
            # í‘œì˜ ê²½ìš° raw_data ì‚¬ìš©
            output_parts.append(doc.metadata['raw_data'])
        else:
            # í…ìŠ¤íŠ¸ì˜ ê²½ìš° page_content ì‚¬ìš©
            output_parts.append(doc.page_content)
        
        # ë¬¸ì„œ ê°„ êµ¬ë¶„
        output_parts.append("\n\n---\n\n")
    
    return "".join(output_parts).strip()


def extract_raw_information(retrieved_docs, question):
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ì›ë³¸ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ í¬ë§·íŒ… (Elbow Method ê¸°ë°˜ ë™ì  ì»·ì˜¤í”„ ì ìš©)
    
    Args:
        retrieved_docs: Document ë¦¬ìŠ¤íŠ¸ (similarity_scoreê°€ ë©”íƒ€ë°ì´í„°ì— í¬í•¨ë¨)
        question: ì‚¬ìš©ì ì§ˆë¬¸
        
    Returns:
        í¬ë§·íŒ…ëœ ì›ë³¸ ì •ë³´ ë¬¸ìì—´
    """
    if not retrieved_docs:
        return "ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
    
    # ë™ì  ì»·ì˜¤í”„ ì ìš© ì „ ë¬¸ì„œ ìˆ˜
    original_count = len(retrieved_docs)
    
    # Elbow Method ê¸°ë°˜ ë™ì  ì»·ì˜¤í”„ ì ìš© (rag_systemì˜ ë©”ì„œë“œ ì‚¬ìš©)
    from core.rag_system import RAGSystem
    rag_system = RAGSystem()
    filtered_docs = rag_system._apply_dynamic_cutoff(retrieved_docs)
    
    if not filtered_docs:
        return "ë™ì  ì»·ì˜¤í”„ ì ìš© í›„ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # ì»·ì˜¤í”„ ì ìš© í›„ ë¬¸ì„œ ìˆ˜
    filtered_count = len(filtered_docs)
    
    # í¬ë§·íŒ…ëœ ì •ë³´ ìƒì„±
    output_parts = []
    output_parts.append(f"## ğŸ“‹ ì§ˆë¬¸: {question}\n")
    output_parts.append(f"**ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜:** {original_count}ê°œ â†’ **ì»·ì˜¤í”„ ì ìš© í›„:** {filtered_count}ê°œ\n")
    if original_count > filtered_count:
        removed_count = original_count - filtered_count
        output_parts.append(f"*âœ‚ï¸ Elbow Method ê¸°ë°˜ ë™ì  ì»·ì˜¤í”„ë¡œ ê´€ë ¨ì„± ë‚®ì€ ë¬¸ì„œ {removed_count}ê°œ ì œê±°ë¨*\n")
    output_parts.append("---\n")
    
    # ì„¹ì…˜ë³„ë¡œ ê·¸ë£¹í™” (ì»·ì˜¤í”„ ì ìš©ëœ ë¬¸ì„œë§Œ)
    docs_by_section = {}
    for doc in filtered_docs:
        section_title = doc.metadata.get('section_title', 'ì•Œ ìˆ˜ ì—†ìŒ')
        if section_title not in docs_by_section:
            docs_by_section[section_title] = []
        docs_by_section[section_title].append(doc)
    
    doc_counter = 1
    for section_title, docs in sorted(docs_by_section.items()):
        output_parts.append(f"### ğŸ“‘ ì„¹ì…˜: {section_title}\n")
        output_parts.append(f"**ë¬¸ì„œ ìˆ˜:** {len(docs)}ê°œ\n\n")
        
        for doc in docs:
            # ë©”íƒ€ë°ì´í„° ì •ë³´ ìˆ˜ì§‘
            metadata_info = []
            is_table = doc.metadata.get('is_table', False)
            chunk_type = doc.metadata.get('chunk_type', 'token')
            
            # í˜ì´ì§€ ì •ë³´
            if chunk_type == 'page':
                page_number = doc.metadata.get('page_number', 0)
                page_range = doc.metadata.get('page_range', None)
                if page_range:
                    metadata_info.append(f"ğŸ“„ í˜ì´ì§€ {page_range}")
                elif page_number > 0:
                    metadata_info.append(f"ğŸ“„ í˜ì´ì§€ {page_number}")
            else:
                if 'section_start' in doc.metadata and 'section_end' in doc.metadata:
                    metadata_info.append(f"ğŸ“„ í˜ì´ì§€ {doc.metadata['section_start']}-{doc.metadata['section_end']}")
            
            # í‘œ ì—¬ë¶€
            if is_table:
                metadata_info.append("ğŸ“Š í‘œ ë°ì´í„°")
            
            # ë³‘í•© ì •ë³´
            if doc.metadata.get('merged_chunks', 0) > 1:
                merged_count = doc.metadata['merged_chunks']
                if chunk_type == 'page':
                    metadata_info.append(f"ğŸ”— {merged_count}ê°œ í˜ì´ì§€ ë³‘í•©")
                else:
                    metadata_info.append(f"ğŸ”— {merged_count}ê°œ ì²­í¬ ë³‘í•©")
            
            # ìœ ì‚¬ë„ ì ìˆ˜
            similarity_score = doc.metadata.get('similarity_score', 0)
            if similarity_score > 0:
                metadata_info.append(f"â­ ê´€ë ¨ì„±: {similarity_score:.3f}")
            
            output_parts.append(f"#### ë¬¸ì„œ {doc_counter}\n")
            if metadata_info:
                output_parts.append(f"**ì •ë³´:** {' | '.join(metadata_info)}\n\n")
            
            # ì›ë³¸ ë‚´ìš© ì¶”ì¶œ
            if is_table and doc.metadata.get('raw_data'):
                # í‘œì˜ ê²½ìš° raw_data ì‚¬ìš©
                output_parts.append("**ì›ë³¸ í‘œ ë°ì´í„°:**\n")
                output_parts.append("```markdown\n")
                output_parts.append(doc.metadata['raw_data'])
                output_parts.append("\n```\n")
                
                # ìš”ì•½ë„ í•¨ê»˜ í‘œì‹œ
                if doc.page_content and doc.page_content.strip():
                    output_parts.append("\n**í‘œ ìš”ì•½:**\n")
                    output_parts.append(f"{doc.page_content}\n")
            else:
                # í…ìŠ¤íŠ¸ì˜ ê²½ìš° page_content ì‚¬ìš©
                output_parts.append("**ì›ë³¸ ë‚´ìš©:**\n")
                output_parts.append("```\n")
                output_parts.append(doc.page_content)
                output_parts.append("\n```\n")
            
            output_parts.append("\n---\n\n")
            doc_counter += 1
    
    return "".join(output_parts)


def query_with_retry(question, pdf_path, toc_index, max_retries=3, extract_only=False, clean_extract=False):
    """
    ì§ˆì˜ì‘ë‹µ ìˆ˜í–‰ (ì¬ì‹œë„ í¬í•¨)
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        toc_index: ëª©ì°¨ ì¸ë±ìŠ¤
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        extract_only: Trueë©´ ì›ë³¸ ì •ë³´ë§Œ ì¶”ì¶œ, Falseë©´ LLM ë‹µë³€ ìƒì„±
        clean_extract: Trueë©´ ë©”íƒ€ë°ì´í„° ì—†ì´ ìˆœìˆ˜ ì •ë³´ë§Œ ì¶œë ¥
        
    Returns:
        {
            "answer": ë‹µë³€ ë˜ëŠ” ì›ë³¸ ì •ë³´,
            "evidence": ê·¼ê±° ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸,
            "selected_sections": ì„ íƒëœ ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸
        }
    """
    rag_system = RAGSystem(selected_model)
    searcher = SearchEngine()
    
    for attempt in range(1, max_retries + 1):
        if attempt > 1:
            st.warning(f"ğŸ”„ {attempt}ë²ˆì§¸ ì‹œë„ ì¤‘... (ì´ì „ ë‹µë³€ í’ˆì§ˆ ê°œì„  í•„ìš”)")
        
        # 1ë‹¨ê³„: ê´€ë ¨ ì„¹ì…˜ ì°¾ê¸°
        st.info(f"ğŸ¤” ì§ˆë¬¸ ë¶„ì„ ë° ê´€ë ¨ ì„¹ì…˜ íƒìƒ‰ ì¤‘... (ì‹œë„ {attempt}/{max_retries})")
        section_indices, thinking_process = rag_system.find_relevant_sections(question, toc_index)
        
        selected_sections = [toc_index[idx] for idx in section_indices]
        
        with st.expander(f"ğŸ§  ì§ˆë¬¸ ë¶„ì„ ë° ì„¹ì…˜ ì„ íƒ (ì‹œë„ {attempt})"):
            st.markdown("**ì§ˆë¬¸ ë¶„ì„:**")
            if thinking_process:
                st.markdown(thinking_process)
            else:
                st.markdown("ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê´€ë ¨ ì„¹ì…˜ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤.")
            
            st.markdown("---")
            st.markdown("**ì„ íƒëœ ì„¹ì…˜:**")
            for idx, section in enumerate(selected_sections, 1):
                st.markdown(f"{idx}. **{section['title']}** (í˜ì´ì§€ {section['start_page']}-{section['end_page']})")
        
        # 2ë‹¨ê³„: ì„¹ì…˜ë³„ ë²¡í„° ê²€ìƒ‰ (í‘œì™€ í…ìŠ¤íŠ¸ ë¶„ë¦¬ ê²€ìƒ‰) - ë³‘ë ¬ ì²˜ë¦¬
        st.info(f"ğŸ“„ ì„ íƒëœ {len(selected_sections)}ê°œ ì„¹ì…˜ì—ì„œ í‘œì™€ í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¦¬í•˜ì—¬ ë³‘ë ¬ ê²€ìƒ‰ ì¤‘...")
        
        all_retrieved_docs_with_scores = []
        section_results = {}  # ì„¹ì…˜ë³„ ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
        
        # ì„¸ì…˜ ìƒíƒœì—ì„œ ë°ì´í„° ì¶”ì¶œ (ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜ì—ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡)
        section_vectorstores_data = st.session_state.get("section_vectorstores", {})
        
        # ë³‘ë ¬ ê²€ìƒ‰ í•¨ìˆ˜
        def search_section(section, section_vectorstores_data, question, searcher):
            """ë‹¨ì¼ ì„¹ì…˜ ê²€ìƒ‰ í•¨ìˆ˜ (ë³‘ë ¬ ì‹¤í–‰ìš©)
            
            Args:
                section: ì„¹ì…˜ ì •ë³´ ë”•ì…”ë„ˆë¦¬
                section_vectorstores_data: ì„¹ì…˜ë³„ ë²¡í„°ìŠ¤í† ì–´ ë°ì´í„° (ì„¸ì…˜ ìƒíƒœì—ì„œ ì¶”ì¶œ)
                question: ì‚¬ìš©ì ì§ˆë¬¸
                searcher: SearchEngine ì¸ìŠ¤í„´ìŠ¤
            """
            section_key = f"{section['start_page']}_{section['end_page']}"
            section_table_docs = []
            section_text_docs = []
            
            if section_key in section_vectorstores_data:
                section_data = section_vectorstores_data[section_key]
                section_documents = section_data["documents"]
                
                # í‘œì™€ í…ìŠ¤íŠ¸ ë¬¸ì„œ ë¶„ë¦¬
                table_documents = [doc for doc in section_documents if doc.metadata.get('is_table', False)]
                text_documents = [doc for doc in section_documents if not doc.metadata.get('is_table', False)]
                
                # í‘œ ê²€ìƒ‰
                if table_documents:
                    try:
                        from langchain_community.vectorstores import FAISS
                        from langchain_google_genai import GoogleGenerativeAIEmbeddings
                        
                        embeddings = GoogleGenerativeAIEmbeddings(model=config.DEFAULT_EMBEDDING_MODEL)
                        table_vectorstore = FAISS.from_documents(documents=table_documents, embedding=embeddings)
                        
                        section_table_docs = searcher.hybrid_search(
                            question=question,
                            vectorstore=table_vectorstore,
                            documents=table_documents,
                            top_k=min(config.TOP_K_PER_SECTION, len(table_documents))
                        )
                    except Exception as e:
                        print(f"âš ï¸ ì„¹ì…˜ '{section['title']}' í‘œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                
                # í…ìŠ¤íŠ¸ ê²€ìƒ‰
                if text_documents:
                    try:
                        from langchain_community.vectorstores import FAISS
                        from langchain_google_genai import GoogleGenerativeAIEmbeddings
                        
                        embeddings = GoogleGenerativeAIEmbeddings(model=config.DEFAULT_EMBEDDING_MODEL)
                        text_vectorstore = FAISS.from_documents(documents=text_documents, embedding=embeddings)
                        
                        section_text_docs = searcher.hybrid_search(
                            question=question,
                            vectorstore=text_vectorstore,
                            documents=text_documents,
                            top_k=config.TOP_K_PER_SECTION
                        )
                    except Exception as e:
                        print(f"âš ï¸ ì„¹ì…˜ '{section['title']}' í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
            return {
                "section": section,
                "section_key": section_key,
                "table_docs": section_table_docs,
                "text_docs": section_text_docs
            }
        
        # ThreadPoolExecutorë¡œ ë³‘ë ¬ ê²€ìƒ‰
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        with ThreadPoolExecutor(max_workers=config.MAX_WORKERS) as executor:
            future_to_section = {
                executor.submit(search_section, section, section_vectorstores_data, question, searcher): section
                for section in selected_sections
            }
            
            for future in as_completed(future_to_section):
                section = future_to_section[future]
                try:
                    result = future.result()
                    section_table_docs = result["table_docs"]
                    section_text_docs = result["text_docs"]
                    section_docs_with_scores = section_table_docs + section_text_docs
                    all_retrieved_docs_with_scores.extend(section_docs_with_scores)
                    
                    # ì„¹ì…˜ë³„ ê²°ê³¼ ì €ì¥
                    section_results[section['title']] = {
                        "docs": section_docs_with_scores,
                        "table_docs": section_table_docs,
                        "text_docs": section_text_docs,
                        "count": len(section_docs_with_scores),
                        "table_count": len(section_table_docs),
                        "text_count": len(section_text_docs),
                        "section_info": section
                    }
                    
                    st.success(
                        f"âœ… ì„¹ì…˜ '{section['title']}' ê²€ìƒ‰ ì™„ë£Œ: "
                        f"í‘œ {len(section_table_docs)}ê°œ, í…ìŠ¤íŠ¸ {len(section_text_docs)}ê°œ "
                        f"(ì´ {len(section_docs_with_scores)}ê°œ)"
                    )
                except Exception as e:
                    st.warning(f"âš ï¸ ì„¹ì…˜ '{section['title']}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    section_results[section['title']] = {
                        "docs": [],
                        "table_docs": [],
                        "text_docs": [],
                        "count": 0,
                        "table_count": 0,
                        "text_count": 0,
                        "section_info": section,
                        "error": str(e)
                    }
        
        # ì„¹ì…˜ë³„ ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ í‘œì‹œ
        with st.expander(f"ğŸ“Š ì„¹ì…˜ë³„ ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ({len(selected_sections)}ê°œ ì„¹ì…˜)"):
            for section_title, result in section_results.items():
                st.markdown(f"### ğŸ“‘ {section_title}")
                st.markdown(f"**í˜ì´ì§€ ë²”ìœ„:** {result['section_info']['start_page']}-{result['section_info']['end_page']}í˜ì´ì§€")
                st.markdown(f"**ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜:** ì´ {result['count']}ê°œ (í‘œ {result.get('table_count', 0)}ê°œ, í…ìŠ¤íŠ¸ {result.get('text_count', 0)}ê°œ)")
                
                if result['count'] > 0:
                    # í‘œ ê²€ìƒ‰ ê²°ê³¼
                    if result.get('table_count', 0) > 0:
                        st.markdown("**ğŸ“Š í‘œ ê²€ìƒ‰ ê²°ê³¼:**")
                        for idx, (doc, score) in enumerate(result['table_docs'][:3], 1):  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                            with st.container():
                                st.markdown(f"**í‘œ {idx}** (ê´€ë ¨ì„± ì ìˆ˜: {score:.3f})")
                                content_preview = doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content
                                st.markdown(f"```\n{content_preview}\n```")
                                if idx < len(result['table_docs'][:3]):
                                    st.markdown("---")
                    
                    # í…ìŠ¤íŠ¸ ê²€ìƒ‰ ê²°ê³¼
                    if result.get('text_count', 0) > 0:
                        st.markdown("**ğŸ“„ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ê²°ê³¼:**")
                        for idx, (doc, score) in enumerate(result['text_docs'][:5], 1):  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                            with st.container():
                                st.markdown(f"**ë¬¸ì„œ {idx}** (ê´€ë ¨ì„± ì ìˆ˜: {score:.3f})")
                                
                                # ë©”íƒ€ë°ì´í„° ì •ë³´
                                metadata_info = []
                                chunk_type = doc.metadata.get('chunk_type', 'token')
                                
                                # í˜ì´ì§€ ë‹¨ìœ„ ì²­í‚¹ì¸ ê²½ìš° í˜ì´ì§€ ë²ˆí˜¸ í‘œì‹œ
                                if chunk_type == 'page':
                                    page_number = doc.metadata.get('page_number', 0)
                                    page_range = doc.metadata.get('page_range', None)
                                    if page_range:
                                        metadata_info.append(f"ğŸ“„ í˜ì´ì§€ {page_range}")
                                    elif page_number > 0:
                                        metadata_info.append(f"ğŸ“„ í˜ì´ì§€ {page_number}")
                                
                                # ë³‘í•©ëœ ì²­í¬ ì •ë³´ í‘œì‹œ
                                if doc.metadata.get('merged_chunks'):
                                    if chunk_type == 'page':
                                        metadata_info.append(f"ğŸ”— {doc.metadata['merged_chunks']}ê°œ í˜ì´ì§€ ë³‘í•©")
                                    else:
                                        metadata_info.append(f"ğŸ”— {doc.metadata['merged_chunks']}ê°œ ì²­í¬ ë³‘í•©")
                                
                                if metadata_info:
                                    st.markdown(" | ".join(metadata_info))
                                
                                # ë¬¸ì„œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                                content_preview = doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content
                                st.markdown(f"```\n{content_preview}\n```")
                                
                                if idx < len(result['text_docs'][:5]):
                                    st.markdown("---")
                else:
                    if 'error' in result:
                        st.warning(f"âš ï¸ {result['error']}")
                    else:
                        st.info("ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                if section_title != list(section_results.keys())[-1]:
                    st.markdown("---")
        
        if not all_retrieved_docs_with_scores:
            st.error("ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "answer": "ì˜¤ë¥˜: ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "evidence": [],
                "selected_sections": selected_sections
            }
        
        # 3ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ í†µí•© ë° ë³‘í•©
        # ì²­í‚¹ ë°©ì‹ì— ë”°ë¼ ë©”ì‹œì§€ ë³€ê²½
        chunking_type = "í˜ì´ì§€ ë‹¨ìœ„" if config.CHUNK_BY_PAGE else "í† í° ê¸°ë°˜"
        merge_method = "í˜ì´ì§€ ë²ˆí˜¸ ê¸°ë°˜ ë³‘í•©" if config.CHUNK_BY_PAGE else "overlap ì •ë³´ ê¸°ë°˜ ë³‘í•©"
        
        st.info(f"ğŸ”— {len(all_retrieved_docs_with_scores)}ê°œ ê²€ìƒ‰ ê²°ê³¼ í†µí•© ì¤‘... ({merge_method} + ì ìˆ˜ ì •ë ¬)")
        
        retrieved_docs = rag_system.merge_and_sort_docs(all_retrieved_docs_with_scores)
        st.success(f"âœ… ì´ {len(retrieved_docs)}ê°œ ë¬¸ì„œ ì„ íƒ ì™„ë£Œ ({merge_method} + ì ìˆ˜ ê¸°ë°˜ ì •ë ¬)")
        
        # í†µí•©ëœ ë¬¸ì„œì˜ ì„¹ì…˜ë³„ ë¶„í¬ í‘œì‹œ
        with st.expander("ğŸ“ˆ ìµœì¢… í†µí•© ë¬¸ì„œì˜ ì„¹ì…˜ë³„ ë¶„í¬"):
            section_doc_counts = {}
            for doc in retrieved_docs:
                section_title = doc.metadata.get('section_title', 'ì•Œ ìˆ˜ ì—†ìŒ')
                if section_title not in section_doc_counts:
                    section_doc_counts[section_title] = 0
                section_doc_counts[section_title] += 1
            
            for section_title, count in sorted(section_doc_counts.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / len(retrieved_docs)) * 100 if retrieved_docs else 0
                st.markdown(f"- **{section_title}**: {count}ê°œ ë¬¸ì„œ ({percentage:.1f}%)")
        
        # 4ë‹¨ê³„: ë‹µë³€ ìƒì„± ë˜ëŠ” ì›ë³¸ ì •ë³´ ì¶”ì¶œ
        if extract_only:
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì›ë³¸ ì •ë³´ë§Œ ì¶”ì¶œ
            if clean_extract:
                # ìˆœìˆ˜ ì •ë³´ë§Œ ì¶œë ¥ (ë©”íƒ€ë°ì´í„° ì œì™¸)
                st.info("ğŸ“‹ ìˆœìˆ˜ ì •ë³´ ì¶”ì¶œ ì¤‘... (ë©”íƒ€ë°ì´í„° ì œì™¸ ëª¨ë“œ)")
                answer = extract_clean_information(retrieved_docs, question)
            else:
                # ì›ë³¸ ì •ë³´ ì¶”ì¶œ (ë©”íƒ€ë°ì´í„° í¬í•¨)
                st.info("ğŸ“‹ ì›ë³¸ ì •ë³´ ì¶”ì¶œ ì¤‘... (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)")
                answer = extract_raw_information(retrieved_docs, question)
            
            # í’ˆì§ˆ í‰ê°€ ê±´ë„ˆë›°ê¸°
            quality_result = {"is_acceptable": True, "evaluation_text": "í…ŒìŠ¤íŠ¸ ëª¨ë“œ: í’ˆì§ˆ í‰ê°€ ê±´ë„ˆëœ€"}
        else:
            # ì¼ë°˜ ëª¨ë“œ: LLM ë‹µë³€ ìƒì„±
            st.info("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘... (ì§ì ‘ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© + ëŒ€í™” ë§¥ë½ ë°˜ì˜)")
            # ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¤€ë¹„
            conversation_history = []
            if st.session_state["messages"]:
                for msg in st.session_state["messages"]:
                    role = msg.role if hasattr(msg, 'role') else msg.get('role', '')
                    content = msg.content if hasattr(msg, 'content') else msg.get('content', '')
                    if role in ["user", "assistant"]:
                        conversation_history.append((role, content))
            
            # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ë‹µë³€ ìƒì„±
            answer_chunks = []
            answer_placeholder = st.empty()
            full_answer = ""
            
            # retrieved_docs_with_scores í˜•íƒœë¡œ ë³€í™˜ (generate_answerê°€ ìš”êµ¬í•˜ëŠ” í˜•íƒœ)
            retrieved_docs_with_scores = [(doc, doc.metadata.get('similarity_score', 0)) for doc in retrieved_docs]
            
            for chunk in rag_system.generate_answer(question, retrieved_docs_with_scores, conversation_history, stream=True):
                answer_chunks.append(chunk)
                full_answer += chunk
                # ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹µë³€ í‘œì‹œ
                answer_placeholder.markdown(full_answer)
            
            answer = full_answer
            
            # 5ë‹¨ê³„: í’ˆì§ˆ í‰ê°€
            st.info("ğŸ“Š ë‹µë³€ í’ˆì§ˆ í‰ê°€ ì¤‘...")
            quality_result = rag_system.quality_evaluator.evaluate(question, answer)
            
            with st.expander("ğŸ“‹ í’ˆì§ˆ í‰ê°€ ê²°ê³¼"):
                st.markdown(quality_result["evaluation_text"])
        
        # ê·¼ê±° ë¬¸ì„œ ì •ë³´ ìˆ˜ì§‘
        evidence_docs = []
        evidence_by_section = {}  # ì„¹ì…˜ë³„ ê·¼ê±° ë¬¸ì„œ ê·¸ë£¹í™”
        
        # retrieved_docsëŠ” ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        for doc in retrieved_docs[:10]:
            page_info = ""
            section_info = ""
            
            if hasattr(doc, 'metadata') and doc.metadata:
                is_table = doc.metadata.get('is_table', False)
                table_label = " [í‘œ ìš”ì•½]" if is_table else ""
                chunk_type = doc.metadata.get('chunk_type', 'token')
                
                if 'section_title' in doc.metadata:
                    section_info = doc.metadata['section_title']
                    
                    # í˜ì´ì§€ ë‹¨ìœ„ ì²­í‚¹ì¸ ê²½ìš° ì •í™•í•œ í˜ì´ì§€ ë²ˆí˜¸ í‘œì‹œ
                    if chunk_type == 'page':
                        page_number = doc.metadata.get('page_number', 0)
                        page_range = doc.metadata.get('page_range', None)
                        
                        if page_range:
                            # ë³‘í•©ëœ í˜ì´ì§€ ë²”ìœ„
                            page_info = f"ì„¹ì…˜: {section_info} (í˜ì´ì§€ {page_range}){table_label}"
                        elif page_number > 0:
                            # ë‹¨ì¼ í˜ì´ì§€
                            page_info = f"ì„¹ì…˜: {section_info} (í˜ì´ì§€ {page_number}){table_label}"
                        else:
                            # í˜ì´ì§€ ë²ˆí˜¸ê°€ ì—†ëŠ” ê²½ìš° ì„¹ì…˜ ë²”ìœ„ í‘œì‹œ
                            if 'section_start' in doc.metadata and 'section_end' in doc.metadata:
                                page_info = f"ì„¹ì…˜: {section_info} (í˜ì´ì§€ {doc.metadata['section_start']}-{doc.metadata['section_end']}){table_label}"
                            else:
                                page_info = f"ì„¹ì…˜: {section_info}{table_label}"
                    else:
                        # í† í° ê¸°ë°˜ ì²­í‚¹: ì„¹ì…˜ ë²”ìœ„ í‘œì‹œ
                        if 'section_start' in doc.metadata and 'section_end' in doc.metadata:
                            page_info = f"ì„¹ì…˜: {section_info} (í˜ì´ì§€ {doc.metadata['section_start']}-{doc.metadata['section_end']}){table_label}"
                        else:
                            page_info = f"ì„¹ì…˜: {section_info}{table_label}"
            
            # í‘œì˜ ê²½ìš° raw_data ìš°ì„  ì‚¬ìš©
            doc_content = doc.page_content
            if doc.metadata.get('is_table', False) and doc.metadata.get('raw_data'):
                doc_content = doc.metadata['raw_data']
            
            doc_info = {
                "content": doc_content[:500] + "..." if len(doc_content) > 500 else doc_content,
                "page_info": page_info,
                "section_info": section_info,
                "full_content": doc_content,
                "is_table": doc.metadata.get('is_table', False) if hasattr(doc, 'metadata') and doc.metadata else False
            }
            evidence_docs.append(doc_info)
            
            # ì„¹ì…˜ë³„ë¡œ ê·¸ë£¹í™”
            if section_info:
                if section_info not in evidence_by_section:
                    evidence_by_section[section_info] = []
                evidence_by_section[section_info].append(doc_info)
        
        # ì„¹ì…˜ë³„ ê·¼ê±° ë¬¸ì„œ í‘œì‹œ
        if evidence_by_section:
            with st.expander("ğŸ“š ë‹µë³€ì— ì‚¬ìš©ëœ ì„¹ì…˜ë³„ ê·¼ê±° ë¬¸ì„œ"):
                for section_title, docs in evidence_by_section.items():
                    st.markdown(f"### ğŸ“‘ {section_title}")
                    st.markdown(f"**ì‚¬ìš©ëœ ë¬¸ì„œ ìˆ˜:** {len(docs)}ê°œ")
                    
                    for idx, doc_info in enumerate(docs, 1):
                        st.markdown(f"**ê·¼ê±° {idx}**")
                        if doc_info['is_table']:
                            st.markdown("ğŸ“Š **í‘œ ë°ì´í„°**")
                        st.markdown(f"*{doc_info['page_info']}*")
                        st.markdown(f"```\n{doc_info['content']}\n```")
                        if idx < len(docs):
                            st.markdown("---")
                    
                    if section_title != list(evidence_by_section.keys())[-1]:
                        st.markdown("---")
        
        # í’ˆì§ˆì´ ì ì ˆí•˜ë©´ ë°˜í™˜
        if quality_result["is_acceptable"]:
            if attempt > 1:
                st.success(f"âœ… {attempt}ë²ˆì§¸ ì‹œë„ì—ì„œ ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤!")
            return {
                "answer": answer,
                "evidence": evidence_docs,
                "selected_sections": selected_sections
            }
        else:
            if attempt < max_retries:
                st.warning(f"âš ï¸ ë‹µë³€ í’ˆì§ˆì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ì¬ì‹œë„í•©ë‹ˆë‹¤... ({attempt}/{max_retries})")
            else:
                st.warning(f"âš ï¸ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜({max_retries})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
                return {
                    "answer": answer,
                    "evidence": evidence_docs,
                    "selected_sections": selected_sections
                }
    
    return {
        "answer": "ì˜¤ë¥˜: ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
        "evidence": [],
        "selected_sections": []
    }


# PDF ì—…ë¡œë“œ ì²˜ë¦¬
if uploaded_file:
    current_pdf = st.session_state.get("pdf_path")
    new_pdf_path = f"{config.FILES_DIR}/{uploaded_file.name}"
    
    # ìƒˆ PDFì¸ì§€ ë˜ëŠ” ê¸°ì¡´ PDFê°€ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸
    is_new_pdf = (
        current_pdf is None or 
        current_pdf != new_pdf_path or
        not os.path.exists(current_pdf)
    )
    
    if is_new_pdf:
        # íŒŒì¼ ì €ì¥
        file_content = uploaded_file.read()
        pdf_path = new_pdf_path
        with open(pdf_path, "wb") as f:
            f.write(file_content)
        
        st.session_state["pdf_path"] = pdf_path
        
        # ê¸°ì¡´ ìºì‹œ ì´ˆê¸°í™” (ìƒˆ PDF ì—…ë¡œë“œ ì‹œ)
        if "section_cache" in st.session_state:
            st.session_state["section_cache"] = {}
        if "section_vectorstores" in st.session_state:
            st.session_state["section_vectorstores"] = {}
        if "toc_index" in st.session_state:
            st.session_state["toc_index"] = None
        
        st.session_state["section_cache"] = {}
        st.session_state["section_vectorstores"] = {}
        
        # ëª©ì°¨ ì¸ë±ìŠ¤ ìƒì„± ë° ëª¨ë“  ì„¹ì…˜ ì „ì²˜ë¦¬
        # íŒŒì¼ ìˆ˜ì • ì‹œê°„ ê°€ì ¸ì˜¤ê¸° (ìºì‹œ ë¬´íš¨í™”ìš©)
        file_mtime = os.path.getmtime(pdf_path) if os.path.exists(pdf_path) else 0
        result = build_toc_index_and_preprocess(pdf_path, uploaded_file.name, file_mtime)
        
        st.session_state["toc_index"] = result["sections"]
        
        for section_key, data in result["section_data"].items():
            st.session_state["section_vectorstores"][section_key] = data
        
        st.success("âœ… PDF ì—…ë¡œë“œ ë° ì „ì²˜ë¦¬ ì™„ë£Œ! (ëª©ì°¨ ë¶„ì„, í‘œ ì¸ì‹, ì„ë² ë”© ìƒì„±)")
        st.rerun()

# ì´ˆê¸°í™” ë²„íŠ¼
if clear_btn:
    st.session_state["messages"] = []

# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
print_messages()

# ì‚¬ìš©ì ì…ë ¥
user_input = st.chat_input("ì…ì‹œ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”!")

warning_msg = st.empty()

# ì§ˆì˜ì‘ë‹µ ì²˜ë¦¬
if user_input:
    toc_index = st.session_state.get("toc_index")
    pdf_path = st.session_state.get("pdf_path")
    
    if toc_index and pdf_path:
        st.chat_message("user").write(user_input)
        
        with st.chat_message("assistant"):
            try:
                result = query_with_retry(
                    user_input,
                    pdf_path,
                    toc_index,
                    max_retries,
                    extract_only=extract_only_mode,
                    clean_extract=clean_extract_mode
                )
                
                answer = result["answer"]
                evidence = result.get("evidence", [])
                selected_sections = result.get("selected_sections", [])
                
                if extract_only_mode:
                    if clean_extract_mode:
                        st.markdown("### ğŸ“‹ ì¶”ì¶œëœ ìˆœìˆ˜ ì •ë³´ (ë©”íƒ€ë°ì´í„° ì œì™¸)")
                        st.markdown(answer)
                    else:
                        st.markdown("### ğŸ“‹ ì¶”ì¶œëœ ì›ë³¸ ì •ë³´ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)")
                        st.markdown(answer)
                else:
                    st.markdown("### ğŸ“ ë‹µë³€")
                    st.markdown(answer)
                
                # ê·¼ê±° ë¬¸ì„œ í‘œì‹œ
                if evidence:
                    st.markdown("---")
                    st.markdown("### ğŸ“š ë‹µë³€ ê·¼ê±° (ê²€ìƒ‰ëœ ë¬¸ì„œ)")
                    st.caption(f"ì´ {len(evidence)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œ ì²­í¬ë¥¼ ì°¸ê³ í–ˆìŠµë‹ˆë‹¤.")
                    
                    for idx, doc in enumerate(evidence, 1):
                        expander_title = f"ğŸ“„ ê·¼ê±° {idx}"
                        if doc['is_table']:
                            expander_title = f"ğŸ“Š í‘œ ìš”ì•½ {idx}"
                        if doc['page_info']:
                            expander_title += f" - {doc['page_info']}"
                        
                        with st.expander(expander_title):
                            if doc['is_table']:
                                st.markdown("**í‘œ ìš”ì•½ ë‚´ìš©:**")
                                st.info("ì´ ë‚´ìš©ì€ PDFì˜ í‘œë¥¼ AIê°€ ë¶„ì„í•˜ì—¬ ìš”ì•½í•œ ê²ƒì…ë‹ˆë‹¤.")
                            else:
                                st.markdown("**ë¬¸ì„œ ë‚´ìš©:**")
                            st.markdown(doc['content'])
                            if doc['page_info']:
                                st.caption(f"ğŸ“ ì¶œì²˜: {doc['page_info']}")
                            if len(doc['full_content']) > 500:
                                with st.expander("ì „ì²´ ë‚´ìš© ë³´ê¸°"):
                                    st.markdown(doc['full_content'])
                
                # ì„ íƒëœ ì„¹ì…˜ ì •ë³´ í‘œì‹œ
                if selected_sections:
                    st.markdown("---")
                    st.markdown("### ğŸ“‹ ì°¸ê³ í•œ ì„¹ì…˜")
                    for section in selected_sections:
                        st.markdown(f"- **{section['title']}** (í˜ì´ì§€ {section['start_page']}-{section['end_page']})")
                
                add_message("user", user_input)
                add_message("assistant", answer)
                
            except Exception as e:
                import traceback
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                with st.expander("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´"):
                    st.code(traceback.format_exc())
    else:
        if not pdf_path:
            warning_msg.error("ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        elif not toc_index:
            warning_msg.error("PDF ëª©ì°¨ ë¶„ì„ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

