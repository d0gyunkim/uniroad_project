"""
ì „ì—­ ê²€ìƒ‰ ëª¨ë“œ Streamlit ì•±
ë¼ìš°íŒ… ì—†ëŠ” ì „ì—­ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì›¹ ì¸í„°í˜ì´ìŠ¤ë¡œ ì œê³µí•©ë‹ˆë‹¤.
"""
import os
import sys
from pathlib import Path
from dotenv import load_dotenv
import streamlit as st
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(override=True)

from core.rag_system import RAGSystem
from supabase import create_client

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì „ì—­ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸",
    page_icon="ğŸ”",
    layout="wide"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'rag_system' not in st.session_state:
    st.session_state.rag_system = None
if 'initialized' not in st.session_state:
    st.session_state.initialized = False
if 'sort_by' not in st.session_state:
    st.session_state.sort_by = 'default'

def get_document_id_from_chunk(supabase_client, chunk_id):
    """chunk_idë¡œ document_id ì¡°íšŒ"""
    if not supabase_client or not chunk_id:
        return None
    
    try:
        response = supabase_client.table("document_chunks").select("document_id").eq("id", chunk_id).execute()
        if response.data and response.data[0].get("document_id"):
            return response.data[0].get("document_id")
    except Exception as e:
        print(f"âš ï¸ chunk_idë¡œ document_id ì¡°íšŒ ì‹¤íŒ¨: {e}")
    return None

def get_document_name(supabase_client, document_id):
    """document_idë¡œ ë¬¸ì„œ ì´ë¦„ ì¡°íšŒ"""
    if not supabase_client or not document_id:
        return None
    
    try:
        response = supabase_client.table("documents").select("filename").eq("id", document_id).execute()
        if response.data:
            return response.data[0].get("filename")
    except Exception as e:
        print(f"âš ï¸ ë¬¸ì„œ ì´ë¦„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    return None

def get_document_names_batch(supabase_client, document_ids):
    """ì—¬ëŸ¬ document_idì˜ ë¬¸ì„œ ì´ë¦„ì„ ì¼ê´„ ì¡°íšŒ"""
    if not supabase_client or not document_ids:
        return {}
    
    document_names = {}
    unique_ids = list(set([did for did in document_ids if did]))
    
    if not unique_ids:
        return document_names
    
    try:
        response = supabase_client.table("documents").select("id, filename").in_("id", unique_ids).execute()
        for doc in response.data:
            document_names[doc["id"]] = doc.get("filename")
    except Exception as e:
        print(f"âš ï¸ ë¬¸ì„œ ì´ë¦„ ì¼ê´„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    return document_names

def get_document_summaries_batch(supabase_client, document_ids):
    """ì—¬ëŸ¬ document_idì˜ summaryë¥¼ ì¼ê´„ ì¡°íšŒ"""
    if not supabase_client or not document_ids:
        return {}
    
    document_summaries = {}
    unique_ids = list(set([did for did in document_ids if did]))
    
    if not unique_ids:
        return document_summaries
    
    try:
        response = supabase_client.table("documents").select("id, summary").in_("id", unique_ids).execute()
        for doc in response.data:
            document_summaries[doc["id"]] = doc.get("summary")
    except Exception as e:
        print(f"âš ï¸ ë¬¸ì„œ summary ì¼ê´„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    return document_summaries

def get_document_summaries_from_chunks(supabase_client, chunk_ids):
    """chunk_id ë¦¬ìŠ¤íŠ¸ë¡œ document_idë¥¼ ì¡°íšŒí•œ í›„ summaryë¥¼ ê°€ì ¸ì˜¤ê¸°"""
    if not supabase_client or not chunk_ids:
        return {}
    
    document_summaries = {}
    chunk_to_doc = {}
    
    try:
        # chunk_idë¡œ document_id ì¡°íšŒ
        unique_chunk_ids = list(set([cid for cid in chunk_ids if cid]))
        if not unique_chunk_ids:
            return document_summaries
        
        response = supabase_client.table("document_chunks").select("id, document_id").in_("id", unique_chunk_ids).execute()
        
        # chunk_id -> document_id ë§¤í•‘
        document_ids = []
        for chunk in response.data:
            chunk_id = chunk.get("id")
            doc_id = chunk.get("document_id")
            if chunk_id and doc_id:
                chunk_to_doc[chunk_id] = doc_id
                document_ids.append(doc_id)
        
        # document_idë¡œ summary ì¡°íšŒ
        if document_ids:
            unique_doc_ids = list(set(document_ids))
            doc_response = supabase_client.table("documents").select("id, summary").in_("id", unique_doc_ids).execute()
            
            doc_id_to_summary = {}
            for doc in doc_response.data:
                doc_id_to_summary[doc["id"]] = doc.get("summary")
            
            # chunk_id -> summary ë§¤í•‘
            for chunk_id, doc_id in chunk_to_doc.items():
                document_summaries[chunk_id] = doc_id_to_summary.get(doc_id)
        
    except Exception as e:
        print(f"âš ï¸ chunk_idë¡œ summary ì¡°íšŒ ì‹¤íŒ¨: {e}")
        import traceback
        print(traceback.format_exc())
    
    return document_summaries

def get_document_names_from_chunks(supabase_client, chunk_ids):
    """chunk_id ë¦¬ìŠ¤íŠ¸ë¡œ document_idë¥¼ ì¡°íšŒí•œ í›„ ë¬¸ì„œ ì´ë¦„ì„ ê°€ì ¸ì˜¤ê¸°"""
    if not supabase_client or not chunk_ids:
        return {}
    
    document_names = {}
    chunk_to_doc = {}
    
    try:
        # chunk_idë¡œ document_id ì¡°íšŒ
        unique_chunk_ids = list(set([cid for cid in chunk_ids if cid]))
        if not unique_chunk_ids:
            return document_names
        
        response = supabase_client.table("document_chunks").select("id, document_id").in_("id", unique_chunk_ids).execute()
        
        # chunk_id -> document_id ë§¤í•‘
        document_ids = []
        for chunk in response.data:
            chunk_id = chunk.get("id")
            doc_id = chunk.get("document_id")
            if chunk_id and doc_id:
                chunk_to_doc[chunk_id] = doc_id
                document_ids.append(doc_id)
        
        # document_idë¡œ ë¬¸ì„œ ì´ë¦„ ì¡°íšŒ
        if document_ids:
            unique_doc_ids = list(set(document_ids))
            doc_response = supabase_client.table("documents").select("id, filename").in_("id", unique_doc_ids).execute()
            
            doc_id_to_name = {}
            for doc in doc_response.data:
                doc_id_to_name[doc["id"]] = doc.get("filename")
            
            # chunk_id -> document_name ë§¤í•‘
            for chunk_id, doc_id in chunk_to_doc.items():
                document_names[chunk_id] = doc_id_to_name.get(doc_id)
        
    except Exception as e:
        print(f"âš ï¸ chunk_idë¡œ ë¬¸ì„œ ì´ë¦„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        import traceback
        print(traceback.format_exc())
    
    return document_names

def calculate_cosine_similarity(vec1, vec2):
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0:
        return 0.0
    return dot_product / (norm1 * norm2)

def calculate_metadata_similarities(results, document_summaries, query, embeddings, content_weight=0.6, summary_weight=0.4):
    """ê° ì²­í¬ì˜ ë‚´ìš© ìœ ì‚¬ë„ì™€ summary ìœ ì‚¬ë„, ê·¸ë¦¬ê³  ê°€ì¤‘í‰ê·  ê³„ì‚°"""
    if not embeddings or not query:
        return {}
    
    try:
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = embeddings.embed_query(query)
        
        similarities = {}
        
        for i, result in enumerate(results):
            content_similarity = 0.0
            summary_similarity = 0.0
            weighted_average = 0.0
            
            # ì²­í¬ ë‚´ìš© ìœ ì‚¬ë„ ê³„ì‚°
            content = result.get('content', '')
            if content:
                try:
                    # ì„±ëŠ¥ ìµœì í™”: ì²˜ìŒ 2000ìë§Œ ì‚¬ìš©
                    content_text = content[:2000] if len(content) > 2000 else content
                    content_embedding = embeddings.embed_query(content_text)
                    content_similarity = calculate_cosine_similarity(query_embedding, content_embedding)
                except Exception as e:
                    print(f"âš ï¸ ë‚´ìš© ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                    content_similarity = 0.0
            
            # summary ìœ ì‚¬ë„ ê³„ì‚°
            summary = result.get('summary', '')
            if not summary and isinstance(document_summaries, dict) and i in document_summaries:
                summary = document_summaries.get(i, '')
            
            if summary:
                try:
                    summary_embedding = embeddings.embed_query(summary)
                    summary_similarity = calculate_cosine_similarity(query_embedding, summary_embedding)
                except Exception as e:
                    print(f"âš ï¸ Summary ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                    summary_similarity = 0.0
            
            # ê°€ì¤‘í‰ê·  ê³„ì‚° (ë‚´ìš© ìœ ì‚¬ë„ì™€ Summary ìœ ì‚¬ë„)
            if content_similarity > 0 or summary_similarity > 0:
                weighted_average = (content_similarity * content_weight) + (summary_similarity * summary_weight)
            
            similarities[i] = {
                'content_similarity': content_similarity,
                'summary_similarity': summary_similarity,
                'weighted_average': weighted_average
            }
        
        return similarities
    except Exception as e:
        print(f"âš ï¸ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return {}

def initialize_rag_system():
    """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    if st.session_state.initialized and st.session_state.rag_system:
        return st.session_state.rag_system
    
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    supabase_url = os.getenv("SUPABASE_URL")
    supabase_key = os.getenv("SUPABASE_KEY")
    gemini_key = os.getenv("GEMINI_API_KEY")
    
    if not supabase_url or not supabase_key:
        st.error("âŒ Supabase í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\n.env íŒŒì¼ì— SUPABASE_URLê³¼ SUPABASE_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        return None
    
    if not gemini_key:
        st.error("âŒ GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\n.env íŒŒì¼ì— GEMINI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        return None
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    with st.spinner("ğŸ“¦ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘..."):
        try:
            rag_system = RAGSystem()
            st.session_state.rag_system = rag_system
            st.session_state.initialized = True
            return rag_system
        except Exception as e:
            st.error(f"âŒ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            st.exception(e)
            return None

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    st.title("ğŸ” ì „ì—­ ê²€ìƒ‰ ëª¨ë“œ í…ŒìŠ¤íŠ¸")
    st.markdown("ë¼ìš°íŒ… ì—†ëŠ” ì „ì—­ ê²€ìƒ‰ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        # ì´ˆê¸°í™” ë²„íŠ¼
        if st.button("ğŸ”„ ì‹œìŠ¤í…œ ì´ˆê¸°í™”", use_container_width=True):
            st.session_state.rag_system = None
            st.session_state.initialized = False
            st.rerun()
        
        st.divider()
        
        # ê²€ìƒ‰ ì„¤ì •
        st.subheader("ê²€ìƒ‰ ì„¤ì •")
        top_k = st.slider("ì´ˆê¸° ê²€ìƒ‰ ê°œìˆ˜ (top_k)", min_value=10, max_value=100, value=30, step=10)
        display_count = st.slider("í‘œì‹œí•  ì²­í¬ ê°œìˆ˜", min_value=1, max_value=20, value=10, step=1)
        
        st.divider()
        
        # ì •ë³´
        st.info("ğŸ’¡ **ì‚¬ìš© ë°©ë²•**\n\n1. í•™êµ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”\n2. ê²€ìƒ‰ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”\n3. ê²€ìƒ‰ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”")
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag_system = initialize_rag_system()
    
    if not rag_system:
        st.stop()
    
    # ë©”ì¸ ì˜ì—­
    col1, col2 = st.columns([1, 1])
    
    with col1:
        school_name = st.text_input(
            "ğŸ« í•™êµ ì´ë¦„",
            value="ê³ ë ¤ëŒ€í•™êµ",
            placeholder="ì˜ˆ: ê³ ë ¤ëŒ€í•™êµ, ì„œìš¸ëŒ€í•™êµ, ê²½í¬ëŒ€í•™êµ"
        )
    
    with col2:
        query = st.text_input(
            "â“ ê²€ìƒ‰ ì§ˆë¬¸",
            value="ìˆ˜ì‹œ ì „í˜• ëª¨ì§‘ì¸ì›",
            placeholder="ê²€ìƒ‰í•  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"
        )
    
    # ê²€ìƒ‰ ë²„íŠ¼
    if st.button("ğŸ” ê²€ìƒ‰ ì‹¤í–‰", type="primary", use_container_width=True):
        if not school_name or not query:
            st.warning("âš ï¸ í•™êµ ì´ë¦„ê³¼ ê²€ìƒ‰ ì§ˆë¬¸ì„ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            st.stop()
        
        # ê²€ìƒ‰ ì§„í–‰ í‘œì‹œ
        with st.spinner(f"ğŸ” '{query}' ê²€ìƒ‰ ì¤‘... (í•™êµ: {school_name})"):
            try:
                # ì „ì—­ ê²€ìƒ‰ ìˆ˜í–‰ (ê°€ì¤‘ í‰ê·  ìœ ì‚¬ë„ ìƒìœ„ 10ê°œ ë°˜í™˜)
                results = rag_system.search_global_raw(
                    school_name=school_name,
                    query=query,
                    top_k=top_k
                )
                
                if not results:
                    st.warning("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    st.stop()
                
                # ê²°ê³¼ ìš”ì•½
                st.success(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ì²­í¬ ë°œê²¬ (ê°€ì¤‘ í‰ê·  ìœ ì‚¬ë„ ìƒìœ„ 10ê°œ)")
                
                # ìƒìœ„ Nê°œ ì²­í¬ í‘œì‹œ
                display_results = results[:display_count]
                
                # Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
                supabase_url = os.getenv("SUPABASE_URL")
                supabase_key = os.getenv("SUPABASE_KEY")
                
                if not supabase_url or not supabase_key:
                    st.error("âŒ Supabase í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    st.stop()
                
                supabase_client = create_client(supabase_url, supabase_key)
                
                # chunk_idë¡œ ì‹¤ì œ ì²­í¬ ì •ë³´ ì¡°íšŒ
                chunk_ids = [r.get('chunk_id') for r in display_results if r.get('chunk_id')]
                
                # ì²­í¬ ì •ë³´ ì¡°íšŒ
                chunks_info = {}
                if chunk_ids:
                    try:
                        response = supabase_client.table("document_chunks").select(
                            "id, content, raw_data, page_number, chunk_type, section_id, document_id"
                        ).in_("id", chunk_ids).execute()
                        
                        for chunk in response.data:
                            chunk_id = chunk.get("id")
                            chunks_info[chunk_id] = {
                                "content": chunk.get("raw_data") or chunk.get("content", ""),
                                "page_number": chunk.get("page_number", 0),
                                "chunk_type": chunk.get("chunk_type", "text"),
                                "section_id": chunk.get("section_id"),
                                "document_id": chunk.get("document_id")
                            }
                    except Exception as e:
                        st.error(f"âŒ ì²­í¬ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                        st.stop()
                
                # ë¬¸ì„œ ì´ë¦„ ë° summary ì¡°íšŒ
                document_names = {}
                document_summaries = {}
                document_ids = [r.get('document_id') for r in display_results if r.get('document_id')]
                
                # document_idë¡œ ë¬¸ì„œ ì´ë¦„ ë° summary ì¡°íšŒ
                if document_ids:
                    try:
                        unique_doc_ids = list(set(document_ids))
                        doc_names_by_id = get_document_names_batch(supabase_client, unique_doc_ids)
                        doc_summaries_by_id = get_document_summaries_batch(supabase_client, unique_doc_ids)
                        
                        # ê²°ê³¼ì— ë¬¸ì„œ ì´ë¦„ê³¼ summary ë§¤í•‘
                        for result in display_results:
                            doc_id = result.get('document_id')
                            chunk_id = result.get('chunk_id')
                            if doc_id and doc_id in doc_names_by_id:
                                result['document_name'] = doc_names_by_id[doc_id]
                            if doc_id and doc_id in doc_summaries_by_id:
                                result['summary'] = doc_summaries_by_id[doc_id]
                            # ì²­í¬ ì •ë³´ë„ ì¶”ê°€
                            if chunk_id and chunk_id in chunks_info:
                                result.update(chunks_info[chunk_id])
                    except Exception as e:
                        st.warning(f"âš ï¸ ë¬¸ì„œ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
                        import traceback
                        st.code(traceback.format_exc())
                
                # ë©”íƒ€ë°ì´í„° ìœ ì‚¬ë„ ê³„ì‚° (summaryì™€ ë‚´ìš© ìœ ì‚¬ë„, ê°€ì¤‘í‰ê· )
                metadata_similarities = {}
                if rag_system.embeddings:
                    with st.spinner("ğŸ“Š ìœ ì‚¬ë„ ê³„ì‚° ì¤‘..."):
                        # display_resultsì—ì„œ summary ì¶”ì¶œ
                        summaries_dict = {}
                        for i, result in enumerate(display_results):
                            if 'summary' in result:
                                summaries_dict[i] = result.get('summary', '')
                        
                        metadata_similarities = calculate_metadata_similarities(
                            display_results, summaries_dict, query, rag_system.embeddings
                        )
                
                # ì •ë ¬ ê¸°ì¤€ ì„ íƒ
                st.divider()
                st.subheader("ğŸ”„ ì •ë ¬ ê¸°ì¤€ ì„ íƒ")
                
                sort_options = {
                    "ê¸°ë³¸ (ë‚´ìš© ìœ ì‚¬ë„)": "default",
                    "ë‚´ìš© ìœ ì‚¬ë„": "content",
                    "Summary ìœ ì‚¬ë„": "summary",
                    "ê°€ì¤‘í‰ê· ": "weighted"
                }
                
                col_sort1, col_sort2, col_sort3, col_sort4 = st.columns(4)
                
                with col_sort1:
                    if st.button("ğŸ“„ ê¸°ë³¸", use_container_width=True, key="sort_default"):
                        st.session_state.sort_by = "default"
                        st.rerun()
                
                with col_sort2:
                    if st.button("ğŸ“ ë‚´ìš©", use_container_width=True, key="sort_content"):
                        st.session_state.sort_by = "content"
                        st.rerun()
                
                with col_sort3:
                    if st.button("ğŸ“„ Summary", use_container_width=True, key="sort_summary"):
                        st.session_state.sort_by = "summary"
                        st.rerun()
                
                with col_sort4:
                    if st.button("âš–ï¸ ê°€ì¤‘í‰ê· ", use_container_width=True, key="sort_weighted"):
                        st.session_state.sort_by = "weighted"
                        st.rerun()
                
                # ì„ íƒëœ ì •ë ¬ ê¸°ì¤€ì— ë”°ë¼ ì¬ì •ë ¬
                sort_by = st.session_state.get('sort_by', 'default')
                
                if sort_by != 'default' and metadata_similarities:
                    # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ê²°ê³¼ì™€ ìœ ì‚¬ë„ ì •ë³´ë¥¼ í•¨ê»˜ ì •ë ¬)
                    results_with_sim = list(zip(display_results, [metadata_similarities.get(i, {}) for i in range(len(display_results))]))
                    
                    sort_key_map = {
                        'content': 'content_similarity',
                        'summary': 'summary_similarity',
                        'weighted': 'weighted_average'
                    }
                    
                    sort_key = sort_key_map.get(sort_by, 'content_similarity')
                    
                    results_with_sim.sort(
                        key=lambda x: x[1].get(sort_key, 0.0),
                        reverse=True
                    )
                    
                    # ì •ë ¬ëœ ê²°ê³¼ ë¶„ë¦¬
                    display_results = [r[0] for r in results_with_sim]
                    metadata_similarities = {i: r[1] for i, r in enumerate(results_with_sim)}
                
                # ì •ë ¬ ê¸°ì¤€ í‘œì‹œ
                if sort_by != 'default':
                    sort_label = {
                        'content': 'ë‚´ìš© ìœ ì‚¬ë„',
                        'summary': 'Summary ìœ ì‚¬ë„',
                        'weighted': 'ê°€ì¤‘í‰ê· '
                    }.get(sort_by, 'ê¸°ë³¸')
                    st.info(f"ğŸ“Œ í˜„ì¬ ì •ë ¬ ê¸°ì¤€: **{sort_label}**")
                
                # ê²°ê³¼ë¥¼ íƒ­ìœ¼ë¡œ êµ¬ë¶„
                tab1, tab2 = st.tabs(["ğŸ“‹ ìƒì„¸ ë³´ê¸°", "ğŸ“Š ìš”ì•½ ë³´ê¸°"])
                
                with tab1:
                    # ê° ì²­í¬ ìƒì„¸ ì •ë³´
                    for i, result in enumerate(display_results, 1):
                        # ìœ ì‚¬ë„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                        sim_info = metadata_similarities.get(i-1, {})
                        content_sim = sim_info.get('content_similarity', 0.0)
                        summary_sim = sim_info.get('summary_similarity', 0.0)
                        weighted_avg = sim_info.get('weighted_average', 0.0)
                        
                        # ì œëª©ì— ìœ ì‚¬ë„ ì •ë³´ ì¶”ê°€
                        title_suffix = ""
                        if sort_by == 'content':
                            title_suffix = f" (ë‚´ìš© ìœ ì‚¬ë„: {content_sim:.4f})"
                        elif sort_by == 'summary':
                            title_suffix = f" (Summary ìœ ì‚¬ë„: {summary_sim:.4f})"
                        elif sort_by == 'weighted':
                            title_suffix = f" (ê°€ì¤‘í‰ê· : {weighted_avg:.4f})"
                        
                        with st.expander(f"ğŸ“„ ì²­í¬ #{i} (ê°€ì¤‘í‰ê· : {weighted_avg:.4f}){title_suffix}", expanded=(i == 1)):
                            # ë©”íƒ€ë°ì´í„°
                            col_meta1, col_meta2, col_meta3 = st.columns(3)
                            
                            with col_meta1:
                                st.metric("í˜ì´ì§€", result.get('page_number', 'N/A'))
                            
                            with col_meta2:
                                st.metric("ì²­í¬ ID", result.get('chunk_id', 'N/A'))
                            
                            with col_meta3:
                                st.metric("ì²­í¬ íƒ€ì…", result.get('chunk_type', 'N/A'))
                            
                            # ë©”íƒ€ë°ì´í„° ìœ ì‚¬ë„ í‘œì‹œ
                            if metadata_similarities:
                                st.divider()
                                st.markdown("**ğŸ“Š ìœ ì‚¬ë„ ì •ë³´:**")
                                col_sim1, col_sim2, col_sim3 = st.columns(3)
                                
                                with col_sim1:
                                    st.metric("ë‚´ìš© ìœ ì‚¬ë„", f"{content_sim:.4f}")
                                
                                with col_sim2:
                                    st.metric("Summary ìœ ì‚¬ë„", f"{summary_sim:.4f}")
                                
                                with col_sim3:
                                    st.metric("ê°€ì¤‘í‰ê· ", f"{weighted_avg:.4f}")
                            
                            st.divider()
                            
                            # ì¶”ê°€ ì •ë³´
                            info_cols = st.columns(3)
                            with info_cols[0]:
                                document_name = result.get('document_name', 'ì •ë³´ ì—†ìŒ')
                                st.markdown(f"**ğŸ“„ ë¬¸ì„œ:** {document_name}")
                                document_id = result.get('document_id')
                                if document_id:
                                    st.caption(f"*ë¬¸ì„œ ID: {document_id}*")
                            
                            with info_cols[1]:
                                section_id = result.get('section_id')
                                if section_id:
                                    st.caption(f"**ì„¹ì…˜ ID:** {section_id}")
                                else:
                                    st.caption("**ì„¹ì…˜ ID:** ì •ë³´ ì—†ìŒ")
                            
                            with info_cols[2]:
                                chunk_id = result.get('chunk_id', 'N/A')
                                st.caption(f"**ì²­í¬ ID:** {chunk_id}")
                            
                            st.divider()
                            
                            # ì²­í¬ ë‚´ìš©
                            content = result.get('content', '')
                            if content:
                                st.markdown("**ë‚´ìš©:**")
                                st.text_area(
                                    "ì²­í¬ ë‚´ìš©",
                                    value=content,
                                    height=200,
                                    disabled=True,
                                    key=f"content_{i}",
                                    label_visibility="collapsed"
                                )
                                
                                # ë‚´ìš© ê¸¸ì´ ì •ë³´
                                if len(content) > 500:
                                    st.caption(f"ğŸ“ ë‚´ìš© ê¸¸ì´: {len(content):,}ì")
                            else:
                                st.info("âš ï¸ ì²­í¬ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                with tab2:
                    # ìš”ì•½ í…Œì´ë¸”
                    import pandas as pd
                    
                    summary_data = []
                    for i, result in enumerate(display_results, 1):
                        document_name = result.get('document_name', '-')
                        sim_info = metadata_similarities.get(i-1, {})
                        content = result.get('content', '')
                        
                        summary_data.append({
                            "ìˆœìœ„": i,
                            "ì²­í¬ ID": result.get('chunk_id', 'N/A'),
                            "ì„¹ì…˜ ID": result.get('section_id', 'N/A'),
                            "ë¬¸ì„œ ID": result.get('document_id', 'N/A'),
                            "ë¬¸ì„œ": document_name,
                            "í˜ì´ì§€": result.get('page_number', 'N/A'),
                            "ë‚´ìš© ìœ ì‚¬ë„": f"{sim_info.get('content_similarity', 0.0):.4f}",
                            "Summary ìœ ì‚¬ë„": f"{sim_info.get('summary_similarity', 0.0):.4f}",
                            "ê°€ì¤‘í‰ê· ": f"{sim_info.get('weighted_average', 0.0):.4f}",
                            "ì²­í¬ íƒ€ì…": result.get('chunk_type', 'N/A'),
                            "ë‚´ìš© ê¸¸ì´": len(content)
                        })
                    
                    df = pd.DataFrame(summary_data)
                    st.dataframe(df, use_container_width=True, hide_index=True)
                    
                    # í†µê³„ ì •ë³´
                    st.divider()
                    col_stat1, col_stat2, col_stat3 = st.columns(3)
                    
                    with col_stat1:
                        st.metric("ì´ ê²€ìƒ‰ ê²°ê³¼", len(results))
                    
                    with col_stat2:
                        st.metric("í‘œì‹œëœ ì²­í¬", len(display_results))
                    
                    with col_stat3:
                        avg_score = sum(r.get('score', 0.0) for r in display_results) / len(display_results) if display_results else 0
                        st.metric("í‰ê·  ìœ ì‚¬ë„", f"{avg_score:.4f}")
                
            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                st.exception(e)

if __name__ == "__main__":
    main()

