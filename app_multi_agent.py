import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_teddynote.prompts import load_prompt
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_community.vectorstores import FAISS
from langchain_core.runnables import RunnablePassthrough
from langchain_teddynote import logging
from dotenv import load_dotenv
import os
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# API KEY ì •ë³´ë¡œë“œ
load_dotenv()

# GEMINI_API_KEYë¥¼ GOOGLE_API_KEYë¡œ ë§¤í•‘
if os.getenv("GEMINI_API_KEY"):
    os.environ["GOOGLE_API_KEY"] = os.getenv("GEMINI_API_KEY")

# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.
logging.langsmith("[Project] Multi-Agent RAG - Gemini")

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists(".cache"):
    os.mkdir(".cache")

if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")

if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")

if not os.path.exists(".cache/multi_agent"):
    os.mkdir(".cache/multi_agent")

st.title("ğŸ¤– ë©€í‹° ì—ì´ì „íŠ¸ ì…ì‹œ ì»¨ì„¤í„´íŠ¸")

# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ
if "messages" not in st.session_state:
    st.session_state["messages"] = []

if "agents" not in st.session_state:
    st.session_state["agents"] = None

if "coordinator" not in st.session_state:
    st.session_state["coordinator"] = None

# ì‚¬ì´ë“œë°” ìƒì„±
with st.sidebar:
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")
    
    # ë¶„í•  PDF í´ë” ê²½ë¡œ
    pdf_folder = st.text_input(
        "ë¶„í•  PDF í´ë” ê²½ë¡œ",
        value="ê²½í¬ëŒ€_ìˆ˜ì‹œìš”ê°•_ë¶„í• í‘œ"
    )
    
    load_btn = st.button("PDF ë¡œë“œ ë° ì—ì´ì „íŠ¸ ìƒì„±")
    
    selected_model = "gemini-2.0-flash-lite"
    
    # ì¬ì‹œë„ ì„¤ì •
    max_retries = st.slider("ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜", min_value=1, max_value=5, value=3, help="ë‹µë³€ í’ˆì§ˆì´ ë¶€ì¡±í•  ë•Œ ì¬ì‹œë„í•˜ëŠ” ìµœëŒ€ íšŸìˆ˜")
    
    st.markdown("---")
    st.markdown("### ğŸ¤– ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ")
    st.markdown("1. 4ê°œ PDFë¥¼ ê°ê° ì²˜ë¦¬")
    st.markdown("2. ê° ì—ì´ì „íŠ¸ê°€ **ë¬¸ì„œ ê¸°ë°˜** ë…ë¦½ ë‹µë³€")
    st.markdown("3. ì½”ë””ë„¤ì´í„°ê°€ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©")
    st.markdown("4. **í’ˆì§ˆ í‰ê°€ ë° ìë™ ì¬ì‹œë„**")
    st.markdown("")
    st.markdown("âœ… ì •ë³´: ë¬¸ì„œ ê¸°ë°˜ ì—„ê²©")
    st.markdown("âœ… ë‹µë³€: ìì—°ìŠ¤ëŸ½ê³  ì¹œì ˆ")
    st.markdown("âœ… í’ˆì§ˆ: ìë™ í‰ê°€ ë° ì¬ì‹œë„")
    
    st.markdown("---")
    st.markdown("### âš™ï¸ ì‹œìŠ¤í…œ ìƒíƒœ")
    if st.session_state["agents"]:
        st.success(f"âœ… {len(st.session_state['agents'])}ê°œ ì—ì´ì „íŠ¸ í™œì„±í™”")
        st.info("ğŸ“„ ë¬¸ì„œ ê¸°ë°˜ ì—„ê²© ëª¨ë“œ")
    else:
        st.warning("â³ ì—ì´ì „íŠ¸ ëŒ€ê¸° ì¤‘")

# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥
def print_messages():
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)

# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€
def add_message(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))

# ë‹¨ì¼ PDFë¥¼ ì„ë² ë”©í•˜ëŠ” í•¨ìˆ˜
def embed_single_pdf(pdf_path, pdf_name):
    """
    ë‹¨ì¼ PDFë¥¼ ì„ë² ë”©í•˜ì—¬ retriever ë°˜í™˜
    """
    # ë¬¸ì„œ ë¡œë“œ
    loader = PDFPlumberLoader(pdf_path)
    docs = loader.load()
    
    # ë¬¸ì„œ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=3000,
        chunk_overlap=600,
        length_function=len,
        separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
    )
    split_documents = text_splitter.split_documents(docs)
    
    # ì„ë² ë”© ìƒì„±
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    
    # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)
    
    # ê²€ìƒ‰ê¸° ìƒì„±
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": 15,  # ë©€í‹° ì—ì´ì „íŠ¸ì´ë¯€ë¡œ ê°œë³„ ì—ì´ì „íŠ¸ëŠ” ì¡°ê¸ˆ ì¤„ì„
            "fetch_k": 30,
            "lambda_mult": 0.8
        }
    )
    
    return {
        "name": pdf_name,
        "retriever": retriever,
        "path": pdf_path
    }

# ë©€í‹° ì—ì´ì „íŠ¸ ë¡œë“œ
@st.cache_resource(show_spinner="ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
def load_multi_agents(folder_path):
    """
    í´ë” ë‚´ ëª¨ë“  PDFë¥¼ ë¡œë“œí•˜ì—¬ ë©€í‹° ì—ì´ì „íŠ¸ ìƒì„±
    """
    folder = Path(folder_path)
    
    if not folder.exists():
        st.error(f"í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}")
        return None
    
    # PDF íŒŒì¼ ì°¾ê¸°
    pdf_files = list(folder.glob("*.pdf"))
    
    if not pdf_files:
        st.error(f"í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {folder_path}")
        return None
    
    st.info(f"ğŸ“„ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ ë°œê²¬")
    
    agents = []
    
    # ê° PDFì— ëŒ€í•´ ì—ì´ì „íŠ¸ ìƒì„±
    for idx, pdf_file in enumerate(pdf_files, 1):
        with st.spinner(f"ì—ì´ì „íŠ¸ {idx}/{len(pdf_files)} ìƒì„± ì¤‘... ({pdf_file.name})"):
            agent = embed_single_pdf(str(pdf_file), pdf_file.stem)
            agents.append(agent)
            st.success(f"âœ… ì—ì´ì „íŠ¸ {idx} ì¤€ë¹„ ì™„ë£Œ: {pdf_file.name}")
    
    return agents

# ë‹¨ì¼ ì—ì´ì „íŠ¸ ì²´ì¸ ìƒì„±
def create_agent_chain(retriever, model_name="gemini-2.0-flash-lite"):
    """
    ë‹¨ì¼ ì—ì´ì „íŠ¸ì˜ RAG ì²´ì¸ ìƒì„± - ë¬¸ì„œ ê¸°ë°˜ ì—„ê²© ëª¨ë“œ
    """
    # ì—ì´ì „íŠ¸ ì „ìš© í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (ë¬¸ì„œ ê¸°ë°˜ ì—„ê²©)
    prompt = load_prompt("prompts/pdf-rag-agent.yaml", encoding="utf-8")
    
    # Temperature 0ìœ¼ë¡œ ê³ ì • (ì •í™•ë„ ìµœìš°ì„ )
    llm = ChatGoogleGenerativeAI(model=model_name, temperature=0)
    
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return chain

# ì½”ë””ë„¤ì´í„° ìƒì„±
def create_coordinator(model_name="gemini-2.0-flash-lite"):
    """
    ì—¬ëŸ¬ ì—ì´ì „íŠ¸ì˜ ë‹µë³€ì„ í‰ê°€í•˜ê³  ìµœì  ë‹µë³€ì„ ì„ íƒ/ì¢…í•©í•˜ëŠ” ì½”ë””ë„¤ì´í„°
    """
    coordinator_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì´ë©°, ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì˜ ì½”ë””ë„¤ì´í„°ì…ë‹ˆë‹¤.

ì—¬ëŸ¬ ì—ì´ì „íŠ¸ê°€ ê°ìì˜ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°™ì€ ì§ˆë¬¸ì— ë‹µë³€í–ˆìŠµë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì—­í• ì€ ì´ ë‹µë³€ë“¤ì„ í†µí•©í•˜ì—¬ í•™ìƒì—ê²Œ ìµœì ì˜ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

**ì •ë³´ ì¶œì²˜ ê·œì¹™ (ì—„ê²©):**

1. ì˜¤ì§ ì—ì´ì „íŠ¸ë“¤ì´ ë¬¸ì„œì—ì„œ ì°¾ì€ êµ¬ì²´ì ì¸ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
2. ì—ì´ì „íŠ¸ê°€ "ì°¾ì„ ìˆ˜ ì—†ë‹¤"ê³  í•œ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ê±°ë‚˜ ë§Œë“¤ì§€ ë§ˆì„¸ìš”
3. ì™¸ë¶€ ì§€ì‹ìœ¼ë¡œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ì •ë³´ë¥¼ ë³´ì¶©í•˜ì§€ ë§ˆì„¸ìš”
4. ìˆ«ì, ë‚ ì§œ, ì¸ì›, ì „í˜•ëª… ë“± ëª¨ë“  êµ¬ì²´ì  ì •ë³´ëŠ” ì—ì´ì „íŠ¸ ë‹µë³€ì— ìˆëŠ” ê²ƒë§Œ ì‚¬ìš©

**ë‹µë³€ êµ¬ì„± (ìì—°ìŠ¤ëŸ½ê²Œ):**

1. í•™ìƒê³¼ í•™ë¶€ëª¨ê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ëª…í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
2. ì—¬ëŸ¬ ì—ì´ì „íŠ¸ì˜ ì •ë³´ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ í†µí•©í•˜ê³  ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”
3. ì§ˆë¬¸ì˜ ë§¥ë½ì— ë§ê²Œ ìì—°ìŠ¤ëŸ¬ìš´ íë¦„ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
4. ì¤‘ë³µë˜ëŠ” ì •ë³´ëŠ” í•œ ë²ˆë§Œ ì–¸ê¸‰í•˜ì„¸ìš”
5. í•„ìš”ì‹œ ì •ë³´ë¥¼ ì •ë¦¬í•˜ì—¬ í‘œë‚˜ ëª©ë¡ í˜•íƒœë¡œ ì œì‹œí•˜ì„¸ìš”
6. ëª¨ë“  ì—ì´ì „íŠ¸ê°€ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆë‹¤ë©´, "ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”

**ì¤‘ìš”:**
- ì •ë³´(ìˆ«ì, ë‚ ì§œ, ì´ë¦„ ë“±) = ì—ì´ì „íŠ¸ ë‹µë³€ì—ì„œë§Œ (ì—„ê²©)
- ì„¤ëª… ë°©ì‹ = ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ

---

**í•™ìƒ ì§ˆë¬¸:**
{question}

---

**ì—ì´ì „íŠ¸ ë‹µë³€ë“¤:**

{agent_responses}

---

**ìµœì¢… ë‹µë³€:**
""")
    
    llm = ChatGoogleGenerativeAI(model=model_name, temperature=0)
    
    chain = coordinator_prompt | llm | StrOutputParser()
    
    return chain

# ë‹µë³€ í’ˆì§ˆ í‰ê°€ í•¨ìˆ˜
def evaluate_answer_quality(question, answer, model_name="gemini-2.0-flash-lite"):
    """
    ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì—¬ ì ì ˆí•œì§€ íŒë‹¨
    """
    evaluation_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ì»¨ì„¤í„´íŠ¸ì˜ ë‹µë³€ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**í‰ê°€ ê¸°ì¤€:**
1. **ê´€ë ¨ì„±** (0-10ì ): ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆëŠ”ê°€?
2. **ì™„ì „ì„±** (0-10ì ): ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ì¶©ë¶„íˆ ì™„ì „í•œê°€? (ì¤‘ìš” ì •ë³´ ëˆ„ë½ ì—†ìŒ)
3. **ì •í™•ì„±** (0-10ì ): ë‹µë³€ì— êµ¬ì²´ì ì¸ ì •ë³´(ìˆ«ì, ë‚ ì§œ, ì´ë¦„ ë“±)ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?
4. **ìœ ìš©ì„±** (0-10ì ): í•™ìƒ/í•™ë¶€ëª¨ì—ê²Œ ì‹¤ì œë¡œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì¸ê°€?

**ë¶ˆí•©ê²© ê¸°ì¤€ (ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¼ë„ í•´ë‹¹ë˜ë©´ ë¶ˆí•©ê²©):**
- "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤" ê°™ì€ ë¶ˆì™„ì „í•œ ë‹µë³€
- ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ë‹µë³€
- êµ¬ì²´ì ì¸ ì •ë³´ ì—†ì´ ì¶”ìƒì ì¸ ì„¤ëª…ë§Œ ìˆëŠ” ë‹µë³€
- ì´ì ì´ 30ì  ë¯¸ë§Œì¸ ê²½ìš°

**í‰ê°€ í˜•ì‹:**
ì´ì : [0-40ì ]
íŒì •: [í•©ê²©/ë¶ˆí•©ê²©]
ì´ìœ : [ê°„ë‹¨í•œ í‰ê°€ ì´ìœ ]

---
**ì§ˆë¬¸:**
{question}

**ë‹µë³€:**
{answer}

---
**í‰ê°€:**
""")
    
    llm = ChatGoogleGenerativeAI(model=model_name, temperature=0)
    chain = evaluation_prompt | llm | StrOutputParser()
    
    evaluation = chain.invoke({
        "question": question,
        "answer": answer
    })
    
    # í‰ê°€ ê²°ê³¼ íŒŒì‹±
    is_acceptable = "ë¶ˆí•©ê²©" not in evaluation and "í•©ê²©" in evaluation
    
    return {
        "is_acceptable": is_acceptable,
        "evaluation_text": evaluation
    }

# ë©€í‹° ì—ì´ì „íŠ¸ ì§ˆì˜ì‘ë‹µ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
def multi_agent_query_with_retry(question, agents, coordinator, model_name="gemini-2.0-flash-lite", max_retries=3):
    """
    ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€ (í’ˆì§ˆ í‰ê°€ ë° ì¬ì‹œë„ í¬í•¨)
    """
    for attempt in range(1, max_retries + 1):
        if attempt > 1:
            st.warning(f"ğŸ”„ {attempt}ë²ˆì§¸ ì‹œë„ ì¤‘... (ì´ì „ ë‹µë³€ í’ˆì§ˆ ê°œì„  í•„ìš”)")
        
        # ë‹¨ê³„ 1: ê° ì—ì´ì „íŠ¸ì—ê²Œ ì§ˆë¬¸
        st.info(f"ğŸ¤– {len(agents)}ê°œ ì—ì´ì „íŠ¸ì—ê²Œ ì§ˆë¬¸ ì¤‘... (ì‹œë„ {attempt}/{max_retries})")
        
        agent_responses = []
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ
        def query_agent(agent):
            chain = create_agent_chain(agent["retriever"], model_name)
            response = chain.invoke(question)
            return {
                "agent_name": agent["name"],
                "response": response
            }
        
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(query_agent, agent) for agent in agents]
            
            for idx, future in enumerate(as_completed(futures), 1):
                result = future.result()
                agent_responses.append(result)
                st.success(f"âœ… ì—ì´ì „íŠ¸ {idx}/{len(agents)} ë‹µë³€ ì™„ë£Œ: {result['agent_name']}")
        
        # ë‹¨ê³„ 2: ì—ì´ì „íŠ¸ ë‹µë³€ í‘œì‹œ (í¼ì¹¨ ê°€ëŠ¥)
        with st.expander(f"ğŸ” ê° ì—ì´ì „íŠ¸ ë‹µë³€ ë³´ê¸° (ì‹œë„ {attempt})"):
            for idx, response in enumerate(agent_responses, 1):
                st.markdown(f"**[ì—ì´ì „íŠ¸ {idx}: {response['agent_name']}]**")
                st.markdown(response['response'])
                st.markdown("---")
        
        # ë‹¨ê³„ 3: ì½”ë””ë„¤ì´í„°ê°€ ìµœì  ë‹µë³€ ì„ íƒ/ì¢…í•©
        st.info("ğŸ¯ ì½”ë””ë„¤ì´í„°ê°€ ìµœì  ë‹µë³€ ìƒì„± ì¤‘...")
        
        # ì—ì´ì „íŠ¸ ë‹µë³€ë“¤ì„ í¬ë§·íŒ…
        formatted_responses = "\n\n".join([
            f"[ì—ì´ì „íŠ¸ {idx}: {resp['agent_name']}]\n{resp['response']}"
            for idx, resp in enumerate(agent_responses, 1)
        ])
        
        # ì½”ë””ë„¤ì´í„°ê°€ ìµœì¢… ë‹µë³€ ìƒì„±
        final_response = coordinator.invoke({
            "question": question,
            "agent_responses": formatted_responses
        })
        
        # ë‹¨ê³„ 4: ë‹µë³€ í’ˆì§ˆ í‰ê°€
        st.info("ğŸ“Š ë‹µë³€ í’ˆì§ˆ í‰ê°€ ì¤‘...")
        quality_result = evaluate_answer_quality(question, final_response, model_name)
        
        # í‰ê°€ ê²°ê³¼ í‘œì‹œ
        with st.expander("ğŸ“‹ í’ˆì§ˆ í‰ê°€ ê²°ê³¼"):
            st.markdown(quality_result["evaluation_text"])
        
        # í’ˆì§ˆì´ ì ì ˆí•˜ë©´ ë°˜í™˜
        if quality_result["is_acceptable"]:
            if attempt > 1:
                st.success(f"âœ… {attempt}ë²ˆì§¸ ì‹œë„ì—ì„œ ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤!")
            return final_response
        else:
            if attempt < max_retries:
                st.warning(f"âš ï¸ ë‹µë³€ í’ˆì§ˆì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ì¬ì‹œë„í•©ë‹ˆë‹¤... ({attempt}/{max_retries})")
                # ì¬ì‹œë„ë¥¼ ìœ„í•´ ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ ì¡°ì •
                # (ë‹¤ìŒ ì‹œë„ì—ì„œ ë” ë§ì€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ë„ë¡)
            else:
                st.warning(f"âš ï¸ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜({max_retries})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
                return final_response
    
    # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ ë§ˆì§€ë§‰ ë‹µë³€ ë°˜í™˜
    return final_response

# ë©€í‹° ì—ì´ì „íŠ¸ ì§ˆì˜ì‘ë‹µ (ê¸°ì¡´ í•¨ìˆ˜ - í˜¸í™˜ì„± ìœ ì§€)
def multi_agent_query(question, agents, coordinator, model_name="gemini-2.0-flash-lite"):
    """
    ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
    """
    return multi_agent_query_with_retry(question, agents, coordinator, model_name, max_retries=3)

# PDF ë¡œë“œ ë²„íŠ¼ì´ ëˆŒë¦¬ë©´
if load_btn:
    agents = load_multi_agents(pdf_folder)
    if agents:
        st.session_state["agents"] = agents
        st.session_state["coordinator"] = create_coordinator(selected_model)
        st.success(f"ğŸ‰ ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! ({len(agents)}ê°œ ì—ì´ì „íŠ¸)")
        st.rerun()

# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´
if clear_btn:
    st.session_state["messages"] = []

# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
print_messages()

# ì‚¬ìš©ìì˜ ì…ë ¥
user_input = st.chat_input("ì…ì‹œ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”!")

# ê²½ê³  ë©”ì‹œì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­
warning_msg = st.empty()

# ë§Œì•½ì— ì‚¬ìš©ì ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´...
if user_input:
    agents = st.session_state["agents"]
    coordinator = st.session_state["coordinator"]
    
    if agents and coordinator:
        # ì‚¬ìš©ìì˜ ì…ë ¥
        st.chat_message("user").write(user_input)
        
        with st.chat_message("assistant"):
            # ë©€í‹° ì—ì´ì „íŠ¸ ì²˜ë¦¬
            try:
                final_answer = multi_agent_query(
                    user_input, 
                    agents, 
                    coordinator, 
                    selected_model
                )
                
                st.markdown("### ğŸ“ ìµœì¢… ë‹µë³€")
                st.markdown(final_answer)
                
                # ëŒ€í™”ê¸°ë¡ ì €ì¥
                add_message("user", user_input)
                add_message("assistant", final_answer)
                
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    else:
        warning_msg.error("ë¨¼ì € 'PDF ë¡œë“œ ë° ì—ì´ì „íŠ¸ ìƒì„±' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•˜ì„¸ìš”.")

